# -*- coding: utf-8 -*-

import os
import sys
import socket
import struct
import azure
import re
import base64
import random
import threading

from azure.storage import BlobService
from azure.servicemanagement import *
from azure.servicemanagement import _XmlSerializer, _lower

from nixops import known_hosts
from nixops.util import wait_for_tcp_port, ping_tcp_port
from nixops.util import attr_property, create_key_pair, generate_random_string, check_wait
from nixops.nix_expr import Function, RawValue

from nixops.backends import MachineDefinition, MachineState
from nixops.azure_common import ResourceDefinition, ResourceState

from xml.etree import ElementTree


def normalize_empty(x):
    return (x if x != "" else None)


def device_name_to_lun(device):
    match = re.match(r'/dev/disk/by-lun/(\d+)$', device)
    return  None if match is None or int(match.group(1))>31 else int(match.group(1) )

def find_root_disk(block_device_mapping):
   return next((d_id for d_id, d in block_device_mapping.iteritems()
                  if d['device'] == '/dev/sda'), None)

# for an ephemeral disk, its unique id is blob url, and the name is randomly generated by azure
# for an external disk, we only know its name
def disk_id(disk):
    return (disk['media_link'] if disk["ephemeral"] else disk['name'])

class AzureDefinition(MachineDefinition, ResourceDefinition):
    """
    Definition of an Azure machine.
    """
    @classmethod
    def get_type(cls):
        return "azure"

    def __init__(self, xml):
        MachineDefinition.__init__(self, xml)

        x = xml.find("attrs/attr[@name='azure']/attrs")
        assert x is not None

        self.copy_option(x, 'machineName', str)

        self.copy_option(x, 'subscriptionId', str)
        self.copy_option(x, 'certificatePath', str)

        self.copy_option(x, 'roleSize', str, empty = False)
        self.copy_option(x, 'storage', 'resource', optional = True)
        self.copy_option(x, 'hostedService', 'resource')
        self.copy_option(x, 'deployment', 'resource')

        self.copy_option(x, 'rootDiskImage', str, empty = False)
        self.copy_option(x, 'baseEphemeralDiskUrl', str, optional = True)

        self.obtain_ip = self.get_option_value(x, 'obtainIP', bool)

        def parse_endpoint(xml, proto):
            probe_xml = xml.find("attrs/attr[@name='probe']/attrs")
            result =  {
                'name': xml.get('name'),
                'protocol': proto,
                'port': self.get_option_value(xml, 'port', int, positive = True),
                'local_port': self.get_option_value(xml, 'localPort', int, positive = True),
                'set': self.get_option_value(xml, 'setName', str, optional = True),
                'dsr': self.get_option_value(xml, 'directServerReturn', bool),
                'probe_path': normalize_empty(probe_xml is not None and
                              self.get_option_value(probe_xml, 'path', str,
                                                    optional = True) ),
                'probe_port': probe_xml is not None and
                              self.get_option_value(probe_xml, 'port', int,
                                                    optional = True, positive = True),
                'probe_protocol': probe_xml is not None and
                                  self.get_option_value(probe_xml, 'protocol', str,
                                                        optional = True)
            }
            if result['probe_protocol'] not in ['HTTP', 'TCP', None]:
                raise Exception('load balancer probe protocol must be either "HTTP" or "TCP"')
            return result

        ie_xml = x.find("attr[@name='inputEndpoints']/attrs")

        tcp_endpoints = [ parse_endpoint(k, "tcp")
                          for k in ie_xml.findall("attr[@name='tcp']/attrs/attr") ]
        udp_endpoints = [ parse_endpoint(k, "udp")
                          for k in ie_xml.findall("attr[@name='udp']/attrs/attr") ]

        self.input_endpoints = sorted(tcp_endpoints + udp_endpoints)

        def opt_disk_name(dname):
            return ("{0}-{1}".format(self.machine_name, dname) if dname is not None else None)

        def parse_block_device(xml):
            disk_resource = normalize_empty(
                self.get_option_value(xml, 'diskResource', 'resource', optional = True))
            ephemeral_name = normalize_empty(
                self.get_option_value(xml, 'ephemeralName', str, optional = True))
            #if ephemeral_name is not None:
            #    ephemeral_name = "{0}-{1}".format(self.machine_name, ephemeral_name)
            media_link = normalize_empty(self.get_option_value(xml, 'mediaLink', str, optional = True))
            if ephemeral_name and self.base_ephemeral_disk_url and media_link is None:
                media_link = "{0}{1}-{2}".format(self.base_ephemeral_disk_url,
                                                 self.machine_name, ephemeral_name)
            result =  {
                'name': disk_resource,
                'ephemeral_name': ephemeral_name,
                'device': xml.get("name"),
                'media_link': media_link,
                'label': self.get_option_value(xml, 'label', str, empty = False),
                'size': self.get_option_value(xml, 'size', int, optional = True),
                'ephemeral': ephemeral_name is not None,
                'host_caching': self.get_option_value(xml, 'hostCaching', str),
                'encrypt': self.get_option_value(xml, 'encrypt', bool),
                'passphrase': self.get_option_value(xml, 'passphrase', str)
            }

            if result['ephemeral'] and result['media_link'] is None:
                raise Exception("{0}: ephemeral disk {1} must specify mediaLink"
                                .format(self.machine_name, result['ephemeral_name']))
            if ( (disk_resource is not None and result['media_link'] is not None) or
                 (disk_resource is None and result['media_link'] is None) ):
                raise Exception("{0}: blockDeviceMapping item must specify either "
                                "diskResource or mediaLink, but not both at once"
                                .format(self.machine_name))
            if ( (disk_resource is None and ephemeral_name is None) or
                 (disk_resource is not None and ephemeral_name is not None) ):
                raise Exception("{0}: blockDeviceMapping item must specify either "
                                "diskResource or ephemeralName, but not both at once"
                                .format(self.machine_name))
            return result

        self.block_device_mapping = { disk_id(parse_block_device(d)): parse_block_device(d)
                                      for d in x.findall("attr[@name='blockDeviceMapping']/attrs/attr") }

        for d_id, disk in self.block_device_mapping.iteritems():
            if disk['device'] != "/dev/sda" and device_name_to_lun(disk['device']) is None:
                raise Exception("{0}: blockDeviceMapping only supports /dev/sda and "
                                "/dev/disk/by-lun/X block devices, where X is in 0..31 range"
                                .format(self.machine_name))
        if find_root_disk(self.block_device_mapping) is None:
            raise Exception("{0} needs a root disk".format(self.machine_name))

    def show_type(self):
        return "{0} [{1}]".format(self.get_type(), self.role_size or "???")


class AzureState(MachineState, ResourceState):
    """
    State of an Azure machine.
    """
    @classmethod
    def get_type(cls):
        return "azure"

    machine_name = attr_property("azure.name", None)
    public_ipv4 = attr_property("publicIpv4", None)

    role_size = attr_property("azure.roleSize", None)

    public_client_key = attr_property("azure.publicClientKey", None)
    private_client_key = attr_property("azure.privateClientKey", None)

    public_host_key = attr_property("azure.publicHostKey", None)
    private_host_key = attr_property("azure.privateHostKey", None)

    storage = attr_property("azure.storage", None)
    hosted_service = attr_property("azure.hostedService", None)
    deployment = attr_property("azure.deployment", None)

    obtain_ip = attr_property("azure.obtain_ip", None, bool)

    input_endpoints = attr_property("azure.input_endpoints", {}, 'json')
    block_device_mapping = attr_property("azure.blockDeviceMapping", {}, 'json')

    backups = attr_property("azure.backups", {}, 'json')

    def __init__(self, depl, name, id):
        MachineState.__init__(self, depl, name, id)
        self._sms = None
        self._bs = None

    @property
    def resource_id(self):
        return self.machine_name

    def show_type(self):
        s = super(AzureState, self).show_type()
        return "{0} [{1}]".format(s, self.role_size)

    credentials_prefix = "deployment.azure"

    @property
    def full_name(self):
        return "Azure machine '{0}'".format(self.machine_name)

    def bs(self):
        if not self._bs:
            storage_resource = next((r for r in self.depl.resources.values()
                                       if getattr(r, 'storage_name', None) == self.storage), None)
            self._bs = BlobService(self.storage, storage_resource.access_key)
        return self._bs

    # delete_vhd = None: ask the user
    def _delete_volume(self, volume_id, disk_id = None, delete_vhd = None):
        if volume_id is None:
            self.warn("attempted to delete a disk without a name; this is a bug")
            return
        try:
            self.log("waiting for Azure disk {0} to detach...".format(disk_id or volume_id))
            def check_detached():
                return self.sms().get_disk(volume_id).attached_to is None
            check_wait(check_detached, initial=1, max_tries=100, exception=True)

            if delete_vhd or (delete_vhd is None and
                              self.depl.logger.confirm("are you sure you want to destroy "
                                                       "the contents(BLOB) of Azure disk '{0}'?"
                                                       .format(disk_id or volume_id)) ):
                self.log("destroying Azure disk '{0}' and its contents...".format(disk_id or volume_id))
                self.sms().delete_disk(volume_id, delete_vhd=True)
            else:
                self.log("keeping the contents but destroying Azure disk resource '{0}'...".format(disk_id or volume_id))
                self.sms().delete_disk(volume_id, delete_vhd=False)

        except azure.WindowsAzureMissingResourceError:
            self.warn("seems to have been destroyed already")

    def _create_disk(self, has_operating_system, label, media_link, name, os):
        if has_operating_system:
            return self.sms()._perform_post(
                      self.sms()._get_disk_path(),
                      _XmlSerializer.doc_from_data(
                          'Disk',
                          [('OS', os),
                          ('HasOperatingSystem', has_operating_system, _lower),
                          ('Label', label),
                          ('MediaLink', media_link),
                          ('Name', name)]))
        else:
            return self.sms().add_disk(has_operating_system, label, media_link, name, os)


    def _node_deleted(self):
        self.vm_id = None
        self.state = self.STOPPED
        for d_id, disk in self.block_device_mapping.iteritems():
            disk['needsAttach'] = True
            self.update_block_device_mapping(d_id, disk)

    defn_properties = [ 'role_size', 'input_endpoints', 'obtain_ip' ]

    def is_deployed(self):
        return (self.vm_id or self.block_device_mapping)

    def get_resource(self):
        try:
            return self.sms().get_role(self.hosted_service, self.deployment, self.resource_id)
        except azure.WindowsAzureMissingResourceError:
            return None

    def destroy_resource(self):
        req = self.sms().delete_role(self.hosted_service, self.deployment, self.resource_id)
        self.finish_request(req)

    def is_settled(self, resource):
        return True

    def fetch_PIP(self):
        d = self.sms().get_deployment_by_name(self.hosted_service, self.deployment)
        instance = next((r for r in d.role_instance_list if r.instance_name == self.machine_name), None)
        return (instance and instance.public_ips and instance.public_ips[0].address)

    def wait_deployment_unlocked(self):
        def check_req():
            self.log("waiting for deployment lock")
            return not self.sms().get_deployment_by_name(self.hosted_service, self.deployment).locked
        check_wait(check_req, initial=1, max_tries=100, exception=True)

    def mk_network_configuration(self, endpoints):
        network_configuration = ConfigurationSet()
        for ie in endpoints:
            cfg = ConfigurationSetInputEndpoint(
                      name = ie['name'],
                      protocol = ie['protocol'],
                      port = ie['port'],
                      local_port = ie['local_port'],
                      load_balanced_endpoint_set_name = ie['set'],
                      enable_direct_server_return = ie['dsr'] )
            cfg.load_balancer_probe.path = ie['probe_path']
            cfg.load_balancer_probe.port = ie['probe_port']
            cfg.load_balancer_probe.protocol = ie['probe_protocol']
            network_configuration.input_endpoints.input_endpoints.append(cfg)
        return network_configuration

    def update_block_device_mapping(self, k, v):
        x = self.block_device_mapping
        if v == None:
            x.pop(k, None)
        else:
            x[k] = v
        self.block_device_mapping = x

    deployment_lock = threading.Lock()

    def create(self, defn, check, allow_reboot, allow_recreate):
        self.no_change(self.machine_name != defn.machine_name, "instance name")
        self.no_property_change(defn, 'hosted_service')
        self.no_property_change(defn, 'deployment')

        self.set_common_state(defn)
        self.copy_credentials(defn)
        self.machine_name = defn.machine_name
        self.storage = defn.storage
        self.hosted_service = defn.hosted_service
        self.deployment = defn.deployment

        if not self.public_client_key:
            (private, public) = create_key_pair()
            self.public_client_key = public
            self.private_client_key = private

        if not self.public_host_key:
            (private, public) = create_key_pair(type="ecdsa")
            self.public_host_key = public
            self.private_host_key = private

        if check:
            vm = self.get_settled_resource()
            if vm:
                if self.vm_id:
                    self.handle_changed_property('public_ipv4', self.fetch_PIP())
                    self.update_ssh_known_hosts()

                    net_cfg = vm.configuration_sets[0]
                    assert net_cfg.configuration_set_type == 'NetworkConfiguration'
                    ies = []
                    for ie in (net_cfg.input_endpoints or []):
                        ies.append({
                            'protocol': ie.protocol,
                            'name': ie.name,
                            'port': int(ie.port),
                            'local_port': int(ie.local_port),
                            'dsr': ie.enable_direct_server_return,
                            'set': normalize_empty(ie.load_balanced_endpoint_set_name),
                            'probe_port': ie.load_balancer_probe and int(ie.load_balancer_probe.port),
                            'probe_path': normalize_empty(ie.load_balancer_probe and ie.load_balancer_probe.path),
                            'probe_protocol': ie.load_balancer_probe and ie.load_balancer_probe.protocol.upper() })
                    self.handle_changed_property('input_endpoints', sorted(ies))

                    # check the root disk
                    os_disk_res_name = "OS disk of {0}".format(self.full_name)
                    _root_disk_id = find_root_disk(self.block_device_mapping)
                    assert _root_disk_id is not None
                    root_disk = self.block_device_mapping[_root_disk_id]
                    self.warn_if_changed(root_disk["host_caching"], vm.os_virtual_hard_disk.host_caching, "host_caching",
                                         resource_name = os_disk_res_name, can_fix = False)
                    self.warn_if_changed(root_disk["label"], vm.os_virtual_hard_disk.disk_label or "", "label",
                                         resource_name = os_disk_res_name, can_fix = False)
                    self.warn_if_changed(root_disk["name"], vm.os_virtual_hard_disk.disk_name, "name",
                                         resource_name = os_disk_res_name, can_fix = False)
                    self.warn_if_changed(root_disk["media_link"], vm.os_virtual_hard_disk.media_link, "media_link",
                                         resource_name = os_disk_res_name, can_fix = False)

                    # check data disks
                    for dvhd in vm.data_virtual_hard_disks.data_virtual_hard_disks:
                        d_id = next((d_id for d_id, d in self.block_device_mapping.iteritems()
                                          if d['name'] == dvhd.disk_name), None)
                        if d_id:
                            disk = self.block_device_mapping[d_id]

                            disk["host_caching"] = self.warn_if_changed(disk["host_caching"], dvhd.host_caching, "host_caching",
                                                                        resource_name = "data disk {0}".format(dvhd.disk_name))
                            disk["label"] = self.warn_if_changed(disk["label"], dvhd.disk_label, "label",
                                                                 resource_name = "data disk {0}".format(dvhd.disk_name))
                            disk["size"] = self.warn_if_changed(disk["size"], dvhd.logical_disk_size_in_gb, "size",
                                                                resource_name = "data disk {0}".format(dvhd.disk_name))
                            if disk.get("needs_attach", False):
                                self.warn("disk {0} was not supposed to be attached".format(d_id))
                                disk["needs_attach"] = False

                            if disk["ephemeral"]:
                                self.warn_if_changed(disk["media_link"], dvhd.media_link, "media_link",
                                                     resource_name = "data disk {0}".format(dvhd.disk_name),
                                                                    can_fix = False)
                            if dvhd.lun != device_name_to_lun(disk['device']):
                                self.warn("disk {0}({1}) is attached to this instance at a wrong LUN"
                                          .format(dvhd.disk_name, dvhd.media_link))
                                if not allow_reboot:
                                    raise Exception("reboot is required to reattach the disk at the correct LUN; "
                                                    "please run with --allow-reboot")
                                self.stop()
                                self.log("detaching disk {0}...".format(dvhd.disk_name))
                                req = self.sms().delete_data_disk(self.hosted_service, self.deployment,
                                                                  self.machine_name, dvhd.lun,
                                                                  delete_vhd = False)
                                self.finish_request(req)
                                disk["needs_attach"] = True
                                self.start()

                            self.update_block_device_mapping(d_id, disk)
                        else:
                            self.warn("unexpected disk {0}({1}) is attached to this instance"
                                      .format(dvhd.disk_name, dvhd.media_link))
                            if not allow_reboot:
                                raise Exception("reboot is required to detach the unexpected disk; "
                                                "please run with --allow-reboot")
                            self.stop()
                            self.log("detaching disk {0}...".format(dvhd.disk_name))
                            req = self.sms().delete_data_disk(self.hosted_service, self.deployment,
                                                              self.machine_name, dvhd.lun,
                                                              delete_vhd = False)
                            self.finish_request(req)
                            self.start()

                    # check for detached disks
                    for d_id, disk in self.block_device_mapping.iteritems():
                        if device_name_to_lun(disk['device']) is not None:
                            dvhd = next((d for d in vm.data_virtual_hard_disks.data_virtual_hard_disks
                                          if d.disk_name == disk['name']), None)
                            if dvhd is None:
                                self.warn("disk {0} has been unexpectedly detached".format(d_id))
                                disk["needs_attach"] = True
                                self.update_block_device_mapping(d_id, disk)
                else:
                    self.warn_not_supposed_to_exist(valuable_data = True)
                    self.confirm_destroy()
            else:
                if self.vm_id:
                    self.warn("the instance seems to have been destroyed behind our back")
                    if not allow_recreate: raise Exception("use --allow-recreate to fix")
                    self._node_deleted()

            # check that all deployed disks exist
            for d_id, disk in self.block_device_mapping.iteritems():
                try:
                    self.sms().get_disk(disk["name"])
                except azure.WindowsAzureMissingResourceError:
                    self.warn("disk {0} has been unexpectedly deleted".format(d_id))
                    self.update_block_device_mapping(d_id, None)

        if self.vm_id and defn.role_size != self.role_size and not allow_reboot:
            raise Exception("reboot is required to change role size; please run with --allow-reboot")

        self._assert_no_impossible_disk_changes(defn)

        # change the root disk of a deployed vm
        # FIXME: implement this better via update_role?
        if self.vm_id:
            def_root_disk_id = find_root_disk(defn.block_device_mapping)
            assert def_root_disk_id is not None
            def_root_disk = defn.block_device_mapping[def_root_disk_id]
            self_root_disk_id = find_root_disk(self.block_device_mapping)
            assert self_root_disk_id is not None
            self_root_disk = self.block_device_mapping[self_root_disk_id]

            if ( (def_root_disk_id != def_root_disk_id) or
                 (def_root_disk["ephemeral"] != self_root_disk["ephemeral"]) or
                 (def_root_disk["host_caching"] != self_root_disk["host_caching"]) or
                 (def_root_disk["label"] != self_root_disk["label"]) ):
                self.warn("modification of the root disk of {0} is requested, "
                          "which requires that the machine is re-created"
                          .format(self.full_name))
                if allow_recreate:
                    self.destroy_resource()
                    self._node_deleted()
                else:
                    raise Exception("use --allow-recreate to fix")

        self._change_existing_disk_parameters(defn)

        # check that reboot is allowed if we need to detach disks
        for d_id, disk in self.block_device_mapping.items():
            lun = device_name_to_lun(disk['device'])
            if( self.vm_id and d_id not in defn.block_device_mapping and
                lun is not None and not disk.get('needsAttach', False) and
                not allow_reboot):
                raise Exception("reboot is required to detach disk {0}; "
                                "please run with --allow-reboot".format(d_id))

        self._create_ephemeral_disks_from_blobs(defn)
        with self.deployment_lock:
            self._create_vm(defn)

        if self.properties_changed(defn):
            with self.deployment_lock:
                self.log("updating properties of {0}...".format(self.full_name))
                network_configuration = self.mk_network_configuration(defn.input_endpoints)
                if defn.obtain_ip:
                    network_configuration.public_ips.public_ips.append(PublicIP(name = "public"))
                req = self.sms().update_role(defn.hosted_service, defn.deployment, defn.machine_name,
                                            network_config = network_configuration,
                                            role_size = defn.role_size)
                self.finish_request(req)
                self.copy_properties(defn)

                new_ip = self.fetch_PIP()
                if self.public_ipv4 != new_ip:
                    self.log("got IP: {0}".format(new_ip))
                self.public_ipv4 = new_ip
                self.update_ssh_known_hosts()
                self.ssh_pinged = False


        self._attach_detached_disks(defn)
        self._create_missing_attach_new(defn)

        self._generate_default_encryption_keys()


    # change existing disk params as much as possible within the technical limitations
    def _change_existing_disk_parameters(self, defn):
        for d_id, disk in defn.block_device_mapping.iteritems():
            curr_disk = self.block_device_mapping.get(d_id, None)
            if curr_disk is None: continue
            lun = device_name_to_lun(disk["device"])
            if lun is None: continue
            if self.vm_id and not curr_disk.get("needs_attach", False):
                if disk["device"] != curr_disk["device"]:
                    raise Exception("can't change LUN of the attached disk {0}; "
                                    "please deploy a configuration with this disk detached first".format(d_id))
                # update_data_disk seems to allow the label change, but it does nothing
                if disk["label"] != curr_disk["label"]: #FIXME: recreate vm for this?
                    raise Exception("can't change the label of the attached disk {0}".format(d_id))
                if disk["host_caching"] != curr_disk["host_caching"]:
                    self.log("changing parameters of the attached disk {0}".format(d_id))
                    req = self.sms().update_data_disk(self.hosted_service, self.deployment, self.machine_name,
                                                      lun, updated_lun = lun, 
                                                      disk_name = curr_disk["name"],
                                                      host_caching = disk["host_caching"] )
                    self.finish_request(req)
            curr_disk["device"] = disk["device"]
            curr_disk["host_caching"] = disk["host_caching"]
            curr_disk["label"] = disk["label"]
            curr_disk["passphrase"] = disk["passphrase"]
            self.update_block_device_mapping(d_id, curr_disk)

    # check that we aren't making impossible changes like
    # changing LUN, ephemeral, media_link for ephemeral disks
    def _assert_no_impossible_disk_changes(self, defn):
        if self.vm_id is None: return

        for d_id, disk in defn.block_device_mapping.iteritems():
            same_lun_id = next((_id for _id, d in self.block_device_mapping.iteritems()
                                    if d["device"] == disk["device"]), None)
            if same_lun_id is not None and (same_lun_id != d_id) and (
                not self.block_device_mapping[same_lun_id].get("needs_attach", False) ):
                raise Exception("can't mount Azure disk '{0}' because its LUN({1}) is already "
                                "occupied by Azure disk '{2}'; you need to deploy a configuration "
                                "with this LUN left empty before using it to attach a different data disk"
                                .format(d_id, disk["device"], same_lun_id))

    # attach existing but detached disks
    def _attach_detached_disks(self, defn):
        for d_id, _disk in defn.block_device_mapping.iteritems():
            disk = self.block_device_mapping.get(d_id, None)
            if disk is None or not disk.get("needs_attach", False): continue
            lun = device_name_to_lun(disk["device"])
            if lun is not None:
                self.log("attaching data disk {0} to {1}"
                         .format(d_id, self.full_name))
                req = self.sms().add_data_disk(
                        self.hosted_service, self.deployment, self.machine_name,
                        lun, disk_name = disk['name'],
                        host_caching = _disk['host_caching'],
                        disk_label = _disk['label'] )
                self.finish_request(req)
                disk["needs_attach"] = False
                disk["host_caching"] = _disk["host_caching"]
                disk["label"] = _disk["label"]
                self.update_block_device_mapping(d_id, disk)

    # create missing disks/attach new external disks
    # creation code assumes that the blob doesn't exist because
    # otherwise a disk would be created from it by _create_ephemeral_disks_from_blobs()
    def _create_missing_attach_new(self, defn):
        for d_id, disk in defn.block_device_mapping.iteritems():
            if d_id in self.block_device_mapping: continue
            self.log("attaching data disk {0} to {1}"
                     .format(d_id, self.full_name))
            if disk["ephemeral"]:
                req = self.sms().add_data_disk(
                          defn.hosted_service, defn.deployment, defn.machine_name,
                          device_name_to_lun(disk['device']),
                          host_caching = disk['host_caching'],
                          media_link = disk['media_link'],
                          disk_label = disk['label'],
                          logical_disk_size_in_gb = disk['size']
                      )
                self.finish_request(req)
                dd = self.sms().get_data_disk(defn.hosted_service, defn.deployment, defn.machine_name,
                                              device_name_to_lun(disk['device']))
                disk['name'] = dd.disk_name
            else:
                req = self.sms().add_data_disk(
                          defn.hosted_service, defn.deployment, defn.machine_name,
                          device_name_to_lun(disk['device']),
                          disk_name = disk['name'],
                          host_caching = disk['host_caching'],
                          disk_label = disk['label']
                      )
                self.finish_request(req)

            self.update_block_device_mapping(d_id, disk)

    # generate LUKS key if the model didn't specify one
    def _generate_default_encryption_keys(self):
        for d_id, disk in self.block_device_mapping.iteritems():
            if disk.get('encrypt', False) and disk.get('passphrase', "") == "" and disk.get('generatedKey', "") == "":
                disk['generatedKey'] = generate_random_string(length=256)
                self.update_block_device_mapping(d_id, disk)

    # This is an ugly hack around the fact that to create a new disk,
    # add_data_disk() needs to be called in a different way depending
    # on whether the blob exists and there's no API to check direcly
    # whether the blob exists or not.
    # We work around it by attempting to create a disk from the blob instead
    # of relying on add_data_disk(), and use add_data_disk() only to create
    # new blobs(for which there's no other API) and attaching existing disks.
    # The same problem and solution applies to the root disk.
    def _create_ephemeral_disks_from_blobs(self, defn):
        for d_id, disk in defn.block_device_mapping.iteritems():
            if d_id in self.block_device_mapping or not disk["ephemeral"]: continue
            try:
                new_name = "nixops-{0}-{1}-{2}".format(self.machine_name, disk["ephemeral_name"],
                                                        random.randrange(1000000000))
                self.log("attempting to create a disk resource for {0}".format(d_id))
                self._create_disk(device_name_to_lun(disk["device"]) is None,
                                  disk["label"], disk["media_link"],
                                  new_name, "Linux")
                new_disk = disk.copy()
                new_disk["name"] = new_name
                new_disk["needs_attach"] = True
                self.update_block_device_mapping(d_id, new_disk)

            except azure.WindowsAzureMissingResourceError:
                self.warn("looks like the underlying blob doesn't exist, so it will be created later")
            except azure.WindowsAzureConflictError:
                self.warn("got ConflictError which most likely means that the blob "
                          "exists and is being used by another disk resource")


    def _create_vm(self, defn):
        if self.vm_id: return

        if self.get_settled_resource():
            raise Exception("tried creating a virtual machine that already exists; "
                            "please run 'deploy --check' to fix this")
        self.log("creating {0}...".format(self.full_name))

        custom_data = ('ssh_host_ecdsa_key=$(cat<<____HERE\n{0}\n____HERE\n)\n'
                      'ssh_host_ecdsa_key_pub="{1}"\nssh_root_auth_key="{2}"\n'
                      ).format(self.private_host_key, self.public_host_key, self.public_client_key)

        root_disk_id = find_root_disk(defn.block_device_mapping)
        root_disk_spec = defn.block_device_mapping[root_disk_id]
        existing_root_disk = self.block_device_mapping.get(root_disk_id, None)
        root_disk_name = (existing_root_disk and existing_root_disk.get("name", None)) or root_disk_spec['name']
        if root_disk_name:
            root_disk = OSVirtualHardDisk(disk_name = root_disk_name,
                                          host_caching = root_disk_spec["host_caching"],
                                          disk_label = root_disk_spec["label"] )
            config = None
        else:
            root_disk = OSVirtualHardDisk(source_image_name = defn.root_disk_image,
                                          media_link = root_disk_spec['media_link'],
                                          host_caching = root_disk_spec["host_caching"],
                                          disk_label = root_disk_spec["label"] )
            config = LinuxConfigurationSet(host_name = defn.machine_name,
                                          user_name = 'user',
                                          user_password = generate_random_string(length=48), #'paSS55'
                                          #disable_ssh_password_authentication = False,
                                          custom_data = base64.b64encode(custom_data))

        network_configuration = self.mk_network_configuration(defn.input_endpoints)
        if defn.obtain_ip:
            network_configuration.public_ips.public_ips.append(PublicIP(name = "public"))

        # data disk config
        disks = DataVirtualHardDisks()
        for d_id, disk in defn.block_device_mapping.iteritems():
            lun = device_name_to_lun(disk['device'])
            if lun is None: continue # skip OS disk
            disk_cfg = DataVirtualHardDisk()
            curr_disk = self.block_device_mapping.get(d_id, None)
            # create a new ephemeral disk or mount the existing one
            if disk['ephemeral'] and curr_disk is None:
                disk_cfg.media_link = disk['media_link']
                disk_cfg.logical_disk_size_in_gb = disk['size']
            else:
                disk_cfg.disk_name = curr_disk['name']
            disk_cfg.disk_label = disk['label']
            disk_cfg.host_caching = disk['host_caching']
            disk_cfg.lun = lun
            disks.data_virtual_hard_disks.append(disk_cfg)

        req = self.sms().add_role(defn.hosted_service, defn.deployment, defn.machine_name,
                                  config, root_disk,
                                  availability_set_name = None,
                                  data_virtual_hard_disks = disks,
                                  network_config = network_configuration,
                                  role_size = defn.role_size)
        self.finish_request(req)

        self.vm_id = self.machine_name
        self.state = self.STARTING
        self.ssh_pinged = False
        self.copy_properties(defn)

        for d_id, disk in defn.block_device_mapping.iteritems():
            lun = device_name_to_lun(disk['device'])
            if disk['ephemeral']:
                if disk['device'] == '/dev/sda':
                    vm = self.get_settled_resource()
                    disk['name'] = vm.os_virtual_hard_disk.disk_name
                if lun is not None:
                    dd = self.sms().get_data_disk(defn.hosted_service, defn.deployment, defn.machine_name, lun)
                    disk['name'] = dd.disk_name
            self.update_block_device_mapping(d_id, disk)

        self.public_ipv4 = self.fetch_PIP()
        self.log("got IP: {0}".format(self.public_ipv4))
        self.update_ssh_known_hosts()


    def warn_automatically_encrypted_disk(self, d_id, disk):
        if disk['encrypt'] and ( disk['passphrase'] == "" and
                                  disk.get('generatedKey', "") != "" ):
            self.warn("azure disk {0} has an automatically generated encryption key "
                      "that will be lost once the disk resource is deleted; "
                      "unless you retrieve the encryption key from the NixOps deployment, "
                      "the access to the data will be lost even if you decide to keep the BLOB; "
                      "pressing Ctrl+C is your last chance to keep the data".format(d_id))


    def after_activation(self, defn):
        # detach the volumes that are no longer in the deployment spec
        stopped = False
        for d_id, disk in self.block_device_mapping.items():
            lun = device_name_to_lun(disk['device'])
            if d_id not in defn.block_device_mapping and lun is not None:
                disk_name = disk['name']

                try:
                    if not disk.get('needsAttach', False):
                        # devices aren't removed correctly if the machine is running
                        if not stopped:
                            self.stop()
                            stopped = True
                        self.log("detaching Azure disk '{0}'...".format(d_id))
                        req = self.sms().delete_data_disk(defn.hosted_service, defn.deployment,
                                                          defn.machine_name, lun, delete_vhd = False)
                        self.finish_request(req)
                        disk['needsAttach'] = True
                        self.update_block_device_mapping(d_id, disk)

                    if disk['ephemeral']:
                        self.warn_automatically_encrypted_disk(d_id, disk)
                        self._delete_volume(disk_name, disk_id = d_id)

                except azure.WindowsAzureMissingResourceError:
                    self.warn("Azure disk '{0}' seems to have been destroyed already".format(d_id))

                self.update_block_device_mapping(d_id, None)
        if stopped:
            self.start()

    def reboot(self, hard=False):
        if hard:
            self.log("sending hard reset to Azure machine...")
            self.node().reboot()
            self.sms().restart_role(self.hosted_service, self.deployment, self.machine_name)
            self.state = self.STARTING
            self.ssh.reset()
        else:
            MachineState.reboot(self, hard=hard)
        self.ssh_pinged = False

    def start(self):
        if self.vm_id:
            # surprisingly although this was indended to avoid deployment lock-up
            # it may actually cause it even when a single operation is attempted
            #self.wait_deployment_unlocked()
            self.state = self.STARTING
            self.log("starting Azure machine...")
            req = self.sms().start_role(self.hosted_service, self.deployment, self.machine_name)
            self.finish_request(req)
            self.wait_for_ssh(check=True)
            self.send_keys()

    def stop(self):
        if self.vm_id:
           #FIXME: there's also "stopped deallocated" version of this. how to integrate?
            #self.wait_deployment_unlocked()
            self.log("stopping Azure machine... ")
            req = self.sms().shutdown_role(self.hosted_service, self.deployment, self.machine_name)
            self.state = self.STOPPING
            self.finish_request(req)
            self.state = self.STOPPED
            self.ssh.reset()
            self.ssh_pinged = False

    def destroy(self, wipe=False):
        if wipe:
            log.warn("wipe is not supported")

        if self.vm_id:
            vm = self.get_resource()
            if vm:
                question = "are you sure you want to destroy {0}?"
                if not self.depl.logger.confirm(question.format(self.full_name)):
                    return False
                with self.deployment_lock:
                    self.log("destroying the Azure machine...")
                    self.destroy_resource()
            else:
                self.warn("seems to have been destroyed already")
        self._node_deleted()

        # Destroy volumes created for this instance.
        for d_id, disk in self.block_device_mapping.items():
            if disk['ephemeral']:
                self.warn_automatically_encrypted_disk(d_id, disk)
                self._delete_volume(disk['name'], disk_id = d_id)
            self.update_block_device_mapping(d_id, None)

        return True


    def parse_blob_url(self, blob):
        match = re.match(r'https?://([^\./]+)\.[^/]+/([^/]+)/(.+)$', blob)
        return None if match is None else {
            "storage": match.group(1),
            "container": match.group(2),
            "name": match.group(3)
        }

    def _need_storage(self):
        if not self.storage:
            raise Exception("{0} needs to have storage specified to manage backups"
                            .format(self.full_name))

    def backup(self, defn, backup_id):
        self.log("backing up {0} using ID '{1}'".format(self.full_name, backup_id))

        if sorted(defn.block_device_mapping.keys()) != sorted(self.block_device_mapping.keys()):
            self.warn("the list of disks currently deployed doesn't match the current deployment"
                     " specification; consider running 'deploy' first; the backup may be incomplete")

        backup = {}
        _backups = self.backups
        for d_id, disk in self.block_device_mapping.iteritems():
            self._need_storage()
            media_link = self.sms().get_disk(disk["name"]).media_link
            self.log("snapshotting the BLOB {0} backing the Azure disk {1}".format(media_link, disk["name"]))
            blob = self.parse_blob_url(media_link)
            if blob["storage"] != self.storage:
                raise Exception("storage {1} provided in the deployment specification "
                                "doesn't match the storage of BLOB {1}"
                                .format(self.storage, blob_url))
            snapshot = self.bs().snapshot_blob(blob["container"], blob["name"],
                                               x_ms_meta_name_values = {
                                                   'nixops_backup_id': backup_id,
                                                   'description': "backup of disk {0} attached to {1}"
                                                                  .format(disk["name"], self.machine_name)
                                               })
            backup[media_link] = snapshot["x-ms-snapshot"]
            _backups[backup_id] = backup
            self.backups = _backups

    def restore(self, defn, backup_id, devices=[]):
        self._need_storage()
        self.log("restoring {0} to backup '{1}'".format(self.full_name, backup_id))

        if self.vm_id:
            self.stop()
            self.log("temporarily deprovisioining {0}".format(self.full_name))
            self.destroy_resource()
            self._node_deleted()

        for d_id, disk in self.block_device_mapping.items():
            azure_disk = self.sms().get_disk(disk["name"])
            s_id = self.backups[backup_id].get(azure_disk.media_link, None)
            if s_id and (devices == [] or azure_disk.media_link in devices or
                         disk["name"] in devices or disk["device"] in devices):
                self._need_storage()
                blob = self.parse_blob_url(azure_disk.media_link)
                if blob is None:
                    self.warn("failed to parse BLOB URL {0}; skipping"
                              .format(azure_disk.media_link))
                    continue
                if blob["storage"] != self.storage:
                    raise Exception("storage {1} provided in the deployment specification "
                                    "doesn't match the storage of BLOB {1}"
                                    .format(self.storage, blob_url))
                try:
                    self.bs().get_blob_properties(
                            blob["container"], "{0}?snapshot={1}"
                                                .format(blob["name"], s_id))
                except azure.WindowsAzureMissingResourceError:
                    self.warn("snapsnot {0} for disk {1} is missing; skipping".format(s_id, d_id))
                    continue

                self._delete_volume(disk["name"], disk_id = d_id, delete_vhd = False)

                self.log("restoring BLOB {0} from snapshot"
                         .format(azure_disk.media_link, s_id))
                self.bs().copy_blob(blob["container"], blob["name"],
                                   "{0}?snapshot={1}"
                                   .format(azure_disk.media_link, s_id) )

                self.log("re-creating disk resource {0} for BLOB {1}"
                         .format(azure_disk.name, azure_disk.media_link))
                self._create_disk(azure_disk.os, azure_disk.label,
                                  azure_disk.media_link, azure_disk.name, azure_disk.os)


    def remove_backup(self, backup_id, keep_physical=False):
        self.log('removing backup {0}'.format(backup_id))
        _backups = self.backups

        if not backup_id in _backups.keys():
            self.warn('backup {0} not found; skipping'.format(backup_id))
        else:
            for blob_url, snapshot_id in _backups[backup_id].iteritems():
                try:
                    self._need_storage()
                    self.log('removing snapshot {0} of BLOB {1}'.format(snapshot_id, blob_url))
                    blob = self.parse_blob_url(blob_url)
                    if blob is None:
                        self.warn("failed to parse BLOB URL {0}; skipping".format(blob_url))
                    if blob["storage"] != self.storage:
                        raise Exception("storage {1} provided in the deployment specification "
                                        "doesn't match the storage of BLOB {1}"
                                        .format(self.storage, blob_url))

                    self.bs().delete_blob(blob["container"], blob["name"], snapshot_id)
                except azure.WindowsAzureMissingResourceError:
                    self.warn('snapshot {0} of BLOB {1} not found; skipping'
                              .format(snapshot_id, blob_url))

            _backups.pop(backup_id)
            self.backups = _backups

    def get_backups(self):
        backups = {}
        for b_id, snapshots in self.backups.iteritems():
            backups[b_id] = {}
            backup_status = "complete"
            info = []
            processed = set()
            for d_id, disk in self.block_device_mapping.items():
                media_link = self.sms().get_disk(disk["name"]).media_link
                if not media_link in snapshots.keys():
                    backup_status = "incomplete"
                    info.append("{0} - {1} - not available in backup"
                                .format(self.name, d_id))
                else:
                    self._need_storage()
                    snapshot_id = snapshots[media_link]
                    processed.add(media_link)
                    blob = self.parse_blob_url(media_link)
                    if blob is None:
                        info.append("failed to parse BLOB URL {0}"
                                    .format(media_link))
                        backup_status = "unavailable"
                    elif blob["storage"] != self.storage:
                        info.append("storage {1} provided in the deployment specification "
                                    "doesn't match the storage of BLOB {1}"
                                    .format(self.storage, media_link))
                        backup_status = "unavailable"
                    else:
                        try:
                            snapshot = self.bs().get_blob_properties(
                                            blob["container"], "{0}?snapshot={1}"
                                                               .format(blob["name"], snapshot_id))
                        except azure.WindowsAzureMissingResourceError:
                            info.append("{0} - {1} - {2} - snapshot has disappeared"
                                        .format(self.name, d_id, snapshot_id))
                            backup_status = "unavailable"

            for media_link in (set(snapshots.keys())-processed):
                info.append("{0} - {1} - {2} - a snapshot of a disk that is not or no longer deployed"
                            .format(self.name, media_link, snapshots[media_link]))
            backups[b_id]['status'] = backup_status
            backups[b_id]['info'] = info

        return backups


    def _check(self, res):
        if self.subscription_id is None and self.certificate_path is None:
            res.exists = False
            res.is_up = False
            self.state = self.MISSING;
            return

        vm = self.get_resource()
        if vm is None:
            res.exists = False
            res.is_up = False
            self.state = self.MISSING;
        else:
            res.exists = True
            d = self.sms().get_deployment_by_name(self.hosted_service, self.deployment)
            role_instance = next((r for r in d.role_instance_list if r.instance_name == self.machine_name), None)
            if role_instance is None:
                self.state = self.UNKNOWN
            else:
                res.is_up = role_instance.power_state == "Started"
                if not res.is_up: self.state = self.STOPPED
                if res.is_up:
                    # check that all disks are attached
                    res.disks_ok = True
                    for d_id, disk in self.block_device_mapping.iteritems():
                        if device_name_to_lun(disk["device"]) is None:
                            if vm.os_virtual_hard_disk.disk_name!=disk["name"]:
                                res.disks_ok = False
                                res.messages.append("different root disk instead of {0}".format(d_id))
                            else: continue
                        if all(disk["name"] != d.disk_name
                               for d in vm.data_virtual_hard_disks.data_virtual_hard_disks):
                            res.disks_ok = False
                            res.messages.append("disk {0} is detached".format(d_id))
                            try:
                                self.sms().get_disk(disk["name"])
                            except azure.WindowsAzureMissingResourceError:
                                res.messages.append("disk {0} is destroyed".format(d_id))

                    self.handle_changed_property('public_ipv4', self.fetch_PIP())
                    self.update_ssh_known_hosts()

                    MachineState._check(self, res)

    def get_physical_spec(self):
        block_device_mapping = {}
        for d_id, disk in self.block_device_mapping.items():
            if (disk.get('encrypt', False)
                and disk.get('passphrase', "") == ""
                and disk.get('generatedKey', "") != ""):
                block_device_mapping[disk["device"]] = {
                    'passphrase': Function("pkgs.lib.mkOverride 10",
                                           disk['generatedKey'], call=True),
                }
        return {
            'require': [
                RawValue("<nixpkgs/nixos/modules/virtualisation/azure-common.nix>")
            ],
            ('deployment', 'azure', 'blockDeviceMapping'): block_device_mapping,
        }

    def get_keys(self):
        keys = MachineState.get_keys(self)
        # Ugly: we have to add the generated keys because they're not
        # there in the first evaluation (though they are present in
        # the final nix-build).
        for d_id, disk in self.block_device_mapping.items():
            if disk.get('encrypt', False) and disk.get('passphrase', "") == "" and disk.get('generatedKey', "") != "":
                key_name = disk["ephemeral_name"] if disk["ephemeral"] else disk["name"]
                keys[key_name] = {
                    'text': disk['generatedKey'],
                    'group': 'root',
                    'permissions': '0600',
                    'user': 'root'
                }
        return keys

    def create_after(self, resources, defn):
        from nixops.resources.azure_blob import AzureBLOBState
        from nixops.resources.azure_blob_container import AzureBLOBContainerState
        from nixops.resources.azure_disk import AzureDiskState
        from nixops.resources.azure_deployment import AzureDeploymentState
        from nixops.resources.azure_storage import AzureStorageState
        from nixops.resources.azure_reserved_ip_address import AzureReservedIPAddressState
        from nixops.resources.azure_hosted_service import AzureHostedServiceState
        return {r for r in resources
                  if isinstance(r, AzureBLOBContainerState) or isinstance(r, AzureStorageState) or
                     isinstance(r, AzureBLOBState) or isinstance(r, AzureReservedIPAddressState) or 
                     isinstance(r, AzureHostedServiceState) or isinstance(r, AzureDeploymentState) or
                     isinstance(r, AzureDiskState)}

    def find_ssh_endpoint(self):
        return next((ie for ie in self.input_endpoints
                        if ie.get('local_port', 0) == super(AzureState, self).ssh_port), None)

    def get_deployment_IP(self):
        deployment_resource = (self.deployment and
                               next((r for r in self.depl.resources.values()
                                       if getattr(r, 'deployment_name', None) == self.deployment), None))
        return (deployment_resource and deployment_resource.public_ipv4)

    def get_ssh_host_port(self):
        if self.public_ipv4:
            return self.public_ipv4
        ep = self.find_ssh_endpoint()
        ip = self.get_deployment_IP()
        if ip is not None and ep is not None and ep.get('port', None) is not None:
            return "[{0}]:{1}".format(ip, ep.get('port'))
        else:
            return None

    @MachineState.ssh_port.getter
    def ssh_port(self):
        if self.public_ipv4:
            return super(AzureState, self).ssh_port
        else:
            ep = self.find_ssh_endpoint()
            return (ep is not None and ep.get('port', None))

    def update_ssh_known_hosts(self):
        ssh_host_port = self.get_ssh_host_port()
        if ssh_host_port:
            known_hosts.add(ssh_host_port, self.public_host_key)

    def get_ssh_name(self):
        ip = self.public_ipv4 or (self.find_ssh_endpoint() and self.get_deployment_IP())
        if ip is None:
            raise Exception("{0} does not have a public IPv4 address and is not reachable "
                            "via an input endpoint on its deployment IP address"
                            .format(self.full_name))
        return ip

    def get_ssh_private_key_file(self):
        return self._ssh_private_key_file or self.write_ssh_private_key(self.private_client_key)

    def get_ssh_flags(self, scp=False):
        return [ "-i", self.get_ssh_private_key_file() ] + super(AzureState, self).get_ssh_flags(scp = scp)
