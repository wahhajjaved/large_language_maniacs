# -- read MARC21 records, yielding dictionary representations of Open Library items

from sys import stdin
from types import *
from string import strip, join
from unicodedata import normalize
from urllib import urlencode
from os import getenv

from MARC21 import *
from MARC21Biblio import *
from catalog.lang import *
from catalog.schema import schema

def warn (msg):
	sys.stderr.write ("%s complained:\n  %s\n" % (curr_loc, msg))


discard_extra_values = getenv ("PHAROS_DISCARD_EXTRA_VALUES")

record_id_delimiter = ":"
record_loc_delimiter = ":"

marc_value_generators = {}

def parser (source_id, file_locator, input):
    if (source_id.find (record_id_delimiter) >= 0):
        die ("the source id '%s' contains the record-id delimiter '%s'" % (source_id, record_id_delimiter))
    if (file_locator.find (record_loc_delimiter) >= 0):
        die ("the file locator '%s' contains the record-locator delimiter '%s'" % (file_locator, record_loc_delimiter))

    f = MARC21BiblioFile (input)
    try:
        while True:
            try:
                record = f.next()
                item = distill_record (record, file_locator, source_id)
                yield item
            except MARC21Exn, e:
                warn ("couldn't parse record: %s" % e)
    except StopIteration:
        pass

def encode_record_locator (r, file_locator):
    return record_loc_delimiter.join ([file_locator, str (r.record_pos ()), str (r.record_len ())])

def urlencode_record_locator (r, file_locator):
    return urlencode ({ 'file': file_locator,
                        'offset': r.record_pos (),
                        'length': r.record_len () })

def distill_record (r, file_locator, source_id):
    global curr_loc
    edition = {}
    edition['source_record_loc'] = [encode_record_locator (r, file_locator)]
    curr_loc = edition['source_record_loc'][0]
    edition['source_record_id'] = [record_id_delimiter.join ([source_id,
                                                              strip (r.get_field_value ('003')),
                                                              strip (r.get_field_value ('001'))])]
    for (field_name, field_spec) in schema['edition'].iteritems ():
        multiple = (field_spec.get ('count', "single") == "multiple")
        field_values = []
        marc_value_generator = marc_value_generators.get (field_name)
        if marc_value_generator:
            field_values = [ normalize_string (s) for s in marc_value_generator (r) ]
            # remove duplicates:
            new_field_values = []
            for value in field_values:
                if value not in new_field_values:
                    new_field_values.append(value)
            field_values = new_field_values
        if (len (field_values) > 1 and not multiple):
            msg = "record [%s]: multiple values from MARC data for single-valued OL field '%s'" % (encode_record_locator (r, file_locator), field_name)
            if discard_extra_values:
                warn (msg + "; using first value")
                edition[field_name] = field_values[0]
            else:
                die (msg)
        if (len (field_values) > 0):
            edition[field_name] = (multiple and field_values) or field_values[0];
    return edition

def initialize_marc_value_generators ():
    for (field_name, field_spec) in schema['edition'].iteritems ():
        marc_specs = field_spec.get ('marc_fields')
        if marc_specs:
            if (type (marc_specs) != list):
                marc_specs = [marc_specs]
            marc_value_generators[field_name] = compile_marc_specs (marc_specs)

def compile_marc_specs (specs):
    generators = map (compile_marc_spec, specs)
    def generator (r):
        for g in generators:
            for v in g (r):
                yield v
    return generator

# term splitter
re_spaces = re.compile (r'\s+')

# terms
re_literal = re.compile (r'"([^"]*)"$')
re_field = re.compile (r'(\d[^:]+):(.*)$')
# any string not matching the above can be a procedure name

def compile_marc_spec (spec):
    def spec_die (msg):
        die ("schema: in marc spec '%s': %s" % (spec, msg))

    # XX: this very basic parsing prevents even literals from containing spaces
    terms = [ t for t in re_spaces.split (spec) if t ]

    vals = []   # a stack of value generators
    def push (v):
        vals[0:0] = [v]
    def pop (n=1):
        vv = vals[0:n]
        del vals[0:n]
        return vv

    # consume terms from the spec, compiling them to value-generators and
    #    pushing those onto the stack
    # procedures, according to how many arguments they take,
    #    pop value-generators from the stack as "arguments" and will later be
    #    applied to the values generated by the arguments
    def handle_literal (m):
        push (literal_generator (m))
    def handle_field (m):
        push (field_generator (m))
    def handle_procedure (name):
        proc = compile_procedure (name)
        if proc:
            func = proc['func']
            nargs = proc['nargs']
            if (len (vals) < nargs):
                spec_die ("procedure '%s' expects %d arguments but only %d remain" %
                          (name, nargs, len (vals)))
            args = pop (nargs)
            push (call_generator (func, args))
        else:
            spec_die ("unknown procedure '%s'" % name)

    for term in terms:
        re_choose ([(re_literal, handle_literal),
                    (re_field,   handle_field),
                    (None,       handle_procedure)],
                   term)

    if len (vals) > 1:
        spec_die ("there are too many values here")
    if len (vals) < 1:
        spec_die ("there are no values to produce here")
    value_generator = vals[0]
    return value_generator

def null_generator (r):
    if 0: yield None

def literal_generator (m):
    s = m.group (1)
    def gen (r):
        yield s
    return gen

procedures = {
    # name : (nargs, function)
    '+': (2, (lambda s1, s2: s1 + s2))
    # more declared below ...
    }

def compile_procedure (name):
    info = procedures.get (name)
    if info:
        return { 'nargs': info[0], 'func': info[1] }
    else:
        return None

def call_generator (f, arg_generators):
    def value_generator (r):
        def generate_arglists (arg_gens):
            # generate the cross-product of the values generated by each arg_generator
            if (len (arg_gens) == 1):
                for a in arg_gens[0](r):
                    yield [a]
            elif (len (arg_gens) > 1):
                rest_lists = list (generate_arglists (arg_gens[1:]))  # don't re-generate these values
                for a in arg_gens[0](r):
                    for rest_list in rest_lists:
                        yield [a] + rest_list
        for arglist in generate_arglists (arg_generators):
            arglist.reverse ()
            val = f (*arglist)
            if val is not None:
                yield val
    return value_generator

# characters in a control field
re_positions = re.compile (r'(\d+)-(\d+)$')

# multiple subfields, concatenated
re_subfields_range = re.compile (r'([a-z])-([a-z])')
re_subfield_exact = re.compile (r'[a-z]')
re_subfield_prefixed = re.compile (r'--([a-z])')

# each instance of a subfield yielded separately (must be alone in a field-spec)
re_subfield_separate = re.compile (r'([a-z])\*$')

def field_generator (m):
    # warn (">>>> '%s'" % m.group (0))
    field_spec = m.group (1)
    subfield_spec = m.group (2)

    field_names = fieldname_generator (field_spec)
    subfield_values = subfield_value_generator (subfield_spec)

    def gen (r):
        for field_name in field_names (r):
            for field in r.get_fields (field_name):
                for val in subfield_values (field):
                    yield val
    return gen

re_field_separator = re.compile (r',')
re_field_exclude = re.compile (r'\!(\d\d\d)')
re_field_range = re.compile (r'(\d\d\d)-(\d\d\d)')
re_field_exact = re.compile (r'\d\d\d')

def fieldname_generator (field_spec):
    excluded = {}
    def handle_excluded_field (m):
        field_name = m.group (1)
        excluded[field_name] = 1
        return None
    def handle_field_range (m):
        lo = int (m.group (1))
        hi = int (m.group (2))
        # warn ("field-range [%d,%d]" % (lo,hi))
        def gen (r):
            for field_name in r.fields ():
                field_n = int (field_name)
                if lo <= field_n <= hi:
                    yield str (field_n)
        return gen
    def handle_exact_field (m):
        field_name = m.group (0)
        # warn ("field '%s'" % field_name)
        def gen (r):
            yield field_name
        return gen

    fieldname_generators = filter (lambda x: x,
                                   re_tokenize ([(re_field_range, handle_field_range),
                                                 (re_field_exact, handle_exact_field),
                                                 (re_field_exclude, handle_excluded_field),
                                                 (re_field_separator, lambda m: None)],
                                                field_spec))
    # if len (excluded):
    #    warn ("excluded: %s" % excluded)

    def gen (r):
        for fg in fieldname_generators:
            for field_name in fg (r):
                if not excluded.get (field_name):
                    yield field_name
    return gen

def subfield_value_generator (subfield_spec):

    # warn ("subfield_generator: '%s'" % subfield_spec)

    def subfield_die (msg):
        die ("schema: in subfield spec '%s': %s" % (subfield_spec, msg))

    def handle_code_chars (m):
        start = int (m.group (1))
        end = int (m.group (2))
        assert start <= end
        lim = end+1
        # warn ("chars: [%d,%d)" % (start, lim))
        def gen (field):
            yield str(field)[start:lim]
        return gen

    def handle_concatenated_subfields (subfield_spec):
        # concatenate all indicated subfields into one string per field instance

        def range_subfields (m):
            low = m.group (1)
            hi = m.group (2)
            if low > hi:
                subfield_die ("range is ill-formed")
            # warn ("range: (%s-%s)" % (low, hi))
            def gen (field):
                for subfield_name in field.subfields ():
                    if (subfield_name >= low and subfield_name <= hi):
                        for subfield in clean_subfields (field, subfield_name):
                            yield subfield
            return gen

        def exact_subfields (m):
            subfield_name = m.group (0)
            # warn ("exact: '%s'" % subfield_name)
            def gen (field):
                for subfield in clean_subfields (field, subfield_name):
                    yield subfield
            return gen

        def prefixed_subfields (m):
            subfield_name = m.group (1)
            # warn ("prefixed: '%s'" % subfield_name)
            def gen (field):
                for subfield in clean_subfields (field, subfield_name):
                    yield ("-- " + subfield)
            return gen

        # parse the subfield_spec into a list of subfield-value generators
        # (note that the order of REs provided is important -- e.g. range before exact)
        subfield_generators = list (re_tokenize ([(re_subfields_range, range_subfields),
                                                  (re_subfield_exact, exact_subfields),
                                                  (re_subfield_prefixed, prefixed_subfields)],
                                                 subfield_spec))

        # given a field object, extract all the subfield values requested
        def subfields_generator (field):
            for sfg in subfield_generators:
                for sf in sfg (field):
                    yield sf

        def gen (field):
            # for each field object, concatenate all requested subfield values into one string
            yield " ".join ([ sf for sf in subfields_generator(field) ])

        return gen

    def handle_separate_subfields (m):
        subfield_name = m.group (1)
        # warn ("separate: %s" % subfield_name)
        def gen (field):
            # produce a value for each individual subfield occurence
            for subfield in clean_subfields (field, subfield_name):
                yield subfield
        return gen

    return re_choose ([(re_positions, handle_code_chars),
                       (re_subfield_separate, handle_separate_subfields),
                       (None, handle_concatenated_subfields)],
                      subfield_spec)

def clean_subfields (field, subfield_name):
    for subfield in field.get_elts (subfield_name):
        subfield = strip (subfield)
        if subfield:
            yield subfield

def unicode_to_utf8 (u):
    nu = normalize ('NFKC', u)
    return nu.encode ('utf8')

def normalize_string (s):
    if (type (s) is StringType):
        return s
    elif (type (s) is UnicodeType):
        return unicode_to_utf8 (s)
    else:
        die ("normalize_string called on non-string: %s" % s)

def re_tokenize (clauses, target, pos=0):
    tlen = len (target)
    while pos < tlen:
        m = None
        for (pattern, handler) in clauses:
            m = pattern.match (target, pos)
            if m: break
        if m:
            pos = m.end ()
            yield (handler (m))
        else:
            die ("re_tokenize: no match at position %d for target '%s'" % (pos, target))

def re_choose (clauses, target):
    for (pattern, handler) in clauses:
        if not pattern:
            return handler (target)
        m = pattern.match (target)
        if m:
            return handler (m)

### authors

re_dates = re.compile (r'(\d{4})-(\d{4})?$')

def author (self):
    a = None
    pn = self.get_field ("100")
    if pn:
        name = pn.get_elt ("a", None)
        if name:
            name = clean_name (name)
            a = { 'name': name }
            dates = pn.get_elt ("d", None)
            if dates:
                m = re_dates.search (dates)
                if m:
                    a["birth_date"] = m.group (1)
                    if m.group (2):
                        a["death_date"] = m.group (2)
    else:
        ts = self.title_statement ()
        name = clean (join (ts.get_elts ("c"), ", "))
        if name:
            a = { 'name': name }
    return a

def authors (self):
    a = self.author ()
    if a: return [a]
    else: return None

### filters, referenced from the schema

re_isbn_chars = re.compile ('([- \dX]+)')
re_not_isbn_chars = re.compile ('[^\dX]')

def clean (s):
    return strip (s, " /.,;:")

procedures['clean'] = (1, clean)

def clean_name (s):
    return strip (s, " /,;:")

procedures['clean_name'] = (1, clean_name)

def normalize_isbn (s):
    m = re_isbn_chars.search(s)
    if m:
        return re_not_isbn_chars.sub('', m.group(1))
    else:
        return None

procedures['normalize_isbn'] = (1, normalize_isbn)

re_digits = re.compile (r'\d\d+')

def biggest_decimal (s):
    nums = re_digits.findall (s)
    if (len (nums) > 0):
        return max (nums)
    else:
        return None

procedures['biggest_decimal'] = (1, biggest_decimal)

re_lccn = re.compile (r'(...\d+).*')

def normalize_lccn (s):
    m = re_lccn.match (s)
    if m:
        return m.group (1)
    else:
        warn ("bad LCCN: '%s'" % s)
        return None

procedures['normalize_lccn'] = (1, normalize_lccn)

re_author_id_parts = re.compile (r'\w+', re.UNICODE)

def author_id (s):
    parts = [ p.lower() for p in re_author_id_parts.findall (s) ]
    return " ".join (parts)

procedures['author_id'] = (1, author_id)

## check for language code field?
#
#       if lang == "|||":
#           if len(self.get_fields ("041")) > 0:
#               self.marc21_record.err ("has LANGUAGE CODE field")

## is "edition number" important for dewey decimal classification?
#
#   def dewey_decimal_class (self):
#       classes = []
#       for ddcn in self.get_fields ("082"):
#           edition_number = ddcn.get_elt ("2", "?")
#           classification_numbers = ddcn.get_elts ("a")
#           classes.extend ([ "%s:%s"%(edition_number,cn) for cn in classification_numbers ])
#       return classes

initialize_marc_value_generators ()

if __name__ == "__main__":
    if len (sys.argv) == 3:
        source_id = sys.argv[1]
        file_locator = sys.argv[2]
        for item in parser (source_id, file_locator, sys.stdin):
            print ""
            print ">>> " + item['source_record_loc'][0]
            print item
    else:
        die ("usage: parse.py source_id file_locator")
