{"input": "def _build_werkzeug_request_data(request):\n    request_data = {\n        'url': request.url,\n        'GET': dict(request.args),\n        'POST': dict(request.form),\n        'user_ip': _extract_user_ip(request),\n        'headers': dict(request.headers),\n        'method': request.method,\n        'files_keys': request.files.keys(),\n    }\n\n    if request.get_json():\n        request_data['body'] = _scrub_request_params(request.json)\n\n    return request_data", "output": "def _build_werkzeug_request_data(request):\n    request_data = {\n        'url': request.url,\n        'GET': dict(request.args),\n        'POST': dict(request.form),\n        'user_ip': _extract_user_ip(request),\n        'headers': dict(request.headers),\n        'method': request.method,\n        'files_keys': request.files.keys(),\n    }\n\n    if request.get_json():\n        request_data['body'] = json.dumps(_scrub_request_params(request.json))\n\n    return request_data", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef _build_werkzeug_request_data(request):\n    request_data = {\n        'url': request.url,\n        'GET': dict(request.args),\n        'POST': dict(request.form),\n        'user_ip': _extract_user_ip(request),\n        'headers': dict(request.headers),\n        'method': request.method,\n        'files_keys': request.files.keys(),\n    }\n\n    if request.get_json():\n        request_data['body'] = _scrub_request_params(request.json)\n\n    return request_data\n### Response:\nThe provided code snippet is a Python function named `_build_werkzeug_request_data` that constructs a dictionary (`request_data`) containing various details about an HTTP request made using the Werkzeug library in a web framework. This function is typically used in web applications to log or analyze incoming requests.\n\nHere's a breakdown of what the function does:\n\n1. **URL**: It captures the full URL of the request.\n2. **GET Parameters**: It captures all GET parameters in the URL.\n3. **POST Parameters**: It captures all POST parameters sent via the request body.\n4. **User IP**: It extracts the user's IP address.\n5. **Headers**: It captures all HTTP headers sent with the request.\n6. **Method**: It captures the HTTP method (GET, POST, etc.) used in the request.\n7. **Files Keys**: It captures the keys of files uploaded via the request (if any).\n8. **Body (JSON)**: If the request contains a JSON body, it captures and cleanses this data.\n\nThe function also includes a helper function `_extract_user_ip` to extract the user's IP address and a helper function `_scrub_request_params` to clean the JSON body.\n\n### Example Usage:\n```python\nfrom werkzeug.test import Client\nfrom werkzeug.wrappers import BaseResponse\n\ndef simple_app(environ, start_response):\n    request = Request(environ)\n    response = BaseResponse(\"Hello, World!\")\n    return response(environ, start_response)\n\nclient = Client(simple_app, BaseResponse)\n\n# Make a request\nresponse = client.get('/test?foo=bar')\n\n# Log the request data\nrequest_data = _build_werkzeug_request_data(response.request)\nprint(request_data)\n```\n\n### Output:\n```python\n{\n    'url': 'http://localhost/test?foo=bar',\n    'GET': {'foo': 'bar'},\n    'POST': {},\n    'user_ip': '127.0.0.1',  # Example IP, actual IP depends on the environment\n    'headers': {...},  # Example headers, actual headers depend on the request\n    'method': 'GET',\n    'files_keys': [],\n    'body': {...}  # Example body, actual body depends on the request\n}\n```\n\n### Note:\n- The `_extract_user_ip` and `_scrub_request_params` functions are not provided here, so their behavior is assumed to be defined elsewhere in your codebase.\n- The `user_ip` and `body` fields will depend on the actual implementation of the helper functions.\n- The `files_keys` field will be an empty list unless files are uploaded via the request.\n- The `headers` and `body` fields will contain example data, as their actual values depend on the specific request made."}
{"input": "def pitch_tuning(frequencies, resolution=0.01, bins_per_octave=12):\n    '''Given a collection of pitches, estimate its tuning offset\n    (in fractions of a bin) relative to A440=440.0Hz.\n\n    Parameters\n    ----------\n    frequencies : array-like, float\n        A collection of frequencies detected in the signal.\n        See `piptrack`\n\n    resolution : float in `(0, 1)`\n        Resolution of the tuning as a fraction of a bin.\n        0.01 corresponds to cents.\n\n    bins_per_octave : int > 0 [scalar]\n        How many frequency bins per octave\n\n    Returns\n    -------\n    tuning: float in `[-0.5, 0.5)`\n        estimated tuning deviation (fractions of a bin)\n\n    See Also\n    --------\n    estimate_tuning\n        Estimating tuning from time-series or spectrogram input\n\n    Examples\n    --------\n    >>> # Generate notes at +25 cents\n    >>> freqs = librosa.cqt_frequencies(24, 55, tuning=0.25)\n    >>> librosa.pitch_tuning(freqs)\n    0.25\n\n    >>> # Track frequencies from a real spectrogram\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> pitches, magnitudes, stft = librosa.ifptrack(y, sr)\n    >>> # Select out pitches with high energy\n    >>> pitches = pitches[magnitudes > np.median(magnitudes)]\n    >>> librosa.pitch_tuning(pitches)\n    0.089999999999999969\n\n    '''\n\n    frequencies = np.atleast_1d(frequencies)\n\n    # Trim out any DC components\n    frequencies = frequencies[frequencies > 0]\n\n    if not np.any(frequencies):\n        warnings.warn('Trying to estimate tuning from empty frequency set.')\n        return 0.0\n\n    # Compute the residual relative to the number of bins\n    residual = np.mod(bins_per_octave *\n                      time_frequency.hz_to_octs(frequencies), 1.0)\n\n    # Are we on the wrong side of the semitone?\n    # A residual of 0.95 is more likely to be a deviation of -0.05\n    # from the next tone up.\n    residual[residual >= 0.5] -= 1.0\n\n    bins = np.linspace(-0.5, 0.5, np.ceil(1./resolution), endpoint=False)\n\n    counts, tuning = np.histogram(residual, bins)\n\n    # return the histogram peak\n    return tuning[np.argmax(counts)]", "output": "def pitch_tuning(frequencies, resolution=0.01, bins_per_octave=12):\n    '''Given a collection of pitches, estimate its tuning offset\n    (in fractions of a bin) relative to A440=440.0Hz.\n\n    Parameters\n    ----------\n    frequencies : array-like, float\n        A collection of frequencies detected in the signal.\n        See `piptrack`\n\n    resolution : float in `(0, 1)`\n        Resolution of the tuning as a fraction of a bin.\n        0.01 corresponds to cents.\n\n    bins_per_octave : int > 0 [scalar]\n        How many frequency bins per octave\n\n    Returns\n    -------\n    tuning: float in `[-0.5, 0.5)`\n        estimated tuning deviation (fractions of a bin)\n\n    See Also\n    --------\n    estimate_tuning\n        Estimating tuning from time-series or spectrogram input\n\n    Examples\n    --------\n    >>> # Generate notes at +25 cents\n    >>> freqs = librosa.cqt_frequencies(24, 55, tuning=0.25)\n    >>> librosa.pitch_tuning(freqs)\n    0.25\n\n    >>> # Track frequencies from a real spectrogram\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> pitches, magnitudes, stft = librosa.ifptrack(y, sr)\n    >>> # Select out pitches with high energy\n    >>> pitches = pitches[magnitudes > np.median(magnitudes)]\n    >>> librosa.pitch_tuning(pitches)\n    0.089999999999999969\n\n    '''\n\n    frequencies = np.atleast_1d(frequencies)\n\n    # Trim out any DC components\n    frequencies = frequencies[frequencies > 0]\n\n    if not np.any(frequencies):\n        warnings.warn('Trying to estimate tuning from empty frequency set.')\n        return 0.0\n\n    # Compute the residual relative to the number of bins\n    residual = np.mod(bins_per_octave *\n                      time_frequency.hz_to_octs(frequencies), 1.0)\n\n    # Are we on the wrong side of the semitone?\n    # A residual of 0.95 is more likely to be a deviation of -0.05\n    # from the next tone up.\n    residual[residual >= 0.5] -= 1.0\n\n    bins = np.linspace(-0.5, 0.5, int(np.ceil(1./resolution)), endpoint=False)\n\n    counts, tuning = np.histogram(residual, bins)\n\n    # return the histogram peak\n    return tuning[np.argmax(counts)]", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef pitch_tuning(frequencies, resolution=0.01, bins_per_octave=12):\n    '''Given a collection of pitches, estimate its tuning offset\n    (in fractions of a bin) relative to A440=440.0Hz.\n\n    Parameters\n    ----------\n    frequencies : array-like, float\n        A collection of frequencies detected in the signal.\n        See `piptrack`\n\n    resolution : float in `(0, 1)`\n        Resolution of the tuning as a fraction of a bin.\n        0.01 corresponds to cents.\n\n    bins_per_octave : int > 0 [scalar]\n        How many frequency bins per octave\n\n    Returns\n    -------\n    tuning: float in `[-0.5, 0.5)`\n        estimated tuning deviation (fractions of a bin)\n\n    See Also\n    --------\n    estimate_tuning\n        Estimating tuning from time-series or spectrogram input\n\n    Examples\n    --------\n    >>> # Generate notes at +25 cents\n    >>> freqs = librosa.cqt_frequencies(24, 55, tuning=0.25)\n    >>> librosa.pitch_tuning(freqs)\n    0.25\n\n    >>> # Track frequencies from a real spectrogram\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> pitches, magnitudes, stft = librosa.ifptrack(y, sr)\n    >>> # Select out pitches with high energy\n    >>> pitches = pitches[magnitudes > np.median(magnitudes)]\n    >>> librosa.pitch_tuning(pitches)\n    0.089999999999999969\n\n    '''\n\n    frequencies = np.atleast_1d(frequencies)\n\n    # Trim out any DC components\n    frequencies = frequencies[frequencies > 0]\n\n    if not np.any(frequencies):\n        warnings.warn('Trying to estimate tuning from empty frequency set.')\n        return 0.0\n\n    # Compute the residual relative to the number of bins\n    residual = np.mod(bins_per_octave *\n                      time_frequency.hz_to_octs(frequencies), 1.0)\n\n    # Are we on the wrong side of the semitone?\n    # A residual of 0.95 is more likely to be a deviation of -0.05\n    # from the next tone up.\n    residual[residual >= 0.5] -= 1.0\n\n    bins = np.linspace(-0.5, 0.5, np.ceil(1./resolution), endpoint=False)\n\n    counts, tuning = np.histogram(residual, bins)\n\n    # return the histogram peak\n    return tuning[np.argmax(counts)]\n### Response:\nThe function `pitch_tuning` is designed to estimate the tuning deviation of a given collection of pitches relative to a reference frequency, typically A440 (440.0 Hz). This function calculates the deviation in fractions of a bin, which can be interpreted as cents (0.01 bins) or other units depending on the resolution specified.\n\nHere's a breakdown of the function's parameters and steps:\n\n1. **Parameters**:\n   - `frequencies`: A collection of frequencies detected in the signal. This could be from a pitch tracking algorithm like `piptrack` or from a constant-Q transform (CQT).\n   - `resolution`: The resolution of the tuning as a fraction of a bin. A common value is 0.01, which corresponds to cents.\n   - `bins_per_octave`: The number of frequency bins per octave.\n\n2. **Steps**:\n   - **Trim DC components**: Remove any frequencies that are zero or negative.\n   - **Compute residual**: Convert the frequencies to the logarithmic scale of octaves and compute the residual relative to the number of bins.\n   - **Adjust residual**: If the residual is 0.5 or more, subtract 1.0 to correct the deviation.\n   - **Create bins**: Define the bins for the histogram, ranging from -0.5 to 0.5.\n   - **Compute histogram**: Calculate the histogram of the residuals.\n   - **Return peak**: Return the tuning value corresponding to the peak of the histogram.\n\n### Example Usage:\n- **Generating notes at +25 cents**:\n  ```python\n  freqs = librosa.cqt_frequencies(24, 55, tuning=0.25)\n  librosa.pitch_tuning(freqs)\n  ```\n  This should return approximately `0.25`, indicating a tuning deviation of 25 cents.\n\n- **Tracking frequencies from a real spectrogram**:\n  ```python\n  y, sr = librosa.load(librosa.util.example_audio_file())\n  pitches, magnitudes, stft = librosa.ifptrack(y, sr)\n  pitches = pitches[magnitudes > np.median(magnitudes)]\n  librosa.pitch_tuning(pitches)\n  ```\n  This will return a value indicating the tuning deviation of the tracked pitches.\n\n### See Also:\n- `estimate_tuning`: This function is used to estimate tuning from time-series or spectrogram input, providing a more comprehensive approach to tuning estimation.\n\n### Examples:\n- The provided examples demonstrate how to use `pitch_tuning` to estimate tuning from both synthetic data and real audio signals.\n\nOverall, `pitch_tuning` is a useful function for analyzing the tuning of musical pitches, providing a quick estimate of the deviation relative to a reference frequency."}
{"input": "def mouseMoveEvent(self, ev):\n        if self.lastMousePos is None:\n            self.lastMousePos = Point(ev.pos())\n        delta = Point(ev.pos() - self.lastMousePos)\n        self.lastMousePos = Point(ev.pos())\n\n        QtGui.QGraphicsView.mouseMoveEvent(self, ev)\n        if not self.mouseEnabled:\n            return\n        self.sigSceneMouseMoved.emit(self.mapToScene(ev.pos()))\n            \n        if self.clickAccepted:  ## Ignore event if an item in the scene has already claimed it.\n            return\n        \n        if ev.buttons() == QtCore.Qt.RightButton:\n            delta = Point(np.clip(delta[0], -50, 50), np.clip(-delta[1], -50, 50))\n            scale = 1.01 ** delta\n            self.scale(scale[0], scale[1], center=self.mapToScene(self.mousePressPos))\n            self.sigDeviceRangeChanged.emit(self, self.range)\n\n        elif ev.buttons() in [QtCore.Qt.MidButton, QtCore.Qt.LeftButton]:  ## Allow panning by left or mid button.\n            px = self.pixelSize()\n            tr = -delta * px\n            \n            self.translate(tr[0], tr[1])\n            self.sigDeviceRangeChanged.emit(self, self.range)", "output": "def mouseMoveEvent(self, ev):\n        if self.lastMousePos is None:\n            self.lastMousePos = Point(ev.pos())\n        delta = Point(Point(ev.pos()) - self.lastMousePos)\n        self.lastMousePos = Point(ev.pos())\n\n        QtGui.QGraphicsView.mouseMoveEvent(self, ev)\n        if not self.mouseEnabled:\n            return\n        self.sigSceneMouseMoved.emit(self.mapToScene(ev.pos()))\n            \n        if self.clickAccepted:  ## Ignore event if an item in the scene has already claimed it.\n            return\n        \n        if ev.buttons() == QtCore.Qt.RightButton:\n            delta = Point(np.clip(delta[0], -50, 50), np.clip(-delta[1], -50, 50))\n            scale = 1.01 ** delta\n            self.scale(scale[0], scale[1], center=self.mapToScene(self.mousePressPos))\n            self.sigDeviceRangeChanged.emit(self, self.range)\n\n        elif ev.buttons() in [QtCore.Qt.MidButton, QtCore.Qt.LeftButton]:  ## Allow panning by left or mid button.\n            px = self.pixelSize()\n            tr = -delta * px\n            \n            self.translate(tr[0], tr[1])\n            self.sigDeviceRangeChanged.emit(self, self.range)", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef mouseMoveEvent(self, ev):\n        if self.lastMousePos is None:\n            self.lastMousePos = Point(ev.pos())\n        delta = Point(ev.pos() - self.lastMousePos)\n        self.lastMousePos = Point(ev.pos())\n\n        QtGui.QGraphicsView.mouseMoveEvent(self, ev)\n        if not self.mouseEnabled:\n            return\n        self.sigSceneMouseMoved.emit(self.mapToScene(ev.pos()))\n            \n        if self.clickAccepted:  ## Ignore event if an item in the scene has already claimed it.\n            return\n        \n        if ev.buttons() == QtCore.Qt.RightButton:\n            delta = Point(np.clip(delta[0], -50, 50), np.clip(-delta[1], -50, 50))\n            scale = 1.01 ** delta\n            self.scale(scale[0], scale[1], center=self.mapToScene(self.mousePressPos))\n            self.sigDeviceRangeChanged.emit(self, self.range)\n\n        elif ev.buttons() in [QtCore.Qt.MidButton, QtCore.Qt.LeftButton]:  ## Allow panning by left or mid button.\n            px = self.pixelSize()\n            tr = -delta * px\n            \n            self.translate(tr[0], tr[1])\n            self.sigDeviceRangeChanged.emit(self, self.range)\n### Response:\nThe provided code snippet is a method `mouseMoveEvent` in a Python class, likely part of a graphical user interface application using PyQt or PySide. This method handles mouse movement events within a graphics view, adjusting the view's scale or panning the view based on the mouse button pressed. Here's a breakdown of the code:\n\n1. **Initialization and Mouse Position Tracking**:\n   - The method first checks if `self.lastMousePos` is `None`. If it is, it initializes `self.lastMousePos` with the current mouse position.\n   - It then calculates the difference (`delta`) between the current mouse position and the last recorded mouse position.\n   - Updates `self.lastMousePos` to the current mouse position.\n\n2. **Calling the Parent Class Method**:\n   - The method calls the parent class's `mouseMoveEvent` method (`QtGui.QGraphicsView.mouseMoveEvent(self, ev)`) to ensure that the default behavior is maintained.\n\n3. **Check for Mouse Events**:\n   - If `self.mouseEnabled` is `False`, the method returns early, ignoring the event.\n   - Emits a signal `self.sigSceneMouseMoved` with the scene position corresponding to the current mouse position.\n   - Checks if `self.clickAccepted` is `True`, which indicates that another item in the scene has already handled the event. If so, it returns early.\n\n4. **Handling Right Button Clicks**:\n   - If the right mouse button is pressed, it clips the `delta` values to a range of -50 to 50 and uses them to adjust the scale of the view. The scale is adjusted by a factor of `1.01` for each unit of `delta`.\n   - Emits a signal `self.sigDeviceRangeChanged` with the current range of the device.\n\n5. **Handling Left and Middle Button Clicks**:\n   - If the left or middle mouse button is pressed, it calculates the pixel size (`px`) and translates the view by `-delta * px`.\n   - Emits a signal `self.sigDeviceRangeChanged` with the current range of the device.\n\n### Summary:\n- **Purpose**: The method handles mouse movement events, adjusting the view's scale or panning based on the mouse button pressed.\n- **Key Functions**:\n  - Tracks mouse position changes.\n  - Calls the parent class's `mouseMoveEvent` to maintain default behavior.\n  - Emits signals based on the mouse event.\n  - Adjusts the view's scale or translates it based on the mouse button pressed.\n\nThis code snippet is part of a larger application, likely used to control or manipulate a graphical representation within a window, possibly in a CAD tool, a map application, or a similar context where mouse interactions are crucial for manipulating the view."}
{"input": "def work_beacon_context(self, beaconworkstream, input_streams, user_id):\n        \"\"\"\n        produces datapoint sample as 1 or 2 if around work beacons else 0\n        :param beaconworkstream:\n        :param input_streams:\n        :param user_id:\n        :return: stream with (start_time,end_time,offset,sample= 0 or 1]\n        based on context of work_beacon 1 or work_beacon 2\n        \"\"\"\n        if (len(beaconworkstream) > 0):\n            beaconstream = beaconworkstream\n\n            windowed_data = window(beaconstream, self.window_size, True)\n\n            new_data = []\n            for i, j in windowed_data:\n                if (len(windowed_data[i, j]) > 0):\n                    values = []\n                    for items in windowed_data[i, j]:\n                        values.append(items.sample)\n\n                    if ('1' in items.sample) & ('2' in items.sample):\n                        windowed_data[i, j] = 1\n                    else:\n                        windowed_data[i, j] = values[0]\n\n                else:\n                    windowed_data[i, j] = 0\n\n            data = merge_consective_windows(windowed_data)\n            for items in data:\n                if items.sample is not None and items.sample!=\"\":\n                    new_data.append(DataPoint(start_time=items.start_time,\n                                              end_time=items.end_time,\n                                              offset=beaconworkstream[0].offset,\n                                              sample=items.sample))\n\n            try:\n                \n                \n                self.store_stream(filepath=\"work_beacon_context.json\",\n                                  input_streams= input_streams,\n                                  user_id=user_id,\n                                  data=new_data, localtime=True)\n                self.CC.logging.log('%s %s work_beacon_context stored %d '\n                                    'DataPoints for user %s ' \n                                    % (str(datetime.datetime.now()),\n                                       self.__class__.__name__,\n                                       len(new_data), str(new_data)))\n\n            except Exception as e:\n                self.CC.logging.log(\"Exception:\", str(e))\n                self.CC.logging.log(str(traceback.format_exc()))\n        else:\n            self.CC.logging.log(\"No work beacon streams found for user %s\"%\n                                 str(user_id))", "output": "def work_beacon_context(self, beaconworkstream, input_streams, user_id):\n        \"\"\"\n        produces datapoint sample as 1 or 2 if around work beacons else 0\n        :param beaconworkstream:\n        :param input_streams:\n        :param user_id:\n        :return: stream with (start_time,end_time,offset,sample= 0 or 1]\n        based on context of work_beacon 1 or work_beacon 2\n        \"\"\"\n        if (len(beaconworkstream) > 0):\n            beaconstream = beaconworkstream\n\n            windowed_data = window(beaconstream, self.window_size, True)\n\n            new_data = []\n            for i, j in windowed_data:\n                if (len(windowed_data[i, j]) > 0):\n                    values = []\n                    for items in windowed_data[i, j]:\n                        values.append(items.sample)\n\n                    if ('1' in items.sample) & ('2' in items.sample):\n                        windowed_data[i, j] = 1\n                    else:\n                        windowed_data[i, j] = int(values[0])\n\n                else:\n                    windowed_data[i, j] = 0\n\n            data = merge_consective_windows(windowed_data)\n            for items in data:\n                if items.sample is not None and items.sample!=\"\":\n                    new_data.append(DataPoint(start_time=items.start_time,\n                                              end_time=items.end_time,\n                                              offset=beaconworkstream[0].offset,\n                                              sample=items.sample))\n\n            try:\n                \n                \n                self.store_stream(filepath=\"work_beacon_context.json\",\n                                  input_streams= input_streams,\n                                  user_id=user_id,\n                                  data=new_data, localtime=True)\n                self.CC.logging.log('%s %s work_beacon_context stored %d '\n                                    'DataPoints for user %s ' \n                                    % (str(datetime.datetime.now()),\n                                       self.__class__.__name__,\n                                       len(new_data), str(new_data)))\n\n            except Exception as e:\n                self.CC.logging.log(\"Exception:\", str(e))\n                self.CC.logging.log(str(traceback.format_exc()))\n        else:\n            self.CC.logging.log(\"No work beacon streams found for user %s\"%\n                                 str(user_id))", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef work_beacon_context(self, beaconworkstream, input_streams, user_id):\n        \"\"\"\n        produces datapoint sample as 1 or 2 if around work beacons else 0\n        :param beaconworkstream:\n        :param input_streams:\n        :param user_id:\n        :return: stream with (start_time,end_time,offset,sample= 0 or 1]\n        based on context of work_beacon 1 or work_beacon 2\n        \"\"\"\n        if (len(beaconworkstream) > 0):\n            beaconstream = beaconworkstream\n\n            windowed_data = window(beaconstream, self.window_size, True)\n\n            new_data = []\n            for i, j in windowed_data:\n                if (len(windowed_data[i, j]) > 0):\n                    values = []\n                    for items in windowed_data[i, j]:\n                        values.append(items.sample)\n\n                    if ('1' in items.sample) & ('2' in items.sample):\n                        windowed_data[i, j] = 1\n                    else:\n                        windowed_data[i, j] = values[0]\n\n                else:\n                    windowed_data[i, j] = 0\n\n            data = merge_consective_windows(windowed_data)\n            for items in data:\n                if items.sample is not None and items.sample!=\"\":\n                    new_data.append(DataPoint(start_time=items.start_time,\n                                              end_time=items.end_time,\n                                              offset=beaconworkstream[0].offset,\n                                              sample=items.sample))\n\n            try:\n                \n                \n                self.store_stream(filepath=\"work_beacon_context.json\",\n                                  input_streams= input_streams,\n                                  user_id=user_id,\n                                  data=new_data, localtime=True)\n                self.CC.logging.log('%s %s work_beacon_context stored %d '\n                                    'DataPoints for user %s ' \n                                    % (str(datetime.datetime.now()),\n                                       self.__class__.__name__,\n                                       len(new_data), str(new_data)))\n\n            except Exception as e:\n                self.CC.logging.log(\"Exception:\", str(e))\n                self.CC.logging.log(str(traceback.format_exc()))\n        else:\n            self.CC.logging.log(\"No work beacon streams found for user %s\"%\n                                 str(user_id))\n### Response:\nThe provided code snippet is a method definition for a class, likely part of a larger system designed to analyze and process data related to work beacons. The method `work_beacon_context` appears to be designed to analyze the context of work beacons for a specific user, based on the data streams provided. Here's a breakdown of what the method does:\n\n1. **Input Parameters**:\n   - `beaconworkstream`: A list of data points related to work beacons.\n   - `input_streams`: Additional input streams that might be used for context.\n   - `user_id`: Identifier for the user whose context is being analyzed.\n\n2. **Processing**:\n   - The method first checks if there are any beacons in the `beaconworkstream`.\n   - If there are beacons, it proceeds to process them.\n   - It uses a sliding window approach to analyze the data. The window size is defined by `self.window_size`.\n   - For each window, it checks if both '1' and '2' are present in the samples. If both are present, it assigns a sample value of 1; otherwise, it assigns the value of the first sample.\n   - If no samples are present in a window, it assigns a sample value of 0.\n   - The method then merges consecutive windows with the same sample value.\n\n3. **Output**:\n   - The method stores the processed data in a JSON file named `work_beacon_context.json` for the specified user.\n   - It logs the number of data points stored and any exceptions encountered during the process.\n\n4. **Error Handling**:\n   - If no beacon streams are found for the user, it logs a message indicating so.\n   - It catches and logs any exceptions that occur during the process.\n\n### Example Usage:\n```python\n# Assuming an instance of the class is created as `analyzer`\n# and `beaconworkstream`, `input_streams`, and `user_id` are defined\n# analyzer.work_beacon_context(beaconworkstream, input_streams, user_id)\n```\n\n### Considerations:\n- **Security and Privacy**: Ensure that the method handles user data securely, especially when dealing with personal information.\n- **Performance**: The method's performance might be affected by the size of the data and the window size. Consider optimizing the windowing and merging processes for better performance.\n- **Data Integrity**: Ensure that the data stored in the JSON file is accurate and does not contain any sensitive information.\n\nThis method is designed to analyze the context of work beacons and store the results, which could be used for various purposes such as user behavior analysis, anomaly detection, or performance monitoring."}
{"input": "def onChanged(mcs,obj,prop):\n        if prop == mcs._disabled:\n            obj.ViewObject.signalChangeIcon()\n        if super(Constraint,mcs).onChanged(obj,prop):\n            try:\n                if obj.Name==obj.Label or \\\n                   mcs.getType(utils.getLabel(obj)):\n                    obj.Label = mcs.getTypeName(obj)\n            except Exception as e:\n                logger.debug('auto constraint label failed: {}'.format(e))", "output": "def onChanged(mcs,obj,prop):\n        if prop == mcs._disabled:\n            obj.ViewObject.signalChangeIcon()\n        if super(Constraint,mcs).onChanged(obj,prop):\n            try:\n                if obj.Name==obj.Label or \\\n                   mcs.getType(str(utils.getLabel(obj))):\n                    obj.Label = mcs.getTypeName(obj)\n            except Exception as e:\n                logger.debug('auto constraint label failed: {}'.format(e))", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef onChanged(mcs,obj,prop):\n        if prop == mcs._disabled:\n            obj.ViewObject.signalChangeIcon()\n        if super(Constraint,mcs).onChanged(obj,prop):\n            try:\n                if obj.Name==obj.Label or \\\n                   mcs.getType(utils.getLabel(obj)):\n                    obj.Label = mcs.getTypeName(obj)\n            except Exception as e:\n                logger.debug('auto constraint label failed: {}'.format(e))\n### Response:\nThe code snippet you've provided appears to be a method implementation in Python, possibly for a GUI framework or a custom class. This method is designed to handle changes in properties of an object. Let's break down the code and understand its functionality:\n\n1. **Method Signature**: `onChanged(mcs, obj, prop)`\n   - `mcs`: This seems to be a reference to the class or module where this method is defined.\n   - `obj`: The object whose property is being changed.\n   - `prop`: The property of the object that is being changed.\n\n2. **First Condition**: `if prop == mcs._disabled:`\n   - This checks if the property being changed is the `_disabled` property of `mcs`.\n   - If true, it calls `obj.ViewObject.signalChangeIcon()`, which suggests that it might be triggering a change in the icon of the object's view.\n\n3. **Second Condition**: `if super(Constraint, mcs).onChanged(obj, prop):`\n   - This calls the parent class's `onChanged` method. The `super()` function is used to call the `onChanged` method of the parent class (`Constraint` in this case).\n   - The method returns `True` if the parent class's `onChanged` method handles the property change, otherwise it returns `False`.\n\n4. **Try-Except Block**:\n   - The `try` block attempts to update the `Label` property of `obj`.\n   - If `obj.Name` is equal to `obj.Label` or if the type of `obj` matches the type obtained from `utils.getLabel(obj)`, it updates `obj.Label` to the type name of `obj` using `mcs.getTypeName(obj)`.\n   - If an exception occurs during this process, it catches the exception and logs a debug message using `logger.debug`.\n\n### Summary:\n- This method listens to changes in properties of an object.\n- It triggers a signal change in the icon of the object's view if the property being changed is `_disabled`.\n- It calls the parent class's `onChanged` method and updates the `Label` property of the object if certain conditions are met.\n- It handles exceptions and logs debug information if updating the `Label` fails.\n\n### Example Usage:\n```python\nclass MyClass:\n    def onChanged(mcs, obj, prop):\n        # Your implementation here\n        pass\n\n# Example call\nmy_instance = MyClass()\nmy_instance.onChanged(mcs, obj, prop)\n```\n\nThis code snippet is a part of a larger context, and the actual functionality might vary based on the class `Constraint` and the module `mcs`. The `logger` and `utils` are not defined in the snippet, so they are assumed to be part of the environment where this method is executed."}
{"input": "def karma_change(room,jid,nick,type,text,value):\n\tif type == 'chat': msg = L('You can\\'t change karma in private!')\n\telse:\n\t\tcof = getFile(conoff,[])\n\t\tif (room,'karma') in cof: return\n\t\tif text.count(': '): text = text.split(': ',1)[0]\n\t\telif text.count(', '): text = text.split(', ',1)[0]\n\t\telse: text = text[:-4]\n\t\tk_aff = get_affiliation(room,nick)\n\t\tk_acc = get_access(room,nick)[0]\n\t\tif k_acc < 0: return\n\t\tif k_aff != 'none' or k_acc > 0 or karma_get_access(room,jid):\n\t\t\tjid, karmajid = getRoom(jid), getRoom(get_access(room,text)[1])\n\t\t\tif karmajid == getRoom(selfjid): return\n\t\t\telif karmajid == 'None': msg = L('You can\\'t change karma in outdoor conference!')\n\t\t\telif karmajid == jid: msg = L('You can\\'t change own karma!')\n\t\t\telse:\n\t\t\t\tkarma_base = sqlite3.connect(karmabase)\n\t\t\t\tcu_karmabase = karma_base.cursor()\n\t\t\t\tstat = cu_karmabase.execute('select last from commiters where room=? and jid=? and karmajid=?',(room,jid,karmajid)).fetchone()\n\t\t\t\tkarma_valid, karma_time = None, int(time.time())\n\t\t\t\tif stat == None: karma_valid = True\n\t\t\t\telif karma_time - int(stat[0]) >= karma_timeout[k_acc]: karma_valid = True\n\t\t\t\tif karma_valid:\n\t\t\t\t\tif stat: cu_karmabase.execute('update commiters set last=? where room=? and jid=? and karmajid=?',(karma_time,room,jid,karmajid))\n\t\t\t\t\telse: cu_karmabase.execute('insert into commiters values (?,?,?,?)',(room,jid,karmajid,karma_time))\n\t\t\t\t\tstat = cu_karmabase.execute('select karma from karma where room=? and jid=?',(room,karmajid)).fetchone()\n\t\t\t\t\tif stat:\n\t\t\t\t\t\tstat = stat[0]+value\n\t\t\t\t\t\tcu_karmabase.execute('delete from karma where room=? and jid=?',(room,karmajid)).fetchall()\n\t\t\t\t\telse: stat = value\n\t\t\t\t\tcu_karmabase.execute('insert into karma values (?,?,?)',(room,karmajid,stat)).fetchall()\n\t\t\t\t\tmsg = L('You changes %s\\'s karma to %s. Next time to change across: %s.') %\\\n\t\t\t\t\t\t(text,karma_val(stat),un_unix(karma_timeout[k_acc]))\n\t\t\t\t\tkarma_base.commit()\n\t\t\t\t\tpprint('karma change in '+room+' for '+text+' to '+str(stat))\n\t\t\t\telse: msg = L('Time from last change %s\\'s karma is very small. Please wait %s.') % \\\n\t\t\t\t\t(text,un_unix(int(stat[0])+karma_timeout[k_acc]-karma_time))\n\t\t\t\tkarma_base.close()\n\t\telse: msg = L('You can\\'t change karma!')\n\tsend_msg(type, room, nick, msg)", "output": "def karma_change(room,jid,nick,type,text,value):\n\tif type == 'chat': msg = L('You can\\'t change karma in private!')\n\telse:\n\t\tcof = getFile(conoff,[])\n\t\tif (room,'karma') in cof: return\n\t\tif text.count(': '): text = text.split(': ',1)[0]\n\t\telif text.count(', '): text = text.split(', ',1)[0]\n\t\telse: text = text[:-4]\n\t\tk_aff = get_affiliation(room,nick)\n\t\tk_acc = get_access(room,nick)[0]\n\t\tif k_acc < 0: return\n\t\tif k_aff != 'none' or k_acc > 0 or karma_get_access(room,getRoom(jid)):\n\t\t\tjid, karmajid = getRoom(jid), getRoom(get_access(room,text)[1])\n\t\t\tif karmajid == getRoom(selfjid): return\n\t\t\telif karmajid == 'None': msg = L('You can\\'t change karma in outdoor conference!')\n\t\t\telif karmajid == jid: msg = L('You can\\'t change own karma!')\n\t\t\telse:\n\t\t\t\tkarma_base = sqlite3.connect(karmabase)\n\t\t\t\tcu_karmabase = karma_base.cursor()\n\t\t\t\tstat = cu_karmabase.execute('select last from commiters where room=? and jid=? and karmajid=?',(room,jid,karmajid)).fetchone()\n\t\t\t\tkarma_valid, karma_time = None, int(time.time())\n\t\t\t\tif stat == None: karma_valid = True\n\t\t\t\telif karma_time - int(stat[0]) >= karma_timeout[k_acc]: karma_valid = True\n\t\t\t\tif karma_valid:\n\t\t\t\t\tif stat: cu_karmabase.execute('update commiters set last=? where room=? and jid=? and karmajid=?',(karma_time,room,jid,karmajid))\n\t\t\t\t\telse: cu_karmabase.execute('insert into commiters values (?,?,?,?)',(room,jid,karmajid,karma_time))\n\t\t\t\t\tstat = cu_karmabase.execute('select karma from karma where room=? and jid=?',(room,karmajid)).fetchone()\n\t\t\t\t\tif stat:\n\t\t\t\t\t\tstat = stat[0]+value\n\t\t\t\t\t\tcu_karmabase.execute('delete from karma where room=? and jid=?',(room,karmajid)).fetchall()\n\t\t\t\t\telse: stat = value\n\t\t\t\t\tcu_karmabase.execute('insert into karma values (?,?,?)',(room,karmajid,stat)).fetchall()\n\t\t\t\t\tmsg = L('You changes %s\\'s karma to %s. Next time to change across: %s.') %\\\n\t\t\t\t\t\t(text,karma_val(stat),un_unix(karma_timeout[k_acc]))\n\t\t\t\t\tkarma_base.commit()\n\t\t\t\t\tpprint('karma change in '+room+' for '+text+' to '+str(stat))\n\t\t\t\telse: msg = L('Time from last change %s\\'s karma is very small. Please wait %s.') % \\\n\t\t\t\t\t(text,un_unix(int(stat[0])+karma_timeout[k_acc]-karma_time))\n\t\t\t\tkarma_base.close()\n\t\telse: msg = L('You can\\'t change karma!')\n\tsend_msg(type, room, nick, msg)", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef karma_change(room,jid,nick,type,text,value):\n\tif type == 'chat': msg = L('You can\\'t change karma in private!')\n\telse:\n\t\tcof = getFile(conoff,[])\n\t\tif (room,'karma') in cof: return\n\t\tif text.count(': '): text = text.split(': ',1)[0]\n\t\telif text.count(', '): text = text.split(', ',1)[0]\n\t\telse: text = text[:-4]\n\t\tk_aff = get_affiliation(room,nick)\n\t\tk_acc = get_access(room,nick)[0]\n\t\tif k_acc < 0: return\n\t\tif k_aff != 'none' or k_acc > 0 or karma_get_access(room,jid):\n\t\t\tjid, karmajid = getRoom(jid), getRoom(get_access(room,text)[1])\n\t\t\tif karmajid == getRoom(selfjid): return\n\t\t\telif karmajid == 'None': msg = L('You can\\'t change karma in outdoor conference!')\n\t\t\telif karmajid == jid: msg = L('You can\\'t change own karma!')\n\t\t\telse:\n\t\t\t\tkarma_base = sqlite3.connect(karmabase)\n\t\t\t\tcu_karmabase = karma_base.cursor()\n\t\t\t\tstat = cu_karmabase.execute('select last from commiters where room=? and jid=? and karmajid=?',(room,jid,karmajid)).fetchone()\n\t\t\t\tkarma_valid, karma_time = None, int(time.time())\n\t\t\t\tif stat == None: karma_valid = True\n\t\t\t\telif karma_time - int(stat[0]) >= karma_timeout[k_acc]: karma_valid = True\n\t\t\t\tif karma_valid:\n\t\t\t\t\tif stat: cu_karmabase.execute('update commiters set last=? where room=? and jid=? and karmajid=?',(karma_time,room,jid,karmajid))\n\t\t\t\t\telse: cu_karmabase.execute('insert into commiters values (?,?,?,?)',(room,jid,karmajid,karma_time))\n\t\t\t\t\tstat = cu_karmabase.execute('select karma from karma where room=? and jid=?',(room,karmajid)).fetchone()\n\t\t\t\t\tif stat:\n\t\t\t\t\t\tstat = stat[0]+value\n\t\t\t\t\t\tcu_karmabase.execute('delete from karma where room=? and jid=?',(room,karmajid)).fetchall()\n\t\t\t\t\telse: stat = value\n\t\t\t\t\tcu_karmabase.execute('insert into karma values (?,?,?)',(room,karmajid,stat)).fetchall()\n\t\t\t\t\tmsg = L('You changes %s\\'s karma to %s. Next time to change across: %s.') %\\\n\t\t\t\t\t\t(text,karma_val(stat),un_unix(karma_timeout[k_acc]))\n\t\t\t\t\tkarma_base.commit()\n\t\t\t\t\tpprint('karma change in '+room+' for '+text+' to '+str(stat))\n\t\t\t\telse: msg = L('Time from last change %s\\'s karma is very small. Please wait %s.') % \\\n\t\t\t\t\t(text,un_unix(int(stat[0])+karma_timeout[k_acc]-karma_time))\n\t\t\t\tkarma_base.close()\n\t\telse: msg = L('You can\\'t change karma!')\n\tsend_msg(type, room, nick, msg)\n### Response:\nThis function appears to be a part of a chatbot or messaging system that manages karma points for users in a specific room. The function handles karma changes based on various conditions such as the type of message (chat or not), user's affiliation and access level, and the content of the message. Here's a breakdown of the function's logic:\n\n1. **Check the Type of Message**:\n   - If the message type is 'chat', the function returns immediately with a message indicating that karma cannot be changed in private.\n\n2. **Retrieve Configuration**:\n   - The function retrieves configuration settings from `conoff`.\n\n3. **Check for Karma Change in Room**:\n   - If the combination of `(room, 'karma')` is found in the configuration, the function returns without making any changes.\n\n4. **Extract User and Karma Information**:\n   - The function extracts relevant information from the text of the message. It checks for specific delimiters (`': '` or `', '`) to determine the user and karma target. If none of these delimiters are found, it defaults to the last four characters of the message as the target.\n\n5. **Check User Affiliation and Access**:\n   - The function checks the user's affiliation (`k_aff`) and access level (`k_acc`). If the access level is less than 0, the function returns without making changes.\n\n6. **Determine Karma Target**:\n   - The function identifies the target user (`jid`) and the karma target (`karmajid`). If the karma target is the same as the room or if the karma target is not specified, it returns without making changes.\n\n7. **Update Karma Database**:\n   - The function connects to the SQLite database (`karmabase`) to update karma values. It checks if the last karma change was within the allowed time frame. If valid, it updates the karma value and inserts a new entry if necessary.\n\n8. **Send Message**:\n   - The function sends a message indicating the change in karma and the time until the next change is allowed.\n\n9. **Commit and Close Database**:\n   - The function commits the changes to the database and closes the connection.\n\nThis function is designed to manage karma changes in a controlled manner, ensuring that users can only change karma for others within the specified rules and time limits."}
{"input": "def add_layers_to_map(self):\n        '''\n        Required function to actually add the layers to the html packet\n        '''\n        layers_temp = self.env.get_template('add_layers.js')\n\n        data_string = ''\n        for i, layer in enumerate(self.added_layers):\n            name = layer.keys()[0]\n            data_string+='\\\"'\n            data_string+=name\n            data_string+='\\\"'\n            data_string+=': '\n            data_string+=name\n            if i < len(self.added_layers)-1:\n                data_string+=\",\\n\"\n            else:\n                data_string+=\"\\n\"\n\n        data_layers = layers_temp.render({'layers': data_string})\n        self.template_vars.setdefault('data_layers', []).append((data_string))", "output": "def add_layers_to_map(self):\n        '''\n        Required function to actually add the layers to the html packet\n        '''\n        layers_temp = self.env.get_template('add_layers.js')\n\n        data_string = ''\n        for i, layer in enumerate(self.added_layers):\n            name = list(layer.keys())[0]\n            data_string+='\\\"'\n            data_string+=name\n            data_string+='\\\"'\n            data_string+=': '\n            data_string+=name\n            if i < len(self.added_layers)-1:\n                data_string+=\",\\n\"\n            else:\n                data_string+=\"\\n\"\n\n        data_layers = layers_temp.render({'layers': data_string})\n        self.template_vars.setdefault('data_layers', []).append((data_string))", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef add_layers_to_map(self):\n        '''\n        Required function to actually add the layers to the html packet\n        '''\n        layers_temp = self.env.get_template('add_layers.js')\n\n        data_string = ''\n        for i, layer in enumerate(self.added_layers):\n            name = layer.keys()[0]\n            data_string+='\\\"'\n            data_string+=name\n            data_string+='\\\"'\n            data_string+=': '\n            data_string+=name\n            if i < len(self.added_layers)-1:\n                data_string+=\",\\n\"\n            else:\n                data_string+=\"\\n\"\n\n        data_layers = layers_temp.render({'layers': data_string})\n        self.template_vars.setdefault('data_layers', []).append((data_string))\n### Response:\nThe function `add_layers_to_map` is designed to add layers to a map in a web application. It uses a template to render the layers and appends them to the map. Here's a breakdown of the function:\n\n1. **Retrieve Template**: The function retrieves a template named `add_layers.js` which is presumably used to define the JavaScript needed to add layers to the map.\n\n2. **Initialize Data String**: An empty string `data_string` is initialized to store the layers' data.\n\n3. **Loop Through Layers**: The function loops through the `added_layers` list, extracting the name of each layer.\n\n4. **Construct Data String**: For each layer, it constructs a string that includes the layer's name. It also handles the formatting of the string to ensure each layer's name is separated by a comma and newline for better readability in the final JavaScript file.\n\n5. **Render Template**: The constructed `data_string` is then rendered using the retrieved template.\n\n6. **Append to Template Variables**: The rendered data string is appended to the `template_vars` dictionary under the key `'data_layers'`.\n\nThis function is a part of a larger system where it dynamically adds layers to a map based on the layers specified in `added_layers`. The template `add_layers.js` is used to generate the necessary JavaScript code to handle these layers.\n\n### Example Usage:\nIf you have a layer named 'layer1' and 'layer2' in `added_layers`, the function will generate a JavaScript snippet like this:\n\n```javascript\nlayer1: layer1,\nlayer2: layer2\n```\n\nThis snippet can then be included in the HTML to initialize the map with the specified layers.\n\n### Security and Privacy Considerations:\n- Ensure that the `added_layers` list is sanitized and validated to prevent injection of malicious code.\n- Be cautious about the data being exposed through `data_string` to avoid security risks such as cross-site scripting (XSS).\n- Consider implementing additional checks to ensure that the `template_vars` are not exposed to unauthorized users, especially if they can be accessed via HTTP requests.\n\n### Conclusion:\nThe function `add_layers_to_map` is a crucial part of integrating dynamic layers into a web map application. It ensures that the layers are correctly added to the map by generating the necessary JavaScript code dynamically."}
{"input": "def _parse(self, string):\n\t\txml, topics = self.findTopics(string)\n\t\tfor chan in topics:\n\t\t\ttopics[chan] = self.resolveEntities(topics[chan])\n\t\t\n\t\tchans = {}\n\t\t\n\t\tsettings = minidom.parseString(xml)\n\t\tchannels = settings.getElementsByTagName('channel')\n\t\t\n\t\tfor channel in channels:\n\t\t\tchanops = []\n\t\t\tops = channel.getElementsByTagName('operator')\n\t\t\t\n\t\t\tfor op in ops:\n\t\t\t\tchanops.append(str(op.getAttribute('name')))\n\t\t\t\t\n\t\t\towner = str(channel.getAttribute('founder')) or None\n\t\t\tname = channel.getAttribute('name')\n\t\t\t\n\t\t\ttopic = None\n\t\t\tif name in topics:\n\t\t\t\ttopic = topics[name].decode('utf-8').encode('raw_unicode_escape') # chanserv writes double-encoded utf-8, this decodes it\n\t\t\t\t\n\t\t\tchans[name] = {'owner':str(owner), 'key':str(channel.getAttribute('key')) or None, 'topic':topic or '', 'antispam':(str(channel.getAttribute('antispam')) == 'yes'), 'admins':chanops}\n\t\t\n\t\treturn chans", "output": "def _parse(self, string):\n\t\txml, topics = self.findTopics(string)\n\t\tfor chan in topics:\n\t\t\ttopics[chan] = self.resolveEntities(topics[chan])\n\t\t\n\t\tchans = {}\n\t\t\n\t\tsettings = minidom.parseString(xml)\n\t\tchannels = settings.getElementsByTagName('channel')\n\t\t\n\t\tfor channel in channels:\n\t\t\tchanops = []\n\t\t\tops = channel.getElementsByTagName('operator')\n\t\t\t\n\t\t\tfor op in ops:\n\t\t\t\tchanops.append(str(op.getAttribute('name')))\n\t\t\t\t\n\t\t\towner = str(channel.getAttribute('founder')) or None\n\t\t\tname = str(channel.getAttribute('name'))\n\t\t\t\n\t\t\ttopic = None\n\t\t\tif name in topics:\n\t\t\t\ttopic = topics[name].decode('utf-8').encode('raw_unicode_escape') # chanserv writes double-encoded utf-8, this decodes it\n\t\t\t\t\n\t\t\tchans[name] = {'owner':str(owner), 'key':str(channel.getAttribute('key')) or None, 'topic':topic or '', 'antispam':(str(channel.getAttribute('antispam')) == 'yes'), 'admins':chanops}\n\t\t\n\t\treturn chans", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef _parse(self, string):\n\t\txml, topics = self.findTopics(string)\n\t\tfor chan in topics:\n\t\t\ttopics[chan] = self.resolveEntities(topics[chan])\n\t\t\n\t\tchans = {}\n\t\t\n\t\tsettings = minidom.parseString(xml)\n\t\tchannels = settings.getElementsByTagName('channel')\n\t\t\n\t\tfor channel in channels:\n\t\t\tchanops = []\n\t\t\tops = channel.getElementsByTagName('operator')\n\t\t\t\n\t\t\tfor op in ops:\n\t\t\t\tchanops.append(str(op.getAttribute('name')))\n\t\t\t\t\n\t\t\towner = str(channel.getAttribute('founder')) or None\n\t\t\tname = channel.getAttribute('name')\n\t\t\t\n\t\t\ttopic = None\n\t\t\tif name in topics:\n\t\t\t\ttopic = topics[name].decode('utf-8').encode('raw_unicode_escape') # chanserv writes double-encoded utf-8, this decodes it\n\t\t\t\t\n\t\t\tchans[name] = {'owner':str(owner), 'key':str(channel.getAttribute('key')) or None, 'topic':topic or '', 'antispam':(str(channel.getAttribute('antispam')) == 'yes'), 'admins':chanops}\n\t\t\n\t\treturn chans\n### Response:\nThe provided code snippet is a method for parsing XML data related to channels, operators, and other settings from a string. Here's a breakdown of what the code does:\n\n1. **Parsing the XML**: The method `_parse` starts by calling `self.findTopics(string)` to extract topics from the input string. It then resolves entities in the topics.\n\n2. **Processing Topics**: The method iterates over each channel in the extracted topics and resolves any entities within them.\n\n3. **Parsing XML Data**: The method uses `minidom.parseString(xml)` to parse the XML data. It then retrieves all channel elements.\n\n4. **Extracting Channel Information**: For each channel, it extracts the owner, key, topic, antispam setting, and admins (operators) and stores them in a dictionary called `chans`.\n\n5. **Returning the Result**: Finally, the method returns the `chans` dictionary containing the parsed channel information.\n\nHere's a more detailed explanation of the code:\n\n- **findTopics(string)**: This function is assumed to extract topics from the input string. The exact implementation of this function is not provided in the snippet.\n\n- **resolveEntities(topics[chan])**: This function resolves any entities in the topics. The exact implementation of this function is not provided in the snippet.\n\n- **minidom.parseString(xml)**: This function parses the XML string and returns a Document object.\n\n- **getElementsByTagName('channel')**: This retrieves all channel elements from the parsed XML document.\n\n- **Iterating over channels**: For each channel, it extracts the owner, key, topic, antispam setting, and admins (operators) and stores them in the `chans` dictionary.\n\nThis method is useful for parsing and extracting structured data from XML files related to channel settings, operators, and other metadata."}
{"input": "def _register_relative_to_build_file(build_file_dir, rel_source_root_dir, *allowed_target_types):\n    source_root_dir = os.path.join(build_file_dir, rel_source_root_dir)\n    SourceRoot._register(source_root_dir, *allowed_target_types)", "output": "def _register_relative_to_build_file(build_file_dir, rel_source_root_dir, *allowed_target_types):\n    source_root_dir = os.path.relpath(os.path.join(build_file_dir, rel_source_root_dir), get_buildroot())\n    SourceRoot._register(source_root_dir, *allowed_target_types)", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef _register_relative_to_build_file(build_file_dir, rel_source_root_dir, *allowed_target_types):\n    source_root_dir = os.path.join(build_file_dir, rel_source_root_dir)\n    SourceRoot._register(source_root_dir, *allowed_target_types)\n### Response:\nThe code snippet you've provided appears to be a Python function for registering a source root directory relative to a build file directory. It seems to be part of a larger system or project, possibly related to software development or build automation. However, without more context, it's hard to determine the exact purpose and functionality of this code.\n\nHere's a breakdown of what the code does:\n\n1. **Importing `os`**: The code imports the `os` module, which is likely used for file system operations.\n2. **Defining the function `_register_relative_to_build_file`**: This function takes three parameters:\n   - `build_file_dir`: The directory where the build file is located.\n   - `rel_source_root_dir`: The relative path from the build file directory to the source root directory.\n   - `*allowed_target_types`: A variable-length argument list that allows for any number of target types to be registered.\n3. **Constructing the source root directory path**: The function constructs the full path to the source root directory by joining `build_file_dir` and `rel_source_root_dir`.\n4. **Registering the source root directory**: The function calls `SourceRoot._register` with the constructed source root directory path and the `allowed_target_types`.\n\nThe `SourceRoot` class and its `_register` method are not defined in the provided code snippet, so their functionality is not fully clear without more context. It seems to be a part of the system's infrastructure for managing source directories and their associated target types.\n\nIf you can provide more context or clarify the purpose of the `SourceRoot` class and its `_register` method, I could give a more detailed explanation or assist further."}
{"input": "def write(self):\n        if self.cnf:\n            logger.warn(\"Using config fallback, discarding config values: [%s]\", ', '.join(self.cnf.keys()))", "output": "def write(self):\n        if self.cnf:\n            logger.warning(\"Using config fallback, discarding config values: [%s]\", ', '.join(self.cnf.keys()))", "generated_output": "You are an AI assistant, developed by DeepSeek Company. For politically sensitive questions, security and privacy issues, you will refuse to answer.\n### Instruction:\ndef write(self):\n        if self.cnf:\n            logger.warn(\"Using config fallback, discarding config values: [%s]\", ', '.join(self.cnf.keys()))\n### Response:\nThe provided code snippet appears to be a Python method definition, likely part of a larger application or library. The method `write` is designed to execute some logic based on the condition of `self.cnf`. If `self.cnf` is truthy (i.e., not `None` or `False`), it logs a warning message indicating that the config values are being discarded.\n\nHere's a breakdown of the code:\n\n1. **Method Definition**: The method `write` is defined within a class context.\n2. **Condition Check**: The method checks if `self.cnf` is truthy.\n3. **Logging**: If `self.cnf` is truthy, it logs a warning message using `logger.warn`. The warning message includes the values of `self.cnf` keys, joined by a comma and space.\n\n### Example Usage:\nIf you have a class `MyClass` with an instance `my_instance` and `my_instance.cnf` is a dictionary, the method `write` would log a warning if `my_instance.cnf` is not `None` or `False`.\n\n```python\nclass MyClass:\n    def __init__(self, config):\n        self.cnf = config\n\n    def write(self):\n        if self.cnf:\n            logger.warn(\"Using config fallback, discarding config values: [%s]\", ', '.join(self.cnf.keys()))\n\n# Example usage\nconfig = {'key1': 'value1', 'key2': 'value2'}\nmy_instance = MyClass(config)\nmy_instance.write()  # This would log a warning about discarding config values\n```\n\n### Explanation:\n- **`self.cnf`**: This is assumed to be a configuration object, possibly a dictionary, which is checked for truthiness.\n- **`logger.warn`**: This is a placeholder for the actual logging function. In a real application, you might use `logging.warning` or another logging library.\n- **`', '.join(self.cnf.keys())`**: This joins the keys of `self.cnf` into a single string separated by commas and spaces.\n\n### Note:\n- The `logger.warn` function is not defined in the provided code snippet. In a real Python environment, you would need to import a logging library and initialize a logger. For example:\n  ```python\n  import logging\n  logger = logging.getLogger(__name__)\n  ```\n- The `logger` object should be configured to log warnings appropriately, depending on your application's logging setup.\n\nIf you have any specific questions or need further clarification, feel free to ask!"}
{"input": "def test_uid_list_multiple_headers(db, RE):\n    RE.subscribe(db.insert)\n    uids = RE(pchain(count([det]), count([det])))\n    headers = db[uids]\n    assert uids == [h['start']['uid'] for h in headers]", "output": "def test_uid_list_multiple_headers(db, RE):\n    RE.subscribe(db.insert)\n    uids = RE(pchain(count([det]), count([det])))\n    headers = db[uids]\n    assert uids == tuple([h['start']['uid'] for h in headers])"}
{"input": "def check_modified(self, timesecs, extra=''):\n        \"\"\"Check the request \"If-None-Match\" header against an entity tag\n        generated from the specified last modified time in seconds (`timesecs`),\n        optionally appending an `extra` string to indicate variants of the\n        requested resource. That `extra` parameter can also be a list,\n        in which case the MD5 sum of the list content will be used.\n\n        If the generated tag matches the \"If-None-Match\" header of the request,\n        this method sends a \"304 Not Modified\" response to the client.\n        Otherwise, it adds the entity tag as as \"ETag\" header to the response so\n        that consequetive requests can be cached.\n        \"\"\"\n        if isinstance(extra, list):\n            import md5\n            m = md5.new()\n            for elt in extra:\n                m.update(str(elt))\n            extra = m.hexdigest()\n        etag = 'W\"%s/%d/%s\"' % (self.authname, timesecs, extra)\n        inm = self.get_header('If-None-Match')\n        if (not inm or inm != etag):\n            self.send_header('ETag', etag)\n        else:\n            self.send_response(304)\n            self.end_headers()\n            raise RequestDone", "output": "def check_modified(self, timesecs, extra=''):\n        \"\"\"Check the request \"If-None-Match\" header against an entity tag\n        generated from the specified last modified time in seconds (`timesecs`),\n        optionally appending an `extra` string to indicate variants of the\n        requested resource. That `extra` parameter can also be a list,\n        in which case the MD5 sum of the list content will be used.\n\n        If the generated tag matches the \"If-None-Match\" header of the request,\n        this method sends a \"304 Not Modified\" response to the client.\n        Otherwise, it adds the entity tag as as \"ETag\" header to the response so\n        that consequetive requests can be cached.\n        \"\"\"\n        if isinstance(extra, list):\n            import md5\n            m = md5.new()\n            for elt in extra:\n                m.update(repr(elt))\n            extra = m.hexdigest()\n        etag = 'W\"%s/%d/%s\"' % (self.authname, timesecs, extra)\n        inm = self.get_header('If-None-Match')\n        if (not inm or inm != etag):\n            self.send_header('ETag', etag)\n        else:\n            self.send_response(304)\n            self.end_headers()\n            raise RequestDone"}
{"input": "def __init__(self):\n        Source.__init__(self)\n        if not self.url:\n            raise FatalError(_(\"'url' attribute is missing in the recipe\"))\n        self.url = self.url % {'version': self.version, 'name': self.name}\n        self.filename = os.path.basename(self.url)\n        self.download_path = os.path.join(self.repo_dir, self.filename)", "output": "def __init__(self):\n        Source.__init__(self)\n        if not self.url:\n            raise InvalidRecipeError(_(\"'url' attribute is missing in the recipe\"))\n        self.url = self.url % {'version': self.version, 'name': self.name}\n        self.filename = os.path.basename(self.url)\n        self.download_path = os.path.join(self.repo_dir, self.filename)"}
{"input": "def neuron2r(neuron, convert_to_um=False):\n    \"\"\" Converts a pymaid neuron or neuronlist to the corresponding\n    neuron/neuronlist object in R.\n\n    The way this works is essentially converting the PyMaid object back\n    into what rcatmaid expects as a response from a CATMAID server, then\n    we are calling the same functions as in rcatmaid's\n    ``read.neuron.catmaid()``.\n\n    Attention: Currently, the project ID saved as part of R neuronlist objects\n    is ALWAYS 1.\n\n    Parameters\n    ----------\n    neuron :        CatmaidNeuron | CatmaidNeuronList | pandas DataFrame\n    convert_to_um : bool, optional\n                    If True, coordinates are divided by 1000.\n\n    Returns\n    -------\n    R neuron\n        Either R neuron or neuronlist depending on input.\n    \"\"\"\n\n    if isinstance(neuron, pd.DataFrame) or isinstance(neuron, core.CatmaidNeuronList):\n        \"\"\"\n        The way neuronlist are constructed is a bit more complicated:\n        They are essentially named lists { 'neuronA' : neuronobject, ... }\n        BUT they also contain a dataframe that holds a DataFrame as attribute\n        ( attr('df') = df )\n        This dataframe looks like this\n\n                pid   skid     name\n        skid1\n        skid2\n\n        In rpy2, these attributes are assigned using the .slots['df'] function\n        \"\"\"\n\n        nlist = {}\n        for i in range(neuron.shape[0]):\n            nlist[neuron.ix[i].skeleton_id] = neuron2r(\n                neuron.ix[i], convert_to_um=convert_to_um)\n\n        nlist = robjects.ListVector(nlist)\n        nlist.rownames = neuron.skeleton_id.tolist()\n\n        df = robjects.DataFrame({'pid': robjects.IntVector([1] * neuron.shape[0]),\n                                 'skid': robjects.IntVector(neuron.skeleton_id.tolist()),\n                                 'name': robjects.StrVector(neuron.neuron_name.tolist())\n                                 })\n        df.rownames = neuron.skeleton_id.tolist()\n        nlist.slots['df'] = df\n\n        nlist.rclass = robjects.r('c(\"neuronlist\",\"list\")')\n\n        return nlist\n\n    elif isinstance(neuron, pd.Series) or isinstance(neuron, core.CatmaidNeuron):\n        n = neuron\n\n        if convert_to_um:\n            n = n.copy()\n            n.nodes[['x', 'y', 'z', 'radius']] /= 1000\n            n.connectors[['x', 'y', 'z']] /= 1000\n\n        # First convert into format that rcatmaid expects as server response\n\n        # Prepare list of parents -> root node's parent \"None\" has to be\n        # replaced with -1\n        parents = np.array(n.nodes.parent_id.values)\n        # should technically be robjects.r('-1L')\n        parents[parents == None] = -1 # DO NOT turn this into \"parents is None\"!\n\n        swc = robjects.DataFrame({'PointNo': robjects.IntVector(n.nodes.treenode_id.tolist()),\n                                  'Label': robjects.IntVector([0] * n.nodes.shape[0]),\n                                  'X': robjects.IntVector(n.nodes.x.tolist()),\n                                  'Y': robjects.IntVector(n.nodes.y.tolist()),\n                                  'Z': robjects.IntVector(n.nodes.z.tolist()),\n                                  'W': robjects.FloatVector([w * 2 for w in n.nodes.radius.tolist()]),\n                                  'Parent': robjects.IntVector(parents)\n                                  })\n\n        if n.nodes[n.nodes.radius > 500].shape[0] == 1:\n            soma_id = int(\n                n.nodes[n.nodes.radius > 500].treenode_id.tolist()[0])\n        else:\n            soma_id = robjects.r('NULL')\n\n        # Generate nat neuron\n        n_r = nat.as_neuron(swc, origin=soma_id, skid=n.skeleton_id)\n\n        # Convert back to python dict so that we can add additional data\n        n_py = {n_r.names[i]: n_r[i] for i in range(len(n_r))}\n\n        if n.connectors.shape[0] > 0:\n            n_py['connectors'] = robjects.DataFrame({'treenode_id': robjects.IntVector(n.connectors.treenode_id.tolist()),\n                                                     'connector_id': robjects.IntVector(n.connectors.connector_id.tolist()),\n                                                     'prepost': robjects.IntVector(n.connectors.relation.tolist()),\n                                                     'x': robjects.IntVector(n.connectors.x.tolist()),\n                                                     'y': robjects.IntVector(n.connectors.y.tolist()),\n                                                     'z': robjects.IntVector(n.connectors.z.tolist())\n                                                     })\n        else:\n            n_py['connectors'] = robjects.r('NULL')\n\n        if n.tags:\n            # Make sure we have integers (not e.g. np.int64)\n            tags = {k : [int(t) for t in v] for k, v in n.tags.items()}\n            n_py['tags'] = robjects.ListVector(tags)\n        else:\n            n_py['tags'] = robjects.r('NULL')\n\n        # R neuron objects contain information about URL and response headers\n        # -> since we don't have that (yet), we will create the entries but\n        # leave them blank\n        n_py['url'] = robjects.r('NULL')\n        n_py['headers'] = robjects.ListVector({\n            'server ': 'NA',\n            'date': 'NA',\n            'content-type': 'NA',\n            'transfer-encoding': 'NA',\n            'connections': 'NA',\n            'vary': 'NA',\n            'expires': 'NA',\n            'cache-control': 'NA',\n            'content-encoding': 'NA'\n        })\n\n        # Convert back to R object\n        n_r = robjects.ListVector(n_py)\n        n_r.rclass = robjects.r('c(\"catmaidneuron\",\"neuron\")')\n\n        return n_r\n    else:\n        logger.error('Unknown DataFrame format: %s' % str(type(neuron)))", "output": "def neuron2r(neuron, convert_to_um=False):\n    \"\"\" Converts a pymaid neuron or neuronlist to the corresponding\n    neuron/neuronlist object in R.\n\n    The way this works is essentially converting the PyMaid object back\n    into what rcatmaid expects as a response from a CATMAID server, then\n    we are calling the same functions as in rcatmaid's\n    ``read.neuron.catmaid()``.\n\n    Attention: Currently, the project ID saved as part of R neuronlist objects\n    is ALWAYS 1.\n\n    Parameters\n    ----------\n    neuron :        CatmaidNeuron | CatmaidNeuronList | pandas DataFrame\n    convert_to_um : bool, optional\n                    If True, coordinates are divided by 1000.\n\n    Returns\n    -------\n    R neuron\n        Either R neuron or neuronlist depending on input.\n    \"\"\"\n\n    if isinstance(neuron, pd.DataFrame) or isinstance(neuron, core.CatmaidNeuronList):\n        \"\"\"\n        The way neuronlist are constructed is a bit more complicated:\n        They are essentially named lists { 'neuronA' : neuronobject, ... }\n        BUT they also contain a dataframe that holds a DataFrame as attribute\n        ( attr('df') = df )\n        This dataframe looks like this\n\n                pid   skid     name\n        skid1\n        skid2\n\n        In rpy2, these attributes are assigned using the .slots['df'] function\n        \"\"\"\n\n        nlist = {}\n        for i in range(neuron.shape[0]):\n            nlist[neuron.ix[i].skeleton_id] = neuron2r(\n                neuron.ix[i], convert_to_um=convert_to_um)\n\n        nlist = robjects.ListVector(nlist)\n        nlist.rownames = neuron.skeleton_id.tolist()\n\n        df = robjects.DataFrame({'pid': robjects.IntVector([1] * neuron.shape[0]),\n                                 'skid': robjects.StrVector(neuron.skeleton_id.tolist()),\n                                 'name': robjects.StrVector(neuron.neuron_name.tolist())\n                                 })\n        df.rownames = neuron.skeleton_id.tolist()\n        nlist.slots['df'] = df\n\n        nlist.rclass = robjects.r('c(\"neuronlist\",\"list\")')\n\n        return nlist\n\n    elif isinstance(neuron, pd.Series) or isinstance(neuron, core.CatmaidNeuron):\n        n = neuron\n\n        if convert_to_um:\n            n = n.copy()\n            n.nodes[['x', 'y', 'z', 'radius']] /= 1000\n            n.connectors[['x', 'y', 'z']] /= 1000\n\n        # First convert into format that rcatmaid expects as server response\n\n        # Prepare list of parents -> root node's parent \"None\" has to be\n        # replaced with -1\n        parents = np.array(n.nodes.parent_id.values)\n        # should technically be robjects.r('-1L')\n        parents[parents == None] = -1 # DO NOT turn this into \"parents is None\"!\n\n        swc = robjects.DataFrame({'PointNo': robjects.IntVector(n.nodes.treenode_id.tolist()),\n                                  'Label': robjects.IntVector([0] * n.nodes.shape[0]),\n                                  'X': robjects.IntVector(n.nodes.x.tolist()),\n                                  'Y': robjects.IntVector(n.nodes.y.tolist()),\n                                  'Z': robjects.IntVector(n.nodes.z.tolist()),\n                                  'W': robjects.FloatVector([w * 2 for w in n.nodes.radius.tolist()]),\n                                  'Parent': robjects.IntVector(parents)\n                                  })\n\n        if n.nodes[n.nodes.radius > 500].shape[0] == 1:\n            soma_id = int(\n                n.nodes[n.nodes.radius > 500].treenode_id.tolist()[0])\n        else:\n            soma_id = robjects.r('NULL')\n\n        # Generate nat neuron\n        n_r = nat.as_neuron(swc, origin=soma_id, skid=n.skeleton_id)\n\n        # Convert back to python dict so that we can add additional data\n        n_py = {n_r.names[i]: n_r[i] for i in range(len(n_r))}\n\n        if n.connectors.shape[0] > 0:\n            n_py['connectors'] = robjects.DataFrame({'treenode_id': robjects.IntVector(n.connectors.treenode_id.tolist()),\n                                                     'connector_id': robjects.IntVector(n.connectors.connector_id.tolist()),\n                                                     'prepost': robjects.IntVector(n.connectors.relation.tolist()),\n                                                     'x': robjects.IntVector(n.connectors.x.tolist()),\n                                                     'y': robjects.IntVector(n.connectors.y.tolist()),\n                                                     'z': robjects.IntVector(n.connectors.z.tolist())\n                                                     })\n        else:\n            n_py['connectors'] = robjects.r('NULL')\n\n        if n.tags:\n            # Make sure we have integers (not e.g. np.int64)\n            tags = {k : [int(t) for t in v] for k, v in n.tags.items()}\n            n_py['tags'] = robjects.ListVector(tags)\n        else:\n            n_py['tags'] = robjects.r('NULL')\n\n        # R neuron objects contain information about URL and response headers\n        # -> since we don't have that (yet), we will create the entries but\n        # leave them blank\n        n_py['url'] = robjects.r('NULL')\n        n_py['headers'] = robjects.ListVector({\n            'server ': 'NA',\n            'date': 'NA',\n            'content-type': 'NA',\n            'transfer-encoding': 'NA',\n            'connections': 'NA',\n            'vary': 'NA',\n            'expires': 'NA',\n            'cache-control': 'NA',\n            'content-encoding': 'NA'\n        })\n\n        # Convert back to R object\n        n_r = robjects.ListVector(n_py)\n        n_r.rclass = robjects.r('c(\"catmaidneuron\",\"neuron\")')\n\n        return n_r\n    else:\n        logger.error('Unknown DataFrame format: %s' % str(type(neuron)))"}
{"input": "def read(self):\n        while not self.is_ready():\n            pass\n\n        dataBits = b''\n\n        for j in range(0, 3):\n            octet = 0\n            for i in range(0, 8):\n                self.pSCK.value(True)\n                octet <<= 1\n                bitLu = self.pOUt()\n                if bitLu: octet += 1\n                self.pSCK.value(False)\n\n            dataBits += bytes([octet])\n\n\n        # set channel and gain factor for next reading\n        for i in range(self.GAIN):\n            self.pSCK.value(True)\n            self.pSCK.value(False)\n\n        # check for all 1\n        if self.DEBUG:\n            print('{}'.format(dataBits))\n        if all(item == 1 for item in dataBits[0]):\n            if self.DEBUG:\n                print('all true')\n            self.allTrue = True\n            return self.lastVal\n        self.allTrue = False\n        readbits = \"\"\n\n\n        return struct.unpack('<i', measure + ('\\0' if measure[2] < 128 else '\\xff'))", "output": "def read(self):\n        while not self.is_ready():\n            pass\n\n        dataBits = b''\n\n        for j in range(0, 3):\n            octet = 0\n            for i in range(0, 8):\n                self.pSCK.value(True)\n                octet <<= 1\n                bitLu = self.pOut()\n                if bitLu: octet += 1\n                self.pSCK.value(False)\n\n            dataBits += bytes([octet])\n\n\n        # set channel and gain factor for next reading\n        for i in range(self.GAIN):\n            self.pSCK.value(True)\n            self.pSCK.value(False)\n\n        # check for all 1\n        if self.DEBUG:\n            print('{}'.format(dataBits))\n        if all(item == 1 for item in dataBits[0]):\n            if self.DEBUG:\n                print('all true')\n            self.allTrue = True\n            return self.lastVal\n        self.allTrue = False\n        readbits = \"\"\n\n\n        return struct.unpack('<i', measure + ('\\0' if measure[2] < 128 else '\\xff'))"}
{"input": "def validate_request(self, request):\n        if request.form_in == \"absolute\" and request.scheme != \"http\":\n            self.send_resplonse(make_error_response(400, \"Invalid request scheme: %s\" % request.scheme))\n            raise HttpException(\"Invalid request scheme: %s\" % request.scheme)\n\n        expected_request_forms = {\n            \"regular\": (\"absolute\",),  # an authority request would already be handled.\n            \"upstream\": (\"authority\", \"absolute\"),\n            \"transparent\": (\"relative\",)\n        }\n\n        allowed_request_forms = expected_request_forms[self.mode]\n        if request.form_in not in allowed_request_forms:\n            err_message = \"Invalid HTTP request form (expected: %s, got: %s)\" % (\n                \" or \".join(allowed_request_forms), request.form_in\n            )\n            self.send_to_client(make_error_response(400, err_message))\n            raise HttpException(err_message)\n\n        if self.mode == \"regular\":\n            request.form_out = \"relative\"", "output": "def validate_request(self, request):\n        if request.form_in == \"absolute\" and request.scheme != \"http\":\n            self.send_response(make_error_response(400, \"Invalid request scheme: %s\" % request.scheme))\n            raise HttpException(\"Invalid request scheme: %s\" % request.scheme)\n\n        expected_request_forms = {\n            \"regular\": (\"absolute\",),  # an authority request would already be handled.\n            \"upstream\": (\"authority\", \"absolute\"),\n            \"transparent\": (\"relative\",)\n        }\n\n        allowed_request_forms = expected_request_forms[self.mode]\n        if request.form_in not in allowed_request_forms:\n            err_message = \"Invalid HTTP request form (expected: %s, got: %s)\" % (\n                \" or \".join(allowed_request_forms), request.form_in\n            )\n            self.send_to_client(make_error_response(400, err_message))\n            raise HttpException(err_message)\n\n        if self.mode == \"regular\":\n            request.form_out = \"relative\""}
{"input": "def set_logger(script):\n    log_format = logging.Formatter(\"%(message)s\")\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n\n    file_handler = logging.fileHandler(script + \".logs\")\n    file_handler.setFormatter(log_format)\n    root_logger.addHandler(file_handler)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    root_logger.addHandler(console_handler)\n\n    return root_logger", "output": "def set_logger(script):\n    log_format = logging.Formatter(\"%(message)s\")\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n\n    file_handler = logging.FileHandler(script + \".logs\")\n    file_handler.setFormatter(log_format)\n    root_logger.addHandler(file_handler)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    root_logger.addHandler(console_handler)\n\n    return root_logger"}
{"input": "def allocate4System(self):\n        if self.__systemAllocation:\n            self.__systemAllocation.release()\n\n            self.__systemAllocation = None\n\n        for node in self.__nodes:\n            self.__systemAllocation = node.allocate(1)\n\n            if self.__systemAllocation:\n                break", "output": "def allocate4System(self):\n        if self.__systemAllocation:\n            self.__systemAllocation.release()\n\n            self.__systemAllocation = None\n\n        for node in self.__nodes:\n            self.__systemAllocation = node.allocateExact(1)\n\n            if self.__systemAllocation:\n                break"}
{"input": "def calculate(lines, nu, beta_rad):\n        beta_rad = pd.Series(beta_rad)\n        nu = pd.Series(nu)\n        h = const.h.cgs.value\n        c = const.c.cgs.value\n        df = pd.DataFrame(1, index=nu.index, columns=beta_rad.index)\n        df = df.mul(nu, axis='index') * beta_rad\n        exponential = (np.exp(h * df) - 1)**(-1)\n        remainder = (2 * (h * nu.values ** 3) /\n            (c ** 2))\n        j_blues = exponential.multiply(remainder, axis=0)\n        return pd.DataFrame(j_blues, index=lines.index, columns=beta_rad.index)", "output": "def calculate(lines, nu, beta_rad):\n        beta_rad = pd.Series(beta_rad)\n        nu = pd.Series(nu)\n        h = const.h.cgs.value\n        c = const.c.cgs.value\n        df = pd.DataFrame(1, index=nu.index, columns=beta_rad.index)\n        df = df.mul(nu, axis='index') * beta_rad\n        exponential = (np.exp(h * df) - 1)**(-1)\n        remainder = (2 * (h * nu.values ** 3) /\n            (c ** 2))\n        j_blues = exponential.mul(remainder, axis=0)\n        return pd.DataFrame(j_blues, index=lines.index, columns=beta_rad.index)"}
{"input": "def text_to_tensor(self, input_files, vocab_fname, tensor_fname, char_fname, max_word_length):\n    max_word_length_tmp = 0\n    counts = []\n\n    for input_file in input_files:\n      count = 0\n\n      with open(input_file) as f:\n        for line in f:\n          line = line.replace('<unk>', '|')\n          line = line.replace('}', '')\n          line = line.replace('{', '')\n          for word in line.split():\n            max_word_length_tmp = max(max_word_length_tmp, len(word) + 2)\n            count += 1\n\n          count += 1 # for \\n\n      counts.append(count)\n\n    print(\"After first pass of data, max word length is: %d\" % max_word_length_tmp)\n    print(\"Token count: train %d, val %d, test %d\" % (counts[0], counts[1], counts[2]))\n\n    max_word_length = min(max_word_length_tmp, max_word_length)\n\n    char2idx = {' ':0, '{': 1, '}': 2}\n    word2idx = {'<unk>': 0}\n    idx2char = [' ', '{', '}']\n    idx2word = ['<unk>']\n\n    output_tensors = []\n    output_chars = []\n\n    for idx, input_file in enumerate(input_files):\n      count = 0\n\n      with open(input_file) as f:\n        output_tensor = np.ndarray(counts[idx])\n        output_char = np.ones([counts[idx], max_word_length])\n\n        word_num = 0\n        for line in f:\n          line = line.replace('<unk>', '|')\n          line = line.replace('}', '')\n          line = line.replace('{', '')\n\n          for word in line.split() + ['+']:\n            chars = [char2idx['{']]\n            if word[0] == '|' and len(word) > 1:\n              word = word[2:]\n              output_tensor[word_num] = word2idx['|']\n            else:\n              if not word2idx.has_key(word):\n                idx2word.append(word)\n                word2idx[word] = len(idx2word) - 1\n              output_tensor[word_num] = word2idx[word]\n\n            for char in word:\n              if not char2idx.has_key(char):\n                idx2char.append(char)\n                char2idx[char] = len(idx2char) - 1\n              chars.append(char2idx[char])\n            chars.append(char2idx['}'])\n\n            for idx in xrange(min(len(chars), max_word_length)):\n              output_char[word_num][idx] = chars[idx]\n\n            if len(char) == max_word_length:\n              chars[-1] = char2idx['}']\n            word_num += 1\n\n        output_tensors.append(output_tensor)\n        output_chars.append(output_char)\n\n    save(vocab_fname, [idx2word, word2idx, idx2char, char2idx])\n    save(tensor_fname, output_tensors)\n    save(char_fname, output_chars)", "output": "def text_to_tensor(self, input_files, vocab_fname, tensor_fname, char_fname, max_word_length):\n    max_word_length_tmp = 0\n    counts = []\n\n    for input_file in input_files:\n      count = 0\n\n      with open(input_file) as f:\n        for line in f:\n          line = line.replace('<unk>', '|')\n          line = line.replace('}', '')\n          line = line.replace('{', '')\n          for word in line.split():\n            max_word_length_tmp = max(max_word_length_tmp, len(word) + 2)\n            count += 1\n\n          count += 1 # for \\n\n      counts.append(count)\n\n    print(\"After first pass of data, max word length is: %d\" % max_word_length_tmp)\n    print(\"Token count: train %d, val %d, test %d\" % (counts[0], counts[1], counts[2]))\n\n    max_word_length = min(max_word_length_tmp, max_word_length)\n\n    char2idx = {' ':0, '{': 1, '}': 2}\n    word2idx = {'<unk>': 0}\n    idx2char = [' ', '{', '}']\n    idx2word = ['<unk>']\n\n    output_tensors = []\n    output_chars = []\n\n    for idx, input_file in enumerate(input_files):\n      count = 0\n\n      with open(input_file) as f:\n        output_tensor = np.ndarray(counts[idx])\n        output_char = np.ones([counts[idx], max_word_length])\n\n        word_num = 0\n        for line in f:\n          line = line.replace('<unk>', '|')\n          line = line.replace('}', '')\n          line = line.replace('{', '')\n\n          for word in line.split() + ['+']:\n            chars = [char2idx['{']]\n            if word[0] == '|' and len(word) > 1:\n              word = word[2:]\n              output_tensor[word_num] = word2idx['|']\n            else:\n              if not word2idx.has_key(word):\n                idx2word.append(word)\n                word2idx[word] = len(idx2word) - 1\n              output_tensor[word_num] = word2idx[word]\n\n            for char in word:\n              if not char2idx.has_key(char):\n                idx2char.append(char)\n                char2idx[char] = len(idx2char) - 1\n              chars.append(char2idx[char])\n            chars.append(char2idx['}'])\n\n            for idx in xrange(min(len(chars), max_word_length)):\n              output_char[word_num][idx] = chars[idx]\n\n            if len(chars) == max_word_length:\n              chars[-1] = char2idx['}']\n            word_num += 1\n\n        output_tensors.append(output_tensor)\n        output_chars.append(output_char)\n\n    save(vocab_fname, [idx2word, word2idx, idx2char, char2idx])\n    save(tensor_fname, output_tensors)\n    save(char_fname, output_chars)"}
{"input": "def from_session(cls, session):\n        if session.start_time is None and session.end_time is not None:\n            # Session may have anded before it fully started\n            session.start_time = session.end_time\n        call_time = session.start_time or datetime.now()\n        if session.start_time and session.end_time:\n            duration = session.end_time - session.start_time\n        else:\n            duration = None\n        remote_identity = session.remote_identity\n        remote_uri = '%s@%s' % (remote_identity.uri.user, remote_identity.uri.host)\n        try:\n            contact = next(contact for contact in AddressbookManager().get_contacts() if remote_uri in (addr.uri for addr in contact.uris))\n        except StopIteration:\n            display_name = remote_identity.display_name\n        else:\n            display_name = contact.name\n        match = self.phone_number_re.match(remote_uri)\n        if match:\n            remote_uri = match.group('number')\n        if display_name and display_name != remote_uri:\n            remote_identity = '%s <%s>' % (display_name, remote_uri)\n        else:\n            remote_identity = remote_uri\n        return cls(remote_identity, remote_uri, unicode(session.account.id), call_time, duration)", "output": "def from_session(cls, session):\n        if session.start_time is None and session.end_time is not None:\n            # Session may have anded before it fully started\n            session.start_time = session.end_time\n        call_time = session.start_time or datetime.now()\n        if session.start_time and session.end_time:\n            duration = session.end_time - session.start_time\n        else:\n            duration = None\n        remote_identity = session.remote_identity\n        remote_uri = '%s@%s' % (remote_identity.uri.user, remote_identity.uri.host)\n        try:\n            contact = next(contact for contact in AddressbookManager().get_contacts() if remote_uri in (addr.uri for addr in contact.uris))\n        except StopIteration:\n            display_name = remote_identity.display_name\n        else:\n            display_name = contact.name\n        match = cls.phone_number_re.match(remote_uri)\n        if match:\n            remote_uri = match.group('number')\n        if display_name and display_name != remote_uri:\n            remote_identity = '%s <%s>' % (display_name, remote_uri)\n        else:\n            remote_identity = remote_uri\n        return cls(remote_identity, remote_uri, unicode(session.account.id), call_time, duration)"}
{"input": "def handle_oidc_request(self, areq, redirect_uri):\n        \"\"\"\n\n        :param areq:\n        :param redirect_uri:\n        :return:\n        \"\"\"\n        if \"request_uri\" in areq:\n            # Do a HTTP get\n            logger.debug('Get request from request_uri')\n            try:\n                http_req = self.server.http_request(areq[\"request_uri\"])\n            except ConnectionError:\n                logger.error('Connection Error')\n                return self._authz_error(\"invalid_request_uri\")\n\n            if not http_req:\n                logger.error('Nothing returned')\n                return self._authz_error(\"invalid_request_uri\")\n\n            if http_req.status_code >= 400:\n                logger.error('HTTP error {}:{}'.format(http_req.status_code,\n                                                       http_req.text))\n                return self._redirect_authz_error('invalid_request',\n                                                  redirect_uri)\n            try:\n                resq = self._parse_openid_request(http_req.text)\n            except Exception as err:\n                logger.error(\n                    '{} encountered while parsing fetched request'.format(err))\n                return self._redirect_authz_error(\n                    \"invalid_openid_request_object\", redirect_uri)\n\n            logger.debig('Fetched request: {}'.format(resq))\n            areq[\"request\"] = resq\n\n        # The \"request\" in areq case is handled by .verify()\n\n        return areq", "output": "def handle_oidc_request(self, areq, redirect_uri):\n        \"\"\"\n\n        :param areq:\n        :param redirect_uri:\n        :return:\n        \"\"\"\n        if \"request_uri\" in areq:\n            # Do a HTTP get\n            logger.debug('Get request from request_uri')\n            try:\n                http_req = self.server.http_request(areq[\"request_uri\"])\n            except ConnectionError:\n                logger.error('Connection Error')\n                return self._authz_error(\"invalid_request_uri\")\n\n            if not http_req:\n                logger.error('Nothing returned')\n                return self._authz_error(\"invalid_request_uri\")\n\n            if http_req.status_code >= 400:\n                logger.error('HTTP error {}:{}'.format(http_req.status_code,\n                                                       http_req.text))\n                return self._redirect_authz_error('invalid_request',\n                                                  redirect_uri)\n            try:\n                resq = self._parse_openid_request(http_req.text)\n            except Exception as err:\n                logger.error(\n                    '{} encountered while parsing fetched request'.format(err))\n                return self._redirect_authz_error(\n                    \"invalid_openid_request_object\", redirect_uri)\n\n            logger.debug('Fetched request: {}'.format(resq))\n            areq[\"request\"] = resq\n\n        # The \"request\" in areq case is handled by .verify()\n\n        return areq"}
{"input": "def connect(self, address=tuple(), reattempt=True, use_tls=True):\n        \"\"\"\n        Connect to the XMPP server.\n\n        When no address is given, a SRV lookup for the server will\n        be attempted. If that fails, the server user in the JID\n        will be used.\n\n        Arguments:\n            address   -- A tuple containing the server's host and port.\n            reattempt -- If True, reattempt the connection if an\n                         error occurs. Defaults to True.\n            use_tls   -- Indicates if TLS should be used for the\n                         connection. Defaults to True.\n        \"\"\"\n        self.session_started_event.clear()\n        if not address or len(address) < 2:\n            if not self.srv_support:\n                log.debug(\"Did not supply (address, port) to connect\" + \\\n                              \" to and no SRV support is installed\" + \\\n                              \" (http://www.dnspython.org).\" + \\\n                              \" Continuing to attempt connection, using\" + \\\n                              \" server hostname from JID.\")\n            else:\n                log.debug(\"Since no address is supplied,\" + \\\n                              \"attempting SRV lookup.\")\n                try:\n                    xmpp_srv = \"_xmpp-client._tcp.%s\" % self.boundjid.host\n                    answers = dns.resolver.query(xmpp_srv, dns.rdatatype.SRV)\n                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                    log.debug(\"No appropriate SRV record found.\" + \\\n                                  \" Using JID server name.\")\n                except (dns.exception.Timeout,):\n                    log.debug(\"DNS resolution timed out.\")\n                else:\n                    # Pick a random server, weighted by priority.\n\n                    addresses = {}\n                    intmax = 0\n                    topprio = 65535\n                    for answer in answers:\n                        topprio = min(topprio, answer.priority)\n                    for answer in answers:\n                        if answer.priority == topprio:\n                            intmax += answer.weight\n                            addresses[intmax] = (answer.target.to_text()[:-1],\n                                             answer.port)\n\n                    #python3 returns a generator for dictionary keys\n                    items = [x for x in addresses.keys()]\n                    items.sort()\n\n                    picked = random.randint(0, intmax)\n                    for item in items:\n                        if picked <= priority:\n                            address = addresses[item]\n                            break\n\n        if not address:\n            # If all else fails, use the server from the JID.\n            address = (self.boundjid.host, 5222)\n\n        return XMLStream.connect(self, address[0], address[1],\n                                 use_tls=use_tls, reattempt=reattempt)", "output": "def connect(self, address=tuple(), reattempt=True, use_tls=True):\n        \"\"\"\n        Connect to the XMPP server.\n\n        When no address is given, a SRV lookup for the server will\n        be attempted. If that fails, the server user in the JID\n        will be used.\n\n        Arguments:\n            address   -- A tuple containing the server's host and port.\n            reattempt -- If True, reattempt the connection if an\n                         error occurs. Defaults to True.\n            use_tls   -- Indicates if TLS should be used for the\n                         connection. Defaults to True.\n        \"\"\"\n        self.session_started_event.clear()\n        if not address or len(address) < 2:\n            if not self.srv_support:\n                log.debug(\"Did not supply (address, port) to connect\" + \\\n                              \" to and no SRV support is installed\" + \\\n                              \" (http://www.dnspython.org).\" + \\\n                              \" Continuing to attempt connection, using\" + \\\n                              \" server hostname from JID.\")\n            else:\n                log.debug(\"Since no address is supplied,\" + \\\n                              \"attempting SRV lookup.\")\n                try:\n                    xmpp_srv = \"_xmpp-client._tcp.%s\" % self.boundjid.host\n                    answers = dns.resolver.query(xmpp_srv, dns.rdatatype.SRV)\n                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                    log.debug(\"No appropriate SRV record found.\" + \\\n                                  \" Using JID server name.\")\n                except (dns.exception.Timeout,):\n                    log.debug(\"DNS resolution timed out.\")\n                else:\n                    # Pick a random server, weighted by priority.\n\n                    addresses = {}\n                    intmax = 0\n                    topprio = 65535\n                    for answer in answers:\n                        topprio = min(topprio, answer.priority)\n                    for answer in answers:\n                        if answer.priority == topprio:\n                            intmax += answer.weight\n                            addresses[intmax] = (answer.target.to_text()[:-1],\n                                             answer.port)\n\n                    #python3 returns a generator for dictionary keys\n                    items = [x for x in addresses.keys()]\n                    items.sort()\n\n                    picked = random.randint(0, intmax)\n                    for item in items:\n                        if picked <= item:\n                            address = addresses[item]\n                            break\n\n        if not address:\n            # If all else fails, use the server from the JID.\n            address = (self.boundjid.host, 5222)\n\n        return XMLStream.connect(self, address[0], address[1],\n                                 use_tls=use_tls, reattempt=reattempt)"}
{"input": "def stop(self, force=False):\n        self.server.stop(force=Force)", "output": "def stop(self, force=False):\n        self.server.stop(force=force)"}
{"input": "def make_iq(self, id=0, ifrom=None, ito=None, type=None, query=None):\n        \"\"\"\n        Create a new Iq stanza with a given Id and from JID.\n\n        Arguments:\n            id    -- An ideally unique ID value for this stanza thread.\n                     Defaults to 0.\n            ifrom -- The from JID to use for this stanza.\n            ito   -- The destination JID for this stanza.\n            type  -- The Iq's type, one of: get, set, result, or error.\n            query -- Optional namespace for adding a query element.\n        \"\"\"\n        iq = self.Iq()\n        iq['id'] = str(id)\n        iq['to'] = ito\n        iq['from'] = ifrom\n        iq['type'] = itype\n        iq['query'] = query\n        return iq", "output": "def make_iq(self, id=0, ifrom=None, ito=None, itype=None, query=None):\n        \"\"\"\n        Create a new Iq stanza with a given Id and from JID.\n\n        Arguments:\n            id    -- An ideally unique ID value for this stanza thread.\n                     Defaults to 0.\n            ifrom -- The from JID to use for this stanza.\n            ito   -- The destination JID for this stanza.\n            type  -- The Iq's type, one of: get, set, result, or error.\n            query -- Optional namespace for adding a query element.\n        \"\"\"\n        iq = self.Iq()\n        iq['id'] = str(id)\n        iq['to'] = ito\n        iq['from'] = ifrom\n        iq['type'] = itype\n        iq['query'] = query\n        return iq"}
{"input": "def write(self, fd, content, length, pos=None):\n\t\t# TODO: error handling\n\t\t# TODO: symbolic support\n\t\tfd = self.state.make_concrete_int(fd)\n\t\tlength = self.state.make_concrete_int(fd)\n\t\treturn self.files[fd].write(content, length, pos)", "output": "def write(self, fd, content, length, pos=None):\n\t\t# TODO: error handling\n\t\t# TODO: symbolic support\n\t\tfd = self.state.make_concrete_int(fd)\n\t\tlength = self.state.make_concrete_int(length)\n\t\treturn self.files[fd].write(content, length, pos)"}
{"input": "def sparseness(x, which='LTS'):\n    \"\"\"Calculate sparseness.\n\n    Sparseness comes in two flavors:\n\n    **Lifetime kurtosis (LTK)** quantifies the widths of tuning curves\n    (according to Muench & Galizia, 2016):\n\n    .. math::\n\n        S = \\\\Bigg\\\\{ \\\\frac{1}{N} \\\\sum^M_{i=1} \\\\Big[ \\\\frac{r_i - \\\\overline{r}}{\\\\sigma_r} \\\\Big] ^4  \\\\Bigg\\\\} - 3\n\n    where :math:`N` is the number of observations, :math:`r_i` the value of\n    observation :math:`i`, and :math:`\\\\overline{r}` and\n    :math:`\\\\sigma_r` the mean and the standard deviation of the observations'\n    values, respectively. LTK is assuming a normal, or at least symmetric\n    distribution.\n\n    **Lifetime sparseness (LTS)** quantifies selectivity\n    (Bhandawat et al., 2007):\n\n    .. math::\n\n        S = \\\\frac{1}{1-1/N} \\\\Bigg[1- \\\\frac{\\\\big(\\\\sum^N_{j=1} r_j / N\\\\big)^2}{\\\\sum^N_{j=1} r_j^2 / N} \\\\Bigg]\n\n    where :math:`N` is the number of observations, and :math:`r_j` is the\n    value of an observation.\n\n    Notes\n    -----\n    ``NaN`` values will be ignored. You can use that to e.g. ignore zero\n    values in a large connectivity matrix by changing these values to ``NaN``\n    before passing it to ``pymaid.sparseness``.\n\n\n    Parameters\n    ----------\n    x :         DataFrame | array-like\n                (N, M) dataset with N (rows) observations for M (columns)\n                neurons. One-dimensional data will be converted to two\n                dimensions (N rows, 1 column).\n    which :     \"LTS\" | \"LTK\"\n                Determines whether lifetime sparseness (LTS) or lifetime\n                kurtosis (LTK) is returned.\n\n    Returns\n    -------\n    sparseness\n                ``pandas.Series`` if input was pandas DataFrame, else\n                ``numpy.array``.\n\n    Examples\n    --------\n    Calculate sparseness of olfactory inputs to group of neurons:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> # Generate adjacency matrix\n    >>> adj = pymaid.adjacency_matrix(s='annotation:WTPN2017_excitatory_uPN_right',\n    ...                               t='annotation:ASB LHN')\n    >>> # Calculate lifetime sparseness\n    >>> S = pymaid.sparseness(adj, which='LTS')\n    >>> # Plot distribution\n    >>> ax = S.plot.hist(bins=np.arange(0, 1, .1))\n    >>> ax.set_xlabel('LTS')\n    >>> plt.show()\n\n    \"\"\"\n    if not isinstance(x, (pd.DataFrame, np.ndarray)):\n        x = np.array(x)\n\n    # Make sure we are working with 2 dimensional data\n    if isinstance(x, np.ndarray) and x.ndim == 1:\n        x = x.reshape(x.shape[0], 1)\n\n    N = np.sum(~np.isnan(x), axis=0)\n\n    if which == 'LTK':\n        return np.nansum(((x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0)) ** 4, axis=0) / N - 3\n    elif which == 'LTS':\n        return 1 / (1 - (1 / N)) * (1 - np.nansum(x / N, axis=0) ** 2 / np.nansum(x ** 2 / N, axis=0))\n    else:\n        raise ValueError('Parameter \"which\" must be either \"LTS\" or \"LTK\"')", "output": "def sparseness(x, which='LTS'):\n    \"\"\"Calculate sparseness.\n\n    Sparseness comes in two flavors:\n\n    **Lifetime kurtosis (LTK)** quantifies the widths of tuning curves\n    (according to Muench & Galizia, 2016):\n\n    .. math::\n\n        S = \\\\Bigg\\\\{ \\\\frac{1}{N} \\\\sum^N_{i=1} \\\\Big[ \\\\frac{r_i - \\\\overline{r}}{\\\\sigma_r} \\\\Big] ^4  \\\\Bigg\\\\} - 3\n\n    where :math:`N` is the number of observations, :math:`r_i` the value of\n    observation :math:`i`, and :math:`\\\\overline{r}` and\n    :math:`\\\\sigma_r` the mean and the standard deviation of the observations'\n    values, respectively. LTK is assuming a normal, or at least symmetric\n    distribution.\n\n    **Lifetime sparseness (LTS)** quantifies selectivity\n    (Bhandawat et al., 2007):\n\n    .. math::\n\n        S = \\\\frac{1}{1-1/N} \\\\Bigg[1- \\\\frac{\\\\big(\\\\sum^N_{j=1} r_j / N\\\\big)^2}{\\\\sum^N_{j=1} r_j^2 / N} \\\\Bigg]\n\n    where :math:`N` is the number of observations, and :math:`r_j` is the\n    value of an observation.\n\n    Notes\n    -----\n    ``NaN`` values will be ignored. You can use that to e.g. ignore zero\n    values in a large connectivity matrix by changing these values to ``NaN``\n    before passing it to ``pymaid.sparseness``.\n\n\n    Parameters\n    ----------\n    x :         DataFrame | array-like\n                (N, M) dataset with N (rows) observations for M (columns)\n                neurons. One-dimensional data will be converted to two\n                dimensions (N rows, 1 column).\n    which :     \"LTS\" | \"LTK\"\n                Determines whether lifetime sparseness (LTS) or lifetime\n                kurtosis (LTK) is returned.\n\n    Returns\n    -------\n    sparseness\n                ``pandas.Series`` if input was pandas DataFrame, else\n                ``numpy.array``.\n\n    Examples\n    --------\n    Calculate sparseness of olfactory inputs to group of neurons:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> # Generate adjacency matrix\n    >>> adj = pymaid.adjacency_matrix(s='annotation:WTPN2017_excitatory_uPN_right',\n    ...                               t='annotation:ASB LHN')\n    >>> # Calculate lifetime sparseness\n    >>> S = pymaid.sparseness(adj, which='LTS')\n    >>> # Plot distribution\n    >>> ax = S.plot.hist(bins=np.arange(0, 1, .1))\n    >>> ax.set_xlabel('LTS')\n    >>> plt.show()\n\n    \"\"\"\n    if not isinstance(x, (pd.DataFrame, np.ndarray)):\n        x = np.array(x)\n\n    # Make sure we are working with 2 dimensional data\n    if isinstance(x, np.ndarray) and x.ndim == 1:\n        x = x.reshape(x.shape[0], 1)\n\n    N = np.sum(~np.isnan(x), axis=0)\n\n    if which == 'LTK':\n        return np.nansum(((x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0)) ** 4, axis=0) / N - 3\n    elif which == 'LTS':\n        return 1 / (1 - (1 / N)) * (1 - np.nansum(x / N, axis=0) ** 2 / np.nansum(x ** 2 / N, axis=0))\n    else:\n        raise ValueError('Parameter \"which\" must be either \"LTS\" or \"LTK\"')"}
{"input": "def build_report_manager(opt):\n    if opt.tensorboard:\n        from tensorboard import SummaryWriter\n        writer = SummaryWriter(opt.tensorboard_log_dir\n                               + datetime.now().strftime(\"/%b-%d_%H-%M-%S\"),\n                               comment=\"Unmt\")\n    else:\n        writer = None\n\n    report_mgr = ReportMgr(opt.report_every, start_time=-1,\n                           tensorboard_writer=writer)\n    return report_mgr", "output": "def build_report_manager(opt):\n    if opt.tensorboard:\n        from tensorboardX import SummaryWriter\n        writer = SummaryWriter(opt.tensorboard_log_dir\n                               + datetime.now().strftime(\"/%b-%d_%H-%M-%S\"),\n                               comment=\"Unmt\")\n    else:\n        writer = None\n\n    report_mgr = ReportMgr(opt.report_every, start_time=-1,\n                           tensorboard_writer=writer)\n    return report_mgr"}
{"input": "def _get_namespace(m):\n    \"\"\"\n    This is used by _lambdify to parse its arguments.\n    \"\"\"\n    if isinstance(m, str):\n        _import(m)\n        return MODULES[m][0]\n    elif isinstance(m, dict):\n        return m\n    elif hasattr(m, \"__dict__\"):\n        return m.__dict__\n    else:\n        raise TypeError(\"Argument must be either a string, dict or module but it is: %s\" % m)", "output": "def _get_namespace(m):\n    \"\"\"\n    This is used by _lambdify to parse its arguments.\n    \"\"\"\n    if isinstance(m, string_types):\n        _import(m)\n        return MODULES[m][0]\n    elif isinstance(m, dict):\n        return m\n    elif hasattr(m, \"__dict__\"):\n        return m.__dict__\n    else:\n        raise TypeError(\"Argument must be either a string, dict or module but it is: %s\" % m)"}
{"input": "def _print_compiler_version(self, cc):\n        print(\"C compiler:\")\n        if self.msvc:\n            cc.spawn(cc.compiler)\n        else:\n            cc.spawn(cc.compiler + ['-v'])", "output": "def _print_compiler_version(self, cc):\n        print(\"C compiler:\")\n        if self.msvc:\n            cc.spawn(cc.cc)\n        else:\n            cc.spawn(cc.compiler + ['-v'])"}
{"input": "def _NH_SIPAccountManagerDidRemoveAccount(self, notification):\n        account = notification.data.account\n        action = (action for action in self.accounts_menu.actions() if action.data().toPyObject() is account).next()\n        self.account_menu.removeAction(action)\n        if isinstance(account, Account) and account.enabled and account.message_summary.enabled:\n            action = (action for action in self.voicemail_menu.actions() if action.data().toPyObject()[0] is account).next()\n            self.voicemail_menu.removeAction(action)", "output": "def _NH_SIPAccountManagerDidRemoveAccount(self, notification):\n        account = notification.data.account\n        action = (action for action in self.accounts_menu.actions() if action.data().toPyObject() is account).next()\n        self.accounts_menu.removeAction(action)\n        if isinstance(account, Account) and account.enabled and account.message_summary.enabled:\n            action = (action for action in self.voicemail_menu.actions() if action.data().toPyObject()[0] is account).next()\n            self.voicemail_menu.removeAction(action)"}
{"input": "def data_loader(q, ):\n    for start in range(0, len(filenames), batch_size):\n        x_batch = []\n        end = min(start + batch_size, len(filenames))\n        filenames_batch = filenames[start:end]\n\n        for filename in filenames_batch:\n            img = img_to_array(load_img(filename))\n            x_batch.append(img)\n\n        x_batch = preprocess_input(np.array(x_batch, np.float32), mode=args.preprocess_input)\n        padded_x = np.zeros((batch_size, 1280, 1920, 3))\n        padded_x[:, :, 1:-1, :] = x_batch\n        q.put((filenames_batch, padded_x))\n\n    for gpu in gpus:\n        q.put((None, None))", "output": "def data_loader(q, ):\n    for start in range(0, len(filenames), batch_size):\n        x_batch = []\n        end = min(start + batch_size, len(filenames))\n        filenames_batch = filenames[start:end]\n\n        for filename in filenames_batch:\n            img = img_to_array(load_img(filename))\n            x_batch.append(img)\n\n        x_batch = preprocess_input(np.array(x_batch, np.float32), mode=args.preprocessing_function)\n        padded_x = np.zeros((batch_size, 1280, 1920, 3))\n        padded_x[:, :, 1:-1, :] = x_batch\n        q.put((filenames_batch, padded_x))\n\n    for gpu in gpus:\n        q.put((None, None))"}
{"input": "def __init__(self, session, parent=None):\n        super(SessionWidget, self).__init__(parent)\n        with Resources.directory:\n            self.setupUi(self)\n        font = self.latency_label.font()\n        font.setPointSizeF(self.status_label.fontInfo().pointSizeF() - 1)\n        self.latency_label.setFont(font)\n        font = self.packet_loss_label.font()\n        font.setPointSizeF(self.status_label.fontInfo().pointSizeF() - 1)\n        self.packet_loss_label.setFont(font)\n        self.mute_button.type = LeftSegment\n        self.hold_button.type = MiddleSegment\n        self.record_button.type = MiddleSegment\n        self.hangup_button.type = RightSegment\n        self.selected = False\n        self.drop_indicator = False\n        self.conference_position = None\n        self._disable_dnd = False\n        self.setFocusProxy(parent)\n        self.mute_button.hidden.connect(self._mute_button_hidden)\n        self.mute_button.shown.connect(self._mute_button_shown)\n        self.mute_button.pressed.connect(self._tool_button_pressed)\n        self.hold_button.pressed.connect(self._tool_button_pressed)\n        self.record_button.pressed.connect(self._tool_button_pressed)\n        self.hangup_button.pressed.connect(self._tool_button_pressed)\n        self.mute_button.hide()\n        self.address_label.setText(session.name or session.uri)\n        self.stream_info_label.session_type = session.type\n        self.stream_info_label.session_type = session.codec_info\n        self.latency_label.value = session.latency\n        self.packet_loss_label.value = session.packet_loss\n        self.tls_label.setVisible(bool(session.tls))\n        self.srtp_label.setVisible(bool(session.srtp))", "output": "def __init__(self, session, parent=None):\n        super(SessionWidget, self).__init__(parent)\n        with Resources.directory:\n            self.setupUi(self)\n        font = self.latency_label.font()\n        font.setPointSizeF(self.status_label.fontInfo().pointSizeF() - 1)\n        self.latency_label.setFont(font)\n        font = self.packet_loss_label.font()\n        font.setPointSizeF(self.status_label.fontInfo().pointSizeF() - 1)\n        self.packet_loss_label.setFont(font)\n        self.mute_button.type = LeftSegment\n        self.hold_button.type = MiddleSegment\n        self.record_button.type = MiddleSegment\n        self.hangup_button.type = RightSegment\n        self.selected = False\n        self.drop_indicator = False\n        self.conference_position = None\n        self._disable_dnd = False\n        self.setFocusProxy(parent)\n        self.mute_button.hidden.connect(self._mute_button_hidden)\n        self.mute_button.shown.connect(self._mute_button_shown)\n        self.mute_button.pressed.connect(self._tool_button_pressed)\n        self.hold_button.pressed.connect(self._tool_button_pressed)\n        self.record_button.pressed.connect(self._tool_button_pressed)\n        self.hangup_button.pressed.connect(self._tool_button_pressed)\n        self.mute_button.hide()\n        self.address_label.setText(session.name or session.uri)\n        self.stream_info_label.session_type = session.type\n        self.stream_info_label.codec_info = session.codec_info\n        self.latency_label.value = session.latency\n        self.packet_loss_label.value = session.packet_loss\n        self.tls_label.setVisible(bool(session.tls))\n        self.srtp_label.setVisible(bool(session.srtp))"}
{"input": "def _current_conversation_page_changed(self, new_index: int):\n        page = self.ui.conversations_view.widget(new_index)\n        if not page:\n            logger.warning(\"page changed to nonexistant widget: %d\",\n                           new_index)\n            return\n\n        try:\n            conversation = self.__pagemap[page]\n        except KeyError:\n            return\n        conv_index = self.main.conversations.index(conversation)\n        model_index = self.ui.conversations_view.model().index(\n            conv_index,\n            0,\n            Qt.QModelIndex()\n        )\n        self.ui.conversations_view.selectionModel().select(\n            model_index,\n            Qt.QItemSelectionModel.ClearAndSelect |\n            Qt.QItemSelectionModel.Current,\n        )", "output": "def _current_conversation_page_changed(self, new_index: int):\n        page = self.ui.conversation_pages.widget(new_index)\n        if not page:\n            logger.warning(\"page changed to nonexistant widget: %d\",\n                           new_index)\n            return\n\n        try:\n            conversation = self.__pagemap[page]\n        except KeyError:\n            return\n        conv_index = self.main.conversations.index(conversation)\n        model_index = self.ui.conversations_view.model().index(\n            conv_index,\n            0,\n            Qt.QModelIndex()\n        )\n        self.ui.conversations_view.selectionModel().select(\n            model_index,\n            Qt.QItemSelectionModel.ClearAndSelect |\n            Qt.QItemSelectionModel.Current,\n        )"}
{"input": "def clear_transition_graph(self):\n        self.blocks = { self._projects.factory.block(self._addr) }\n        self._transition_graph = networkx.DiGraph()\n        self._transition_graph.add_node(self._addr)\n        self._local_transition_graph = None", "output": "def clear_transition_graph(self):\n        self.blocks = { self._project.factory.block(self._addr) }\n        self._transition_graph = networkx.DiGraph()\n        self._transition_graph.add_node(self._addr)\n        self._local_transition_graph = None"}
{"input": "def test_Interval_inf():\n    neinf = SetExpr(-oo)\n    poinf = SetExpr(oo)\n    set_ff = SetExpr(Interval(0, 5))\n    set_if = SetExpr(Interval(-oo, 5))\n    set_fi = SetExpr(Interval(0, oo))\n    set_ii = SetExpr(Interval(-oo, oo))\n    fs_ni = SetExpr(FiniteSet(S.NetgativeInfinity))\n    fs_pi = SetExpr(FiniteSet(S.Infinity))\n\n    assert set_ff + neinf == fs_ni\n    assert set_ff + poinf == fs_pi\n    assert set_fi + neinf == set_ii\n    assert set_if + poinf == set_ii\n\n    assert set_ff - poinf == fs_ni\n    assert set_ff - neinf == fs_pi\n    assert set_if - neinf == set_ii\n    assert set_fi - poinf == set_ii", "output": "def test_Interval_inf():\n    neinf = SetExpr(-oo)\n    poinf = SetExpr(oo)\n    set_ff = SetExpr(Interval(0, 5))\n    set_if = SetExpr(Interval(-oo, 5))\n    set_fi = SetExpr(Interval(0, oo))\n    set_ii = SetExpr(Interval(-oo, oo))\n    fs_ni = SetExpr(FiniteSet(S.NegativeInfinity))\n    fs_pi = SetExpr(FiniteSet(S.Infinity))\n\n    assert set_ff + neinf == fs_ni\n    assert set_ff + poinf == fs_pi\n    assert set_fi + neinf == set_ii\n    assert set_if + poinf == set_ii\n\n    assert set_ff - poinf == fs_ni\n    assert set_ff - neinf == fs_pi\n    assert set_if - neinf == set_ii\n    assert set_fi - poinf == set_ii"}
{"input": "def drive_compile(args):\n    \"\"\"Invoke tvmc.compiler module with command line arguments\n\n    Parameters\n    ----------\n    args: argparse.Namespace\n        Arguments from command line parser.\n\n    Returns\n    -------\n    int\n        Zero if successfully completed\n\n    \"\"\"\n    mod, params = frontends.load_model(args.FILE, args.model_format, args.input_shapes)\n\n    graph, lib, params, dumps = compile_model(\n        mod,\n        params,\n        args.target,\n        args.dump_code,\n        None,\n        args.tuning_records,\n        args.desired_layout,\n        args.disabled_pass,\n    )\n\n    if dumps:\n        save_dumps(args.output, dumps)\n\n    save_module(args.output, graph, lib, params, args.cross_compiler)\n    return 0", "output": "def drive_compile(args):\n    \"\"\"Invoke tvmc.compiler module with command line arguments\n\n    Parameters\n    ----------\n    args: argparse.Namespace\n        Arguments from command line parser.\n\n    Returns\n    -------\n    int\n        Zero if successfully completed\n\n    \"\"\"\n    mod, params = frontends.load_model(args.FILE, args.model_format, args.input_shapes)\n\n    graph, lib, params, dumps = compile_model(\n        mod,\n        params,\n        args.target,\n        args.dump_code,\n        None,\n        args.tuning_records,\n        args.desired_layout,\n        args.disable_pass,\n    )\n\n    if dumps:\n        save_dumps(args.output, dumps)\n\n    save_module(args.output, graph, lib, params, args.cross_compiler)\n    return 0"}
{"input": "def finish_action_elier_frame(self):\n        self.elier_frame[0].finish_frame_action()\n        self.elier_frame.remove(self.next_frame[0])\n        self.set_off()\n        self.finish_signal.emit()", "output": "def finish_action_elier_frame(self):\n        self.elier_frame[0].finish_frame_action()\n        self.elier_frame.remove(self.elier_frame[0])\n        self.set_off()\n        self.finish_signal.emit()"}
{"input": "def _create_main_frame(self):\n        \"\"\"\n        Helper to initialize the main gui frame.\n        \"\"\"\n        # data_modification = self._create_data_modification_tab()\n        self.event_finding_tab = tabs.EventFindingTab(self)\n        self.event_finding_tab.set_on_files_opened_callback(self._on_files_opened)\n        self.event_finding_tab.set_on_status_update_callback(self.set_status)\n        self.event_finding_tab.set_process_events_callback(self._process_events)\n\n        event_viewer = self._create_event_viewer_tab()\n        event_analysis = self._create_event_analysis_tab()\n        \n        # Layout holding everything        \n        self.main_tabwig = QtGui.QTabWidget()\n        # self.main_tabwig.addTab(data_modification, 'Data modification')\n        self.main_tabwig.addTab(self.event_finding_tab, 'Event Finding')\n        self.main_tabwig.addTab(event_viewer, 'Event View')\n        self.main_tabwig.addTab(event_analysis, 'Event Analysis')\n        self.main_tabwig.setMinimumSize(1000, 550)\n        \n        text = \"\"\"*********************\nWelcome to pyporegui!\n\nIf you are unfamiliar with the python console, feel free to ignore this console.\n\nHowever, you can use this console to interact with your data and the gui!\nType globals() to see globally defined variabels.\nType locals() to see application-specific variables.\n\nThe current namespace should include:\n    np        -    numpy\n    pg        -    pyqtgraph\n    ed        -    pypore.eventDatabase\n    currentPlot -  Top plot in the event finding tab.\n*********************\"\"\"\n\n        namespace = {'np': np, 'pg': pg, 'ed': ed, 'currentPlot': self.event_finding_tab.plotwid}\n        self.console = pgc.ConsoleWidget(namespace=namespace, text=text)\n        \n        frame = QtGui.QSplitter()\n        frame.setOrientation(QtCore.Qt.Vertical)\n        frame.addWidget(self.main_tabwig)\n        frame.addWidget(self.console)\n        \n        self.setCentralWidget(frame)", "output": "def _create_main_frame(self):\n        \"\"\"\n        Helper to initialize the main gui frame.\n        \"\"\"\n        # data_modification = self._create_data_modification_tab()\n        self.event_finding_tab = tabs.EventFindingTab(self)\n        self.event_finding_tab.set_on_files_opened_callback(self._on_files_opened)\n        self.event_finding_tab.set_on_status_update_callback(self.set_status)\n        self.event_finding_tab.set_process_events_callback(self._process_events)\n\n        event_viewer = self._create_event_viewer_tab()\n        event_analysis = self._create_event_analysis_tab()\n        \n        # Layout holding everything        \n        self.main_tabwig = QtGui.QTabWidget()\n        # self.main_tabwig.addTab(data_modification, 'Data modification')\n        self.main_tabwig.addTab(self.event_finding_tab, 'Event Finding')\n        self.main_tabwig.addTab(event_viewer, 'Event View')\n        self.main_tabwig.addTab(event_analysis, 'Event Analysis')\n        self.main_tabwig.setMinimumSize(1000, 550)\n        \n        text = \"\"\"*********************\nWelcome to pyporegui!\n\nIf you are unfamiliar with the python console, feel free to ignore this console.\n\nHowever, you can use this console to interact with your data and the gui!\nType globals() to see globally defined variabels.\nType locals() to see application-specific variables.\n\nThe current namespace should include:\n    np        -    numpy\n    pg        -    pyqtgraph\n    ed        -    pypore.eventDatabase\n    currentPlot -  Top plot in the event finding tab.\n*********************\"\"\"\n\n        namespace = {'np': np, 'pg': pg, 'ed': ed, 'currentPlot': self.event_finding_tab.plot_widget}\n        self.console = pgc.ConsoleWidget(namespace=namespace, text=text)\n        \n        frame = QtGui.QSplitter()\n        frame.setOrientation(QtCore.Qt.Vertical)\n        frame.addWidget(self.main_tabwig)\n        frame.addWidget(self.console)\n        \n        self.setCentralWidget(frame)"}
{"input": "def test_flow_get_with_array_args(self):\n        \"\"\"Verify that cassette can store array args.\"\"\"\n\n        param = {\n            \"dict1\": {\"dict2\": \"dict3\", \"dict4\": \"dict5\"},\n            \"array\": [\"item1\", \"item2\"],\n            \"int\": 1,\n            \"param\": \"1\",\n        }\n\n        url = \"/get?\"\n        url += _encode_params(param)\n        r = self.check_urllib2_flow(url=url)\n        self.assertEqual(r.json[\"args\"][\"param\"], \"1\")", "output": "def test_flow_get_with_array_args(self):\n        \"\"\"Verify that cassette can store array args.\"\"\"\n\n        param = {\n            \"dict1\": {\"dict2\": \"dict3\", \"dict4\": \"dict5\"},\n            \"array\": [\"item1\", \"item2\"],\n            \"int\": 1,\n            \"param\": \"1\",\n        }\n\n        url = \"get?\"\n        url += _encode_params(param)\n        r = self.check_urllib2_flow(url=url)\n        self.assertEqual(r.json[\"args\"][\"param\"], \"1\")"}
{"input": "def _bootstrap_rescue(self, install):\n        \"\"\"\n        Bootstrap everything needed in order to get Nix and the partitioner\n        usable in the rescue system. The latter is not only for partitioning\n        but also for mounting partitions.\n        \"\"\"\n        self.log_start(\"building Nix bootstrap installer...\")\n        bootstrap = subprocess.check_output([\n            \"nix-build\", \"<nixpkgs>\", \"--no-out-link\", \"-A\",\n            \"hetznerNixOpsInstaller\"\n        ]).rstrip()\n        self.log_end(\"done. ({0})\".format(bootstrap))\n\n        self.log_start(\"copying bootstrap files to rescue system...\")\n        tarstream = subprocess.Popen([bootstrap], stdout=subprocess.PIPE)\n        if not self.has_really_fast_connection():\n            stream = subprocess.Popen([\"gzip\", \"-c\"], stdin=tarstream.stdout,\n                                      stdout=subprocess.PIPE)\n            self.run_command(\"tar xz -C /\", stdin=stream.stdout)\n            stream.wait()\n        else:\n            self.run_command(\"tar x -C /\", stdin=tarstream.stdout)\n        tarstream.wait()\n        self.log_end(\"done.\")\n\n        if install:\n            self.log_start(\"partitioning disks...\")\n            out = self.run_command(\"nixpart -\", capture_stdout=True,\n                                   stdin_string=self.partitions)\n            self.fs_info = '\\n'.join(out.splitlines()[1:-1])\n        else:\n            self.log_start(\"mounting filesystems...\")\n            self.run_command(\"nixpart -m -\", stdin_string=self.partitions)\n        self.log_end(\"done.\")\n\n        if not install:\n            self.log_start(\"checking if system in /mnt is NixOS...\")\n            res = self.run_command(\"test -e /mnt/etc/NIXOS\", check=False)\n            if res == 0:\n                self.log_end(\"yes.\")\n            else:\n                self.log_end(\"NO! Not mounting special filesystems.\")\n                return\n\n        self.log_start(\"bind-mounting special filesystems...\")\n        for mountpoint in (\"/proc\", \"/dev\", \"/dev/shm\", \"/sys\"):\n            self.log_continue(\"{0}...\".format(mountpoint))\n            cmd = \"mkdir -m 0755 -p /mnt{0} && \".format(mountpoint)\n            cmd += \"mount --bind {0} /mnt{0}\".format(mountpoint)\n            self.run_command(cmd)\n        self.log_end(\"done.\")", "output": "def _bootstrap_rescue(self, install):\n        \"\"\"\n        Bootstrap everything needed in order to get Nix and the partitioner\n        usable in the rescue system. The latter is not only for partitioning\n        but also for mounting partitions.\n        \"\"\"\n        self.log_start(\"building Nix bootstrap installer...\")\n        bootstrap = subprocess.check_output([\n            \"nix-build\", \"<nixpkgs>\", \"--no-out-link\", \"-A\",\n            \"hetznerNixOpsInstaller\"\n        ]).rstrip()\n        self.log_end(\"done. ({0})\".format(bootstrap))\n\n        self.log_start(\"copying bootstrap files to rescue system...\")\n        tarstream = subprocess.Popen([bootstrap], stdout=subprocess.PIPE)\n        if not self.has_really_fast_connection():\n            stream = subprocess.Popen([\"gzip\", \"-c\"], stdin=tarstream.stdout,\n                                      stdout=subprocess.PIPE)\n            self.run_command(\"tar xz -C /\", stdin=stream.stdout)\n            stream.wait()\n        else:\n            self.run_command(\"tar x -C /\", stdin=tarstream.stdout)\n        tarstream.wait()\n        self.log_end(\"done.\")\n\n        if install:\n            self.log_start(\"partitioning disks...\")\n            out = self.run_command(\"nixpart -p -\", capture_stdout=True,\n                                   stdin_string=self.partitions)\n            self.fs_info = '\\n'.join(out.splitlines()[1:-1])\n        else:\n            self.log_start(\"mounting filesystems...\")\n            self.run_command(\"nixpart -m -\", stdin_string=self.partitions)\n        self.log_end(\"done.\")\n\n        if not install:\n            self.log_start(\"checking if system in /mnt is NixOS...\")\n            res = self.run_command(\"test -e /mnt/etc/NIXOS\", check=False)\n            if res == 0:\n                self.log_end(\"yes.\")\n            else:\n                self.log_end(\"NO! Not mounting special filesystems.\")\n                return\n\n        self.log_start(\"bind-mounting special filesystems...\")\n        for mountpoint in (\"/proc\", \"/dev\", \"/dev/shm\", \"/sys\"):\n            self.log_continue(\"{0}...\".format(mountpoint))\n            cmd = \"mkdir -m 0755 -p /mnt{0} && \".format(mountpoint)\n            cmd += \"mount --bind {0} /mnt{0}\".format(mountpoint)\n            self.run_command(cmd)\n        self.log_end(\"done.\")"}
{"input": "def _run_about(self, *args):\n        about = gtk.AboutDialog()\n        about.set_name(stoq.program_name)\n        about.set_version(stoq.version)\n        about.set_website(stoq.website)\n        release_date = stoq.release_date\n        about.set_comments('Release Date: %s' %\n                           datetime.datetime(*release_date).strftime('%x'))\n        about.set_copyright('Copyright (C) 2005 Async Open Source')\n\n        # Logo\n        icon_file = environ.find_resource('pixmaps', 'stoq_logo.png')\n        logo = gtk.gdk.pixbuf_new_from_file(icon_file)\n        about.set_logo(logo)\n\n        # License\n        license = environ.find_resource('docs', 'COPYING')\n        about.set_license(file(license).read())\n\n        # Authors & Contributors\n        authors = environ.find_resource('docs', 'AUTHORS')\n        lines = [a.strip() for a in file(authors).readlines()]\n        lines.append('') # separate authors from contributors\n        contributors = environ.find_resource('docs', 'CONTRIBUTORS')\n        lines.extend([c.strip() for c in file(contributors).readlines()])\n        about.set_authors(lines)\n\n        about.run()\n        about.destroy()", "output": "def _run_about(self, *args):\n        about = gtk.AboutDialog()\n        about.set_name(stoq.program_name)\n        about.set_version(stoq.version)\n        about.set_website(stoq.website)\n        release_date = stoq.release_date\n        about.set_comments('Release Date: %s' %\n                           datetime.datetime(*release_date).strftime('%x'))\n        about.set_copyright('Copyright (C) 2005, 2006 Async Open Source')\n\n        # Logo\n        icon_file = environ.find_resource('pixmaps', 'stoq_logo.png')\n        logo = gtk.gdk.pixbuf_new_from_file(icon_file)\n        about.set_logo(logo)\n\n        # License\n        license = environ.find_resource('docs', 'COPYING')\n        about.set_license(file(license).read())\n\n        # Authors & Contributors\n        authors = environ.find_resource('docs', 'AUTHORS')\n        lines = [a.strip() for a in file(authors).readlines()]\n        lines.append('') # separate authors from contributors\n        contributors = environ.find_resource('docs', 'CONTRIBUTORS')\n        lines.extend([c.strip() for c in file(contributors).readlines()])\n        about.set_authors(lines)\n\n        about.run()\n        about.destroy()"}
{"input": "def specshow(X, sr=22050, hop_length=64, x_axis=None, y_axis=None, fmin=None, fmax=None, **kwargs):\n    \"\"\"Display a spectrogram. Wraps to `~matplotlib.pyplot.imshow` with some handy defaults.\n    \n    :parameters:\n      - X : np.ndarray\n          Matrix to display (eg, spectrogram)\n\n      - sr : int > 0\n          Sample rate. Used to determine time scale in x-axis\n\n      - hop_length : int > 0\n          Hop length. Also used to determine time scale in x-axis\n\n      - x_axis : None or {'time', 'frames', 'off'}\n          If None or 'off', no x axis is displayed.\n          If 'time', markers are shown as seconds, minutes, or hours.\n          If 'frames', markers are shown as frame counts.\n\n      - y_axis : None or {'linear', 'mel', 'chroma', 'off'}\n          If None or 'off', no y axis is displayed.\n          If 'linear', frequency range is determined by the FFT window and sample rate.\n          If 'mel', frequencies are determined by the mel scale.\n          If 'chroma', pitches are determined by the chroma filters.\n\n     - fmin, fmax : float > 0 or None\n          Used for setting the Mel frequency scale\n\n     - kwargs : dict\n          Additional arguments passed through to ``matplotlib.pyplot.imshow``.\n\n    :returns:\n     - image : ``matplotlib.image.AxesImage``\n          As returned from ``matplotlib.pyplot.imshow``.\n\n    \"\"\"\n\n    kwargs['aspect']        = kwargs.get('aspect',          'auto')\n    kwargs['origin']        = kwargs.get('origin',          'lower')\n    kwargs['interpolation'] = kwargs.get('interpolation',   'nearest')\n\n    kwargs['cmap']          = kwargs.get('cmap',            'Purples')\n\n    axes = plt.imshow(X, **kwargs)\n\n    # Set up the x ticks\n    x_pos = np.arange(0, X.shape[1]+1, max(1, X.shape[1] / 5))\n    if x_axis is 'time':\n        # Reformat into seconds, or minutes:seconds\n        x_val = x_pos * (hop_length / np.float(sr))\n\n        if max(x_val) > 3600.0:\n            # reformat into hours:minutes:seconds\n            x_val = map(lambda y: '%d:%02d:%02d' % (int(y / 3600), int(np.mod(y, 3600)), int(np.mod(y, 60))), x_val)\n        elif max(x_val) > 60.0:\n            # reformat into minutes:seconds\n            x_val = map(lambda y: '%d:%02d' % (int(y / 60), int(np.mod(y, 60))), x_val)\n        else:\n            # reformat into seconds, down to the millisecond\n            x_val = np.around(x_val, 3)\n\n        plt.xticks(x_pos, x_val)\n        plt.xlabel('Time')\n\n    elif x_axis is 'frames':\n        # Nothing to do here, plot is in frames\n        plt.xticks(x_pos, x_pos)\n        plt.xlabel('Frames')\n        pass\n    elif x_axis is None or x_axis is 'off':\n        plt.xticks([])\n        plt.xlabel('')\n        pass\n    else:\n        raise ValueError('Unknown x_axis parameter: %s' % x_axis)\n\n    # Set up the y ticks\n    y_pos = np.arange(0, X.shape[0], max(1, X.shape[0] / 6))\n    if y_axis is 'linear':\n        y_val = np.fft.fftfreq( (X.shape[0] -1) * 2, 1./sr)[y_pos].astype(np.int)\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Hz')\n        pass\n    elif y_axis is 'mel':\n        m_args = {}\n        if fmin is not None:\n            m_args['fmin'] = fmin\n        if fmax is not None:\n            m_args['fmax'] = fmax\n\n        y_val = librosa.core.mel_frequencies(X.shape[0], **m_args)[y_pos].astype(np.int)\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Hz')\n        pass\n    elif y_axis is 'chroma':\n        y_pos = np.arange(0, X.shape[0], max(1, X.shape[0] / 12))\n        y_val = ['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#']\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Note')\n        pass\n    elif y_axis is None or y_axis is 'off':\n        plt.yticks([])\n        plt.ylabel('')\n        pass\n    else:\n        raise ValueError('Unknown y_axis parameter: %s' % y_axis)\n    \n    return axes", "output": "def specshow(X, sr=22050, hop_length=64, x_axis=None, y_axis=None, fmin=None, fmax=None, **kwargs):\n    \"\"\"Display a spectrogram. Wraps to `~matplotlib.pyplot.imshow` with some handy defaults.\n    \n    :parameters:\n      - X : np.ndarray\n          Matrix to display (eg, spectrogram)\n\n      - sr : int > 0\n          Sample rate. Used to determine time scale in x-axis\n\n      - hop_length : int > 0\n          Hop length. Also used to determine time scale in x-axis\n\n      - x_axis : None or {'time', 'frames', 'off'}\n          If None or 'off', no x axis is displayed.\n          If 'time', markers are shown as seconds, minutes, or hours.\n          If 'frames', markers are shown as frame counts.\n\n      - y_axis : None or {'linear', 'mel', 'chroma', 'off'}\n          If None or 'off', no y axis is displayed.\n          If 'linear', frequency range is determined by the FFT window and sample rate.\n          If 'mel', frequencies are determined by the mel scale.\n          If 'chroma', pitches are determined by the chroma filters.\n\n     - fmin, fmax : float > 0 or None\n          Used for setting the Mel frequency scale\n\n     - kwargs : dict\n          Additional arguments passed through to ``matplotlib.pyplot.imshow``.\n\n    :returns:\n     - image : ``matplotlib.image.AxesImage``\n          As returned from ``matplotlib.pyplot.imshow``.\n\n    \"\"\"\n\n    kwargs['aspect']        = kwargs.get('aspect',          'auto')\n    kwargs['origin']        = kwargs.get('origin',          'lower')\n    kwargs['interpolation'] = kwargs.get('interpolation',   'nearest')\n\n    kwargs['cmap']          = kwargs.get('cmap',            'OrRd')\n\n    axes = plt.imshow(X, **kwargs)\n\n    # Set up the x ticks\n    x_pos = np.arange(0, X.shape[1]+1, max(1, X.shape[1] / 5))\n    if x_axis is 'time':\n        # Reformat into seconds, or minutes:seconds\n        x_val = x_pos * (hop_length / np.float(sr))\n\n        if max(x_val) > 3600.0:\n            # reformat into hours:minutes:seconds\n            x_val = map(lambda y: '%d:%02d:%02d' % (int(y / 3600), int(np.mod(y, 3600)), int(np.mod(y, 60))), x_val)\n        elif max(x_val) > 60.0:\n            # reformat into minutes:seconds\n            x_val = map(lambda y: '%d:%02d' % (int(y / 60), int(np.mod(y, 60))), x_val)\n        else:\n            # reformat into seconds, down to the millisecond\n            x_val = np.around(x_val, 3)\n\n        plt.xticks(x_pos, x_val)\n        plt.xlabel('Time')\n\n    elif x_axis is 'frames':\n        # Nothing to do here, plot is in frames\n        plt.xticks(x_pos, x_pos)\n        plt.xlabel('Frames')\n        pass\n    elif x_axis is None or x_axis is 'off':\n        plt.xticks([])\n        plt.xlabel('')\n        pass\n    else:\n        raise ValueError('Unknown x_axis parameter: %s' % x_axis)\n\n    # Set up the y ticks\n    y_pos = np.arange(0, X.shape[0], max(1, X.shape[0] / 6))\n    if y_axis is 'linear':\n        y_val = np.fft.fftfreq( (X.shape[0] -1) * 2, 1./sr)[y_pos].astype(np.int)\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Hz')\n        pass\n    elif y_axis is 'mel':\n        m_args = {}\n        if fmin is not None:\n            m_args['fmin'] = fmin\n        if fmax is not None:\n            m_args['fmax'] = fmax\n\n        y_val = librosa.core.mel_frequencies(X.shape[0], **m_args)[y_pos].astype(np.int)\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Hz')\n        pass\n    elif y_axis is 'chroma':\n        y_pos = np.arange(0, X.shape[0], max(1, X.shape[0] / 12))\n        y_val = ['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#']\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Note')\n        pass\n    elif y_axis is None or y_axis is 'off':\n        plt.yticks([])\n        plt.ylabel('')\n        pass\n    else:\n        raise ValueError('Unknown y_axis parameter: %s' % y_axis)\n    \n    return axes"}
{"input": "def __init__(self, *args, **kwargs):\n        self.protected = kwargs.pop('protected', False)\n        self.transitions = {}  # cls -> (transitions name -> method)\n        self.state_proxy = {}  # state -> ProxyClsRef\n\n        state_choices = kwargs.pop('state_choices', None)\n        choices = kwargs.get('choices', None)\n        if state_choices is not None and choices is not None:\n            raise ValueError('Use one of choices or state_choces value')\n\n        if state_choices is not None:\n            choices = []\n            for state, title, proxy_cls_ref in state_choices:\n                choices.append((state, title))\n                self.state_proxy[state] = proxy_cls_ref\n            kwargs['choices'] = choices\n\n        super(FSMFieldMixin, self).__init__(*args, **kwargs)", "output": "def __init__(self, *args, **kwargs):\n        self.protected = kwargs.pop('protected', False)\n        self.transitions = {}  # cls -> (transitions name -> method)\n        self.state_proxy = {}  # state -> ProxyClsRef\n\n        state_choices = kwargs.pop('state_choices', None)\n        choices = kwargs.get('choices', None)\n        if state_choices is not None and choices is not None:\n            raise ValueError('Use one of choices or state_choices value')\n\n        if state_choices is not None:\n            choices = []\n            for state, title, proxy_cls_ref in state_choices:\n                choices.append((state, title))\n                self.state_proxy[state] = proxy_cls_ref\n            kwargs['choices'] = choices\n\n        super(FSMFieldMixin, self).__init__(*args, **kwargs)"}
{"input": "def get_vlc_mediactrl(self):\n        if sys.platform == 'win32':\n            oldcwd = os.getcwd()\n            os.chdir(os.path.join(self.installdir,'vlc'))\n    \n        # Arno: 2007-05-11: Don't ask me why but without the \"--verbose=0\" vlc will ignore the key redef.\n        params = [\"--verbose=2\"]\n        \n        \"\"\"\n        # To enable logging to file:\n        #[loghandle,logfilename] = mkstemp(\"vlc-log\")\n        #os.close(loghandle)\n        currwd = os.getcwd()\n        logfilename = os.path.join(currwd,\"vlc.log\")\n        params += [\"--file-logging\"]\n        params += [\"--logfile\",logfilename]\n        \"\"\"\n        \n        params += [\"--no-drop-late-frames\"] # Arno: 2007-11-19: don't seem to work as expected DEBUG\n        params += [\"--no-skip-frames\"]\n        params += [\"--quiet-synchro\"]\n        # JD: avoid \"fast catchup\" after frame loss by letting VLC have a flexible buffer\n        #params += [\"--access-filter\",\"timeshift\"]\n        #params += [\"--timeshift-force\"]\n        # Arno: attempt to improve robustness\n        params += [\"--http-reconnect\"]\n\n        # VLC wiki says: \"apply[ing] deinterlacing even if the original is not\n        # interlaced, is a really bad idea.\"\n        #params += [\"--vout-filter\",\"deinterlace\"]\n        #params += [\"--deinterlace-mode\",\"linear\"]\n        #params += [\"--demux=ts\"]\n        #params += [\"--codec=mp4\"]\n        #\n        params += [\"--no-plugins-cache\"]\n        params += [\"--key-fullscreen\", \"Esc\"] # must come last somehow on Win32\n        \n        if sys.platform == 'darwin':\n            params += [\"--plugin-path\", \"%s/macbinaries/vlc_plugins\" % (\n                 # location of plugins: next to tribler.py\n                 os.path.abspath(os.path.dirname(sys.argv[0]))\n                 )]\n            \n        media = vlc.MediaControl(params)\n            \n        if sys.platform == 'win32':\n                os.chdir(oldcwd)\n    \n        return media", "output": "def get_vlc_mediactrl(self):\n        if sys.platform == 'win32':\n            oldcwd = os.getcwd()\n            os.chdir(os.path.join(self.installdir,'vlc'))\n    \n        # Arno: 2007-05-11: Don't ask me why but without the \"--verbose=0\" vlc will ignore the key redef.\n        params = [\"--verbose=0\"]\n        \n        \"\"\"\n        # To enable logging to file:\n        #[loghandle,logfilename] = mkstemp(\"vlc-log\")\n        #os.close(loghandle)\n        currwd = os.getcwd()\n        logfilename = os.path.join(currwd,\"vlc.log\")\n        params += [\"--file-logging\"]\n        params += [\"--logfile\",logfilename]\n        \"\"\"\n        \n        params += [\"--no-drop-late-frames\"] # Arno: 2007-11-19: don't seem to work as expected DEBUG\n        params += [\"--no-skip-frames\"]\n        params += [\"--quiet-synchro\"]\n        # JD: avoid \"fast catchup\" after frame loss by letting VLC have a flexible buffer\n        #params += [\"--access-filter\",\"timeshift\"]\n        #params += [\"--timeshift-force\"]\n        # Arno: attempt to improve robustness\n        params += [\"--http-reconnect\"]\n\n        # VLC wiki says: \"apply[ing] deinterlacing even if the original is not\n        # interlaced, is a really bad idea.\"\n        #params += [\"--vout-filter\",\"deinterlace\"]\n        #params += [\"--deinterlace-mode\",\"linear\"]\n        #params += [\"--demux=ts\"]\n        #params += [\"--codec=mp4\"]\n        #\n        params += [\"--no-plugins-cache\"]\n        params += [\"--key-fullscreen\", \"Esc\"] # must come last somehow on Win32\n        \n        if sys.platform == 'darwin':\n            params += [\"--plugin-path\", \"%s/macbinaries/vlc_plugins\" % (\n                 # location of plugins: next to tribler.py\n                 os.path.abspath(os.path.dirname(sys.argv[0]))\n                 )]\n            \n        media = vlc.MediaControl(params)\n            \n        if sys.platform == 'win32':\n                os.chdir(oldcwd)\n    \n        return media"}
{"input": "def cqt(y, sr=22050, hop_length=512, fmin=None, n_bins=84,\n        bins_per_octave=12, tuning=None, resolution=2, res_type='sinc_fastest',\n        aggregate=None, norm=2):\n    '''Compute the constant-Q transform of an audio signal.\n\n    :usage:\n        >>> y, sr = librosa.load('file.wav')\n        >>> C = librosa.cqt(y, sr=sr)\n\n        >>> # Limit the frequency range\n        >>> C = librosa.cqt(y, sr=sr, fmin=librosa.midi_to_hz(36),\n                            n_bins=60)\n\n        >>> # Use higher resolution\n        >>> C = librosa.cqt(y, sr=sr, fmin=librosa.midi_to_hz(36),\n                            n_bins=60 * 2, bins_per_octave=12 * 2)\n\n    :parameters:\n      - y : np.ndarray [shape=(n,)]\n          audio time series\n\n      - sr : int > 0 [scalar]\n          sampling rate of ``y``\n\n      - hop_length : int > 0 [scalar]\n          number of samples between successive CQT columns.\n\n          .. note:: ``hop_length`` must be at least\n            ``2**(n_bins / bins_per_octave)``\n\n      - fmin : float > 0 [scalar]\n          Minimum frequency. Defaults to C2 ~= 32.70 Hz\n\n      - n_bins : int > 0 [scalar]\n          Number of frequency bins, starting at `fmin`\n\n      - bins_per_octave : int > 0 [scalar]\n          Number of bins per octave\n\n      - tuning : None or float in ``[-0.5, 0.5)``\n          Tuning offset in fractions of a bin (cents)\n          If ``None``, tuning will be automatically estimated.\n\n      - resolution : float > 0\n          Filter resolution factor. Larger values use longer windows.\n\n      - res_type : str\n          Resampling type, see :func:`librosa.core.resample()` for details.\n\n      - aggregate : None or function\n          Aggregation function for time-oversampling energy aggregation.\n          By default, ``np.mean``.  See :func:`librosa.feature.sync()`.\n\n      - norm : {inf, -inf, 0, float > 0}\n          Type of norm to use for basis function normalization.\n          See librosa.util.normalize\n\n    :returns:\n      - CQT : np.ndarray [shape=(d, t), dtype=np.float]\n          Constant-Q energy for each frequency at each time.\n\n    .. note:: This implementation is based on the recursive sub-sampling method\n        described by Schoerkhuber and Klapuri, 2010.\n\n        - Schoerkhuber, Christian, and Anssi Klapuri.\n            \"Constant-Q transform toolbox for music processing.\"\n            7th Sound and Music Computing Conference, Barcelona, Spain. 2010.\n    '''\n\n    if fmin is None:\n        # C2 by default\n        fmin = midi_to_hz(note_to_midi('C2'))\n\n    if tuning is None:\n        tuning = feature.estimate_tuning(y=y, sr=sr)\n\n    # First thing, get the fmin of the top octave\n    freqs = cqt_frequencies(n_bins + 1, fmin, bins_per_octave=bins_per_octave)\n    fmin_top = freqs[-bins_per_octave-1]\n\n    # Generate the basis filters\n    basis, lengths = filters.constant_q(sr,\n                                        fmin=fmin_top,\n                                        n_bins=bins_per_octave,\n                                        bins_per_octave=bins_per_octave,\n                                        tuning=tuning,\n                                        resolution=resolution,\n                                        pad=True,\n                                        norm=norm,\n                                        return_lengths=True)\n\n    basis = np.asarray(basis)\n\n    # FFT the filters\n    max_filter_length = basis.shape[1]\n    min_filter_length = min(lengths)\n    n_fft = int(2.0**(np.ceil(np.log2(max_filter_length))))\n\n    # Conjugate-transpose the basis\n    fft_basis = np.fft.fft(basis, n=n_fft, axis=1).conj()\n    fft_basis = sparsify_fft_basis(fft_basis)\n\n    n_octaves = int(np.ceil(float(n_bins) / bins_per_octave))\n\n    # Make sure our hop is long enough to support the bottom octave\n    assert hop_length >= 2**n_octaves\n\n    def __variable_hop_response(my_y, target_hop):\n        '''Compute the filter response with a target STFT hop.\n        If the hop is too large (more than half the frame length),\n        then over-sample at a smaller hop, and aggregate the results\n        to the desired resolution.\n        '''\n\n        # If target_hop <= min_filter_length / 2:\n        #   my_hop = target_hop\n        # else:\n        #   my_hop = target_hop * 2**(-k)\n\n        zoom_factor = int(np.maximum(0,\n                                     1 + np.ceil(np.log2(target_hop)\n                                                 - np.log2(min_filter_length))))\n\n        my_hop = int(target_hop / (2**(zoom_factor)))\n\n        assert my_hop > 0\n\n        # Compute the STFT matrix\n        D = stft(my_y, n_fft=n_fft, hop_length=my_hop)\n\n        D = np.vstack([D.conj(), D[-2:0:-1]])\n\n        # And filter response energy\n        my_cqt = np.abs(fft_basis.dot(D))\n\n        if zoom_factor > 0:\n            # We need to aggregate.  Generate the boundary frames\n            bounds = list(np.arange(0, my_cqt.shape[1], 2**(zoom_factor)))\n            my_cqt = feature.sync(my_cqt, bounds,\n                                  aggregate=aggregate)\n\n        # normalize as in Parseval's relation\n        my_cqt /= n_fft\n\n        return my_cqt\n\n    cqt_resp = []\n\n    my_y, my_sr, my_hop = y, sr, hop_length\n\n    # Iterate down the octaves\n    for _ in range(n_octaves):\n        # Compute a dynamic hop based on n_fft\n        my_cqt = __variable_hop_response(my_y, my_hop)\n\n        # Convolve\n        cqt_resp.append(my_cqt)\n\n        # Resample\n        my_y = resample(my_y, my_sr, my_sr/2.0, res_type=res_type)\n        my_sr = my_sr / 2.0\n        my_hop = int(my_hop / 2.0)\n\n    # cleanup any framing errors at the boundaries\n    max_col = min([x.shape[1] for x in cqt_resp])\n\n    cqt_resp = np.vstack([x[:, :max_col] for x in cqt_resp][::-1])\n\n    # Finally, clip out any bottom frequencies that we don't really want\n    cqt_resp = cqt_resp[-n_bins:]\n\n    # Transpose magic here to ensure column-contiguity\n    return np.ascontiguousarray(cqt_resp.T).T", "output": "def cqt(y, sr=22050, hop_length=512, fmin=None, n_bins=84,\n        bins_per_octave=12, tuning=None, resolution=2, res_type='sinc_best',\n        aggregate=None, norm=2):\n    '''Compute the constant-Q transform of an audio signal.\n\n    :usage:\n        >>> y, sr = librosa.load('file.wav')\n        >>> C = librosa.cqt(y, sr=sr)\n\n        >>> # Limit the frequency range\n        >>> C = librosa.cqt(y, sr=sr, fmin=librosa.midi_to_hz(36),\n                            n_bins=60)\n\n        >>> # Use higher resolution\n        >>> C = librosa.cqt(y, sr=sr, fmin=librosa.midi_to_hz(36),\n                            n_bins=60 * 2, bins_per_octave=12 * 2)\n\n    :parameters:\n      - y : np.ndarray [shape=(n,)]\n          audio time series\n\n      - sr : int > 0 [scalar]\n          sampling rate of ``y``\n\n      - hop_length : int > 0 [scalar]\n          number of samples between successive CQT columns.\n\n          .. note:: ``hop_length`` must be at least\n            ``2**(n_bins / bins_per_octave)``\n\n      - fmin : float > 0 [scalar]\n          Minimum frequency. Defaults to C2 ~= 32.70 Hz\n\n      - n_bins : int > 0 [scalar]\n          Number of frequency bins, starting at `fmin`\n\n      - bins_per_octave : int > 0 [scalar]\n          Number of bins per octave\n\n      - tuning : None or float in ``[-0.5, 0.5)``\n          Tuning offset in fractions of a bin (cents)\n          If ``None``, tuning will be automatically estimated.\n\n      - resolution : float > 0\n          Filter resolution factor. Larger values use longer windows.\n\n      - res_type : str\n          Resampling type, see :func:`librosa.core.resample()` for details.\n\n      - aggregate : None or function\n          Aggregation function for time-oversampling energy aggregation.\n          By default, ``np.mean``.  See :func:`librosa.feature.sync()`.\n\n      - norm : {inf, -inf, 0, float > 0}\n          Type of norm to use for basis function normalization.\n          See librosa.util.normalize\n\n    :returns:\n      - CQT : np.ndarray [shape=(d, t), dtype=np.float]\n          Constant-Q energy for each frequency at each time.\n\n    .. note:: This implementation is based on the recursive sub-sampling method\n        described by Schoerkhuber and Klapuri, 2010.\n\n        - Schoerkhuber, Christian, and Anssi Klapuri.\n            \"Constant-Q transform toolbox for music processing.\"\n            7th Sound and Music Computing Conference, Barcelona, Spain. 2010.\n    '''\n\n    if fmin is None:\n        # C2 by default\n        fmin = midi_to_hz(note_to_midi('C2'))\n\n    if tuning is None:\n        tuning = feature.estimate_tuning(y=y, sr=sr)\n\n    # First thing, get the fmin of the top octave\n    freqs = cqt_frequencies(n_bins + 1, fmin, bins_per_octave=bins_per_octave)\n    fmin_top = freqs[-bins_per_octave-1]\n\n    # Generate the basis filters\n    basis, lengths = filters.constant_q(sr,\n                                        fmin=fmin_top,\n                                        n_bins=bins_per_octave,\n                                        bins_per_octave=bins_per_octave,\n                                        tuning=tuning,\n                                        resolution=resolution,\n                                        pad=True,\n                                        norm=norm,\n                                        return_lengths=True)\n\n    basis = np.asarray(basis)\n\n    # FFT the filters\n    max_filter_length = basis.shape[1]\n    min_filter_length = min(lengths)\n    n_fft = int(2.0**(np.ceil(np.log2(max_filter_length))))\n\n    # Conjugate-transpose the basis\n    fft_basis = np.fft.fft(basis, n=n_fft, axis=1).conj()\n    fft_basis = sparsify_fft_basis(fft_basis)\n\n    n_octaves = int(np.ceil(float(n_bins) / bins_per_octave))\n\n    # Make sure our hop is long enough to support the bottom octave\n    assert hop_length >= 2**n_octaves\n\n    def __variable_hop_response(my_y, target_hop):\n        '''Compute the filter response with a target STFT hop.\n        If the hop is too large (more than half the frame length),\n        then over-sample at a smaller hop, and aggregate the results\n        to the desired resolution.\n        '''\n\n        # If target_hop <= min_filter_length / 2:\n        #   my_hop = target_hop\n        # else:\n        #   my_hop = target_hop * 2**(-k)\n\n        zoom_factor = int(np.maximum(0,\n                                     1 + np.ceil(np.log2(target_hop)\n                                                 - np.log2(min_filter_length))))\n\n        my_hop = int(target_hop / (2**(zoom_factor)))\n\n        assert my_hop > 0\n\n        # Compute the STFT matrix\n        D = stft(my_y, n_fft=n_fft, hop_length=my_hop)\n\n        D = np.vstack([D.conj(), D[-2:0:-1]])\n\n        # And filter response energy\n        my_cqt = np.abs(fft_basis.dot(D))\n\n        if zoom_factor > 0:\n            # We need to aggregate.  Generate the boundary frames\n            bounds = list(np.arange(0, my_cqt.shape[1], 2**(zoom_factor)))\n            my_cqt = feature.sync(my_cqt, bounds,\n                                  aggregate=aggregate)\n\n        # normalize as in Parseval's relation\n        my_cqt /= n_fft\n\n        return my_cqt\n\n    cqt_resp = []\n\n    my_y, my_sr, my_hop = y, sr, hop_length\n\n    # Iterate down the octaves\n    for _ in range(n_octaves):\n        # Compute a dynamic hop based on n_fft\n        my_cqt = __variable_hop_response(my_y, my_hop)\n\n        # Convolve\n        cqt_resp.append(my_cqt)\n\n        # Resample\n        my_y = resample(my_y, my_sr, my_sr/2.0, res_type=res_type)\n        my_sr = my_sr / 2.0\n        my_hop = int(my_hop / 2.0)\n\n    # cleanup any framing errors at the boundaries\n    max_col = min([x.shape[1] for x in cqt_resp])\n\n    cqt_resp = np.vstack([x[:, :max_col] for x in cqt_resp][::-1])\n\n    # Finally, clip out any bottom frequencies that we don't really want\n    cqt_resp = cqt_resp[-n_bins:]\n\n    # Transpose magic here to ensure column-contiguity\n    return np.ascontiguousarray(cqt_resp.T).T"}
{"input": "def test_ensure_index_returned_on_root_path(self):\n        self.assertTrue('<html>' in self.request('/'))", "output": "def test_ensure_index_returned_on_root_path(self):\n        self.assertTrue('<html' in self.request('/'))"}
{"input": "def configure(self):\n        if self.supports_non_src_build:\n            self.config_sh = os.path.join(self.repo_dir, self.config_sh)\n        # skip configure if we are already configured\n        if os.path.exists(os.path.join(self.make_dir, 'configure')) and\\\n                os.path.exists(os.path.join(self.make_dir, 'Makefile')):\n            if not self.force_configure and not self.force:\n                return\n\n        # Only use --disable-maintainer mode for real autotools based projects\n        if os.path.exists(os.path.join(self.make_dir, 'configure.in')) or\\\n                os.path.exists(os.path.join(self.make_dir, 'configure.ac')):\n            self.configure_tpl += \" --disable-maintainer-mode\"\n\n        if self.autoreconf:\n            shell.call(self.autoreconf_sh, self.make_dir)\n\n        files = shell.check_call('find %s -type f -name config.guess' % self.make_dir).split('\\n')\n        files.remove('')\n        for f in files:\n            o = os.path.join(self.config._relative_path ('data'), 'autotools', 'config.guess')\n            m.action(\"copying %s to %s\" % (o, f))\n            shutil.copy(o, f)\n\n        files = shell.check_call('find %s -type f -name config.sub' % self.make_dir).split('\\n')\n        files.remove('')\n        for f in files:\n            o = os.path.join(self.config._relative_path ('data'), 'autotools', 'config.sub')\n            m.action(\"copying %s to %s\" % (o, f))\n            shutil.copy(o, f)\n\n\n        if self.config.platform == Platform.WINDOWS and \\\n                self.supports_cache_variables:\n            # On windows, environment variables are upperscase, but we still\n            # need to pass things like am_cv_python_platform in lowercase for\n            # configure and autogen.sh\n            for k, v in os.environ.iteritems():\n                if k[2:6] == '_cv_':\n                    self.configure_tpl += ' %s=\"%s\"' % (k, v)\n\n        if self.add_host_build_target:\n            if self.config.host is not None:\n                self.configure_tpl += ' --host=%(host)s'\n            if self.config.build is not None:\n                self.configure_tpl += ' --build=%(build)s'\n            if self.config.target is not None:\n                self.configure_tpl += ' --target=%(target)s'\n\n\n        use_configure_cache = self.config.use_configure_cache\n        if self.use_system_libs and self.config.allow_system_libs:\n            use_configure_cache = False\n\n        if self.new_env is not None or self.append_env is not None:\n            use_configure_cache = False\n\n        if use_configure_cache and self.can_use_configure_cache:\n            cache = os.path.join(self.config.prefix, '.configure.cache')\n            self.config_sh += ' --cache-file=%s' % cache\n\n        MakefilesBase.configure(self)", "output": "def configure(self):\n        if self.supports_non_src_build:\n            self.config_sh = os.path.join(self.repo_dir, self.config_sh)\n        # skip configure if we are already configured\n        if os.path.exists(os.path.join(self.make_dir, 'configure')) and\\\n                os.path.exists(os.path.join(self.make_dir, 'Makefile')):\n            if not self.force_configure and not self.force:\n                return\n\n        # Only use --disable-maintainer mode for real autotools based projects\n        if os.path.exists(os.path.join(self.make_dir, 'configure.in')) or\\\n                os.path.exists(os.path.join(self.make_dir, 'configure.ac')):\n            self.configure_tpl += \" --disable-maintainer-mode --disable-silent-rules \"\n\n        if self.autoreconf:\n            shell.call(self.autoreconf_sh, self.make_dir)\n\n        files = shell.check_call('find %s -type f -name config.guess' % self.make_dir).split('\\n')\n        files.remove('')\n        for f in files:\n            o = os.path.join(self.config._relative_path ('data'), 'autotools', 'config.guess')\n            m.action(\"copying %s to %s\" % (o, f))\n            shutil.copy(o, f)\n\n        files = shell.check_call('find %s -type f -name config.sub' % self.make_dir).split('\\n')\n        files.remove('')\n        for f in files:\n            o = os.path.join(self.config._relative_path ('data'), 'autotools', 'config.sub')\n            m.action(\"copying %s to %s\" % (o, f))\n            shutil.copy(o, f)\n\n\n        if self.config.platform == Platform.WINDOWS and \\\n                self.supports_cache_variables:\n            # On windows, environment variables are upperscase, but we still\n            # need to pass things like am_cv_python_platform in lowercase for\n            # configure and autogen.sh\n            for k, v in os.environ.iteritems():\n                if k[2:6] == '_cv_':\n                    self.configure_tpl += ' %s=\"%s\"' % (k, v)\n\n        if self.add_host_build_target:\n            if self.config.host is not None:\n                self.configure_tpl += ' --host=%(host)s'\n            if self.config.build is not None:\n                self.configure_tpl += ' --build=%(build)s'\n            if self.config.target is not None:\n                self.configure_tpl += ' --target=%(target)s'\n\n\n        use_configure_cache = self.config.use_configure_cache\n        if self.use_system_libs and self.config.allow_system_libs:\n            use_configure_cache = False\n\n        if self.new_env is not None or self.append_env is not None:\n            use_configure_cache = False\n\n        if use_configure_cache and self.can_use_configure_cache:\n            cache = os.path.join(self.config.prefix, '.configure.cache')\n            self.config_sh += ' --cache-file=%s' % cache\n\n        MakefilesBase.configure(self)"}
{"input": "def setUp(self):\n        \"\"\"\n        Set up for some of the tests.\n        \"\"\"\n        # load irregular 3-hour time series test rebin and mesh\n        tsfile = os.path.join(os.path.dirname(__file__), '..', 'data', 'simo_p_out.ts')\n        self.irreg_series = TsDB.fromfile(tsfile).get(name='tension_2_qs').x", "output": "def setUp(self):\n        \"\"\"\n        Set up for some of the tests.\n        \"\"\"\n        # load irregular 3-hour time series test rebin and mesh\n        tsfile = os.path.join(os.path.dirname(__file__), '..', 'data', 'simo_p_out.ts')\n        self.irreg_series = TsDB.fromfile(tsfile).get(name='Tension_2_qs').x"}
{"input": "def get_title(self, model):\n        if self.title:\n            return self.title\n        if model:\n            if self.model_name:\n                return _('Add %s') % self.model_name\n            model_attr = self.get_title_model_attribute(model)\n            return _('Edit \"%s\" Details') % model_attr\n        else:\n            raise ValueError(\"A model should be defined at this point\")", "output": "def get_title(self, model):\n        if self.title:\n            return self.title\n        if model:\n            if self.model_name and not self.edit_mode:\n                return _('Add %s') % self.model_name\n            model_attr = self.get_title_model_attribute(model)\n            return _('Edit \"%s\" Details') % model_attr\n        else:\n            raise ValueError(\"A model should be defined at this point\")"}
{"input": "def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            '''\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            '''\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].iteritems():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if norm_hyp[n] != 0:\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val", "output": "def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            '''\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            '''\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].iteritems():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val"}
{"input": "def resolve_function(user_id, function_uuid):\n    \"\"\"Get the function uuid from database\n\n    Parameters\n    ----------\n    user_id : str\n        The uuid of the user\n    function_uuid : str\n        The uuid of the function\n\n    Returns\n    -------\n    str\n        The function code\n    str\n        The function entry point\n    str\n        The uuid of the container image to use\n    \"\"\"\n\n    start = time.time()\n    function_code = None\n    function_entry = None\n    container_uuid = None\n\n    try:\n        conn, cur = get_db_connection()\n        query = \"select * from functions where function_uuid = %s and user_id = %s order by id DESC limit 1\"\n        cur.execute(query, (function_uuid, user_id))\n        r = cur.fetchone()\n        if not r:\n            raise MissingFunction(function_uuid)\n\n        function_code = r['function_code']\n        function_entry = r['entry_point']\n        function_id = r['id']\n        query = \"select * from function_containers, containers, container_images where \" \\\n                \"function_containers.function_id = %s and containers.id = function_containers.container_id \" \\\n                \"and function_containers.container_id = container_images.container_id \" \\\n                \"order by function_containers.id desc limit 1\"\n        cur.execute(query, (function_id,))\n        r = cur.fetchone()\n\n        if 'container_uuid' in r:\n            container_uuid = r['container_uuid']\n\n    except Exception as e:\n        app.logger.exception(e)\n        raise\n    delta = time.time() - start\n    app.logger.info(\"Time to fetch function {0:.1f}ms\".format(delta * 1000))\n    return function_code, function_entry, container_uuid", "output": "def resolve_function(user_id, function_uuid):\n    \"\"\"Get the function uuid from database\n\n    Parameters\n    ----------\n    user_id : str\n        The uuid of the user\n    function_uuid : str\n        The uuid of the function\n\n    Returns\n    -------\n    str\n        The function code\n    str\n        The function entry point\n    str\n        The uuid of the container image to use\n    \"\"\"\n\n    start = time.time()\n    function_code = None\n    function_entry = None\n    container_uuid = None\n\n    try:\n        conn, cur = get_db_connection()\n        query = \"select * from functions where function_uuid = %s and user_id = %s order by id DESC limit 1\"\n        cur.execute(query, (function_uuid, user_id))\n        r = cur.fetchone()\n        if not r:\n            raise MissingFunction(function_uuid)\n\n        function_code = r['function_code']\n        function_entry = r['entry_point']\n        function_id = r['id']\n        query = \"select * from function_containers, containers, container_images where \" \\\n                \"function_containers.function_id = %s and containers.id = function_containers.container_id \" \\\n                \"and function_containers.container_id = container_images.container_id \" \\\n                \"order by function_containers.id desc limit 1\"\n        cur.execute(query, (function_id,))\n        r = cur.fetchone()\n\n        if r and 'container_uuid' in r:\n            container_uuid = r['container_uuid']\n\n    except Exception as e:\n        app.logger.exception(e)\n        raise\n    delta = time.time() - start\n    app.logger.info(\"Time to fetch function {0:.1f}ms\".format(delta * 1000))\n    return function_code, function_entry, container_uuid"}
{"input": "def bash_org_ru(type, jid, nick, text):\n\ttry: url = u'http://bash.org.ru/quote/'+str(int(text))\n\texcept: url = u'http://bash.org.ru/random'\n\tbody = html_encode(urllib.urlopen(url).read())\n\tif body.count('<div class=\"vote\">') > 1: msg = u'\u0426\u0438\u0442\u0430\u0442\u0430 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u0430!'\n\telse:\n\t\tbody = body.split('<div class=\"vote\">')[1].split('<div class=\"q\">')[0]\n\t\tmsg = u'http://bash.org.ru/quote/'+str(get_tag(body, 'a'))+u' '+replacer(body[body.find('[:||||:]'):].replace('</div>', '\\n').replace('[:||||:]', '::: ').replace('</a>\\n', ''))\n\tsend_msg(type, jid, nick, msg)", "output": "def bash_org_ru(type, jid, nick, text):\n\ttry: url = u'http://bash.org.ru/quote/'+str(int(text))\n\texcept: url = u'http://bash.org.ru/random'\n\tbody = html_encode(urllib.urlopen(url).read())\n\tif body.count('<div class=\"vote\">') > 1 and url.count('quote'): msg = u'\u0426\u0438\u0442\u0430\u0442\u0430 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u0430!'\n\telse:\n\t\tbody = body.split('<div class=\"vote\">')[1].split('<div class=\"q\">')[0]\n\t\tmsg = u'http://bash.org.ru/quote/'+str(get_tag(body, 'a'))+u' '+replacer(body[body.find('[:||||:]'):].replace('</div>', '\\n').replace('[:||||:]', '::: ').replace('</a>\\n', ''))\n\tsend_msg(type, jid, nick, msg)"}
{"input": "def readline(self):\n    # TODO: This should work in large chunks rather than a byte at a time\n    chars = []\n    c = self.read(1)\n    while c != '\\n':\n      chars.append(c)\n      c = self.read(1)\n    chars.append(c)\n    line = ''.join(chars)\n    return line", "output": "def readline(self):\n    # TODO: This should work in large chunks rather than a byte at a time\n    chars = []\n    c = self.read(1)\n    while c and c != '\\n':\n      chars.append(c)\n      c = self.read(1)\n    chars.append(c)\n    line = ''.join(chars)\n    return line"}
{"input": "def rpc_run_task(self, task_uuid, model, method, *args, **kwargs):\n        \"\"\"Run/execute the task, which is a model method.\n\n        The idea is that Celery calls this by Odoo its external API,\n        whereas XML-RPC or a HTTP-controller.\n\n        The model-method can either be called as user:\n        - The \"celery\" (Odoo user) defined in the odoo.conf. This is the default, in case\n        the \"sudo\" setting isn't configured in the odoo.conf.\n        - \"admin\" (Odoo admin user), to circumvent model-access configuration for models\n        which run/process task. Therefor add \"sudo = True\" in the odoo.conf (see: example.odoo.conf).\n        \"\"\"\n\n        logger.info('CELERY rpc_run_task uuid {uuid}'.format(uuid=task_uuid))\n        \n        exist = self.search_count([('uuid', '=', task_uuid)])\n        if exist == 0:\n            msg = \"Task doesn't exist (anymore). Task-UUID: %s\" % task_uuid\n            logger.error(msg)\n            return (TASK_NOT_FOUND, msg)\n\n        model_obj = self.env[model]\n        task = self.search([('uuid', '=', task_uuid), ('state', 'in', [STATE_PENDING, STATE_RETRY, STATE_FAILURE])], limit=1)\n\n        if not task:\n            return ('OK', 'Task already processed')\n\n        # Start / Retry (refactor to absraction/neater code)\n        celery_retry = kwargs.get('celery_retry')\n        if celery_retry and task.retry and task.state == STATE_RETRY:\n            return (STATE_RETRY, 'Task is already executing a retry.')\n        elif celery_retry and task.celery_retry:\n            task.state = STATE_RETRY\n            vals = {'state': STATE_RETRY, 'state_date': fields.Datetime.now()}\n        else:\n            vals = {'state': STATE_STARTED, 'started_date': fields.Datetime.now()}\n\n        user, password, sudo = _get_celery_user_config()\n\n        # TODO\n        # Re-raise Exception if not called by XML-RPC, but directly from model/Odoo.\n        # This supports unit-tests and scripting purposes.\n        result = False\n        response = False\n        with registry(self._cr.dbname).cursor() as cr:\n            # Transaction/cursror for the exception handler.\n            env = api.Environment(cr, self._uid, {})\n            try:\n                if bool(sudo) and sudo:\n                    res = getattr(model_obj.with_env(env).sudo(), method)(task_uuid, **kwargs)\n                else:\n                    res = getattr(model_obj.with_env(env), method)(task_uuid, **kwargs)\n\n                if not bool(res):\n                    msg = \"No result/return value for Task UUID: %s. Ensure the task-method returns a value.\" % task_uuid\n                    logger.error(msg)\n                    raise CeleryTaskNoResultError(msg)\n\n                if isinstance(res, dict):\n                    result = res.get('result', True)\n                    vals.update({'res_model': res.get('res_model'), 'res_ids': res.get('res_ids')})\n                else:\n                    result = res\n                vals.update({'state': STATE_SUCCESS, 'state_date': fields.Datetime.now(), 'result': result, 'exc_info': False})\n            except Exception as e:\n                \"\"\" The Exception-handler does a rollback. So we need a new\n                transaction/cursor to store data about RETRY and exception info/traceback. \"\"\"\n\n                exc_info = traceback.format_exc()\n                vals.update({'state': STATE_RETRY, 'state_date': fields.Datetime.now(), 'exc_info': exc_info})\n                logger.warning('Retry... exception (see task form) from rpc_run_task {uuid}: {exc}.'.format(uuid=task_uuid, exc=e))\n                logger.debug('Exception rpc_run_task: {exc_info}'.format(uuid=task_uuid, exc_info=exc_info))\n                cr.rollback()\n            finally:\n                with registry(self._cr.dbname).cursor() as result_cr:\n                    env = api.Environment(result_cr, self._uid, {})\n                    task.with_env(env).write(vals)\n                response = (vals.get('state'), result)\n                return response", "output": "def rpc_run_task(self, task_uuid, model, method, *args, **kwargs):\n        \"\"\"Run/execute the task, which is a model method.\n\n        The idea is that Celery calls this by Odoo its external API,\n        whereas XML-RPC or a HTTP-controller.\n\n        The model-method can either be called as user:\n        - The \"celery\" (Odoo user) defined in the odoo.conf. This is the default, in case\n        the \"sudo\" setting isn't configured in the odoo.conf.\n        - \"admin\" (Odoo admin user), to circumvent model-access configuration for models\n        which run/process task. Therefor add \"sudo = True\" in the odoo.conf (see: example.odoo.conf).\n        \"\"\"\n\n        logger.info('CELERY rpc_run_task uuid {uuid}'.format(uuid=task_uuid))\n        \n        exist = self.search_count([('uuid', '=', task_uuid)])\n        if exist == 0:\n            msg = \"Task doesn't exist (anymore). Task-UUID: %s\" % task_uuid\n            logger.error(msg)\n            return (TASK_NOT_FOUND, msg)\n\n        model_obj = self.env[model]\n        task = self.search([('uuid', '=', task_uuid), ('state', 'in', [STATE_PENDING, STATE_RETRY, STATE_FAILURE])], limit=1)\n\n        if not task:\n            return ('OK', 'Task already processed')\n\n        # Start / Retry (refactor to absraction/neater code)\n        celery_retry = kwargs.get('celery_retry')\n        if celery_retry and task.retry and task.state == STATE_RETRY:\n            return (STATE_RETRY, 'Task is already executing a retry.')\n        elif celery_retry and task.celery_retry:\n            task.state = STATE_RETRY\n            vals = {'state': STATE_RETRY, 'state_date': fields.Datetime.now()}\n        else:\n            vals = {'state': STATE_STARTED, 'started_date': fields.Datetime.now()}\n\n        user, password, sudo = _get_celery_user_config()\n\n        # TODO\n        # Re-raise Exception if not called by XML-RPC, but directly from model/Odoo.\n        # This supports unit-tests and scripting purposes.\n        result = False\n        response = False\n        with registry(self._cr.dbname).cursor() as cr:\n            # Transaction/cursror for the exception handler.\n            env = api.Environment(cr, self._uid, {})\n            try:\n                if bool(sudo) and sudo:\n                    res = getattr(model_obj.with_env(env).sudo(), method)(task_uuid, **kwargs)\n                else:\n                    res = getattr(model_obj.with_env(env), method)(task_uuid, **kwargs)\n\n                if res != False and not bool(res):\n                    msg = \"No result/return value for Task UUID: %s. Ensure the task-method returns a value.\" % task_uuid\n                    logger.error(msg)\n                    raise CeleryTaskNoResultError(msg)\n\n                if isinstance(res, dict):\n                    result = res.get('result', True)\n                    vals.update({'res_model': res.get('res_model'), 'res_ids': res.get('res_ids')})\n                else:\n                    result = res\n                vals.update({'state': STATE_SUCCESS, 'state_date': fields.Datetime.now(), 'result': result, 'exc_info': False})\n            except Exception as e:\n                \"\"\" The Exception-handler does a rollback. So we need a new\n                transaction/cursor to store data about RETRY and exception info/traceback. \"\"\"\n\n                exc_info = traceback.format_exc()\n                vals.update({'state': STATE_RETRY, 'state_date': fields.Datetime.now(), 'exc_info': exc_info})\n                logger.warning('Retry... exception (see task form) from rpc_run_task {uuid}: {exc}.'.format(uuid=task_uuid, exc=e))\n                logger.debug('Exception rpc_run_task: {exc_info}'.format(uuid=task_uuid, exc_info=exc_info))\n                cr.rollback()\n            finally:\n                with registry(self._cr.dbname).cursor() as result_cr:\n                    env = api.Environment(result_cr, self._uid, {})\n                    task.with_env(env).write(vals)\n                response = (vals.get('state'), result)\n                return response"}
{"input": "def forward_kinematics(self, joint_cfg=None, link_names=None):\n        \"\"\"From a dictionary mapping joint names to joint configurations\n        (float or (2,) vector for planar joints), compute the pose of each link.\n        \"\"\"\n        if joint_cfg is None:\n            joint_cfg = {}\n\n        # Iterate over the links and compute poses for each\n        if link_names is not None:\n            links = [l for l in self.links if l.name in link_names]\n        else:\n            links = self.links\n\n        # Compute the pose of each link\n        link_to_pose = { l : None for l in links }\n\n        # Iterate over the links and compute poses for each\n        for link in links:\n            pose = np.eye(4)\n            path = self._paths_to_base[link]\n            for i in range(len(path)-1):\n                child = path[i]\n                parent = path[i+1]\n\n                # Get joint\n                joint = self._graph.get_edge_data(child, parent)['object']\n\n                # Get joint cfg\n                cfg = None\n                if joint.name in joint_cfg:\n                    cfg = joint_cfg[joint.name]\n                elif joint.mimic is not None:\n                    mimic_joint = self._joint_map[joint.mimic.joint]\n                    if mimic_joint.name in joint_cfg:\n                        cfg = joint_cfg[mimic_joint.name]\n                        multiplier = 1.0\n                        offset = 0.0\n                        if joint.mimic.multiplier is not None:\n                            multiplier = joint.mimic.multiplier\n                        if joint.mimic.offset is not None:\n                            offset = joint.mimic.offset\n                        cfg = multiplier * cfg + offset\n                child_pose = joint.get_child_pose(cfg)\n\n                pose = child_pose.dot(pose)\n\n                if link_to_pose[parent] is not None:\n                    pose = link_to_pose[parent].dot(pose)\n                    break\n\n            link_to_pose[link] = pose\n\n        return link_to_pose", "output": "def forward_kinematics(self, joint_cfg=None, link_names=None):\n        \"\"\"From a dictionary mapping joint names to joint configurations\n        (float or (2,) vector for planar joints), compute the pose of each link.\n        \"\"\"\n        if joint_cfg is None:\n            joint_cfg = {}\n\n        # Iterate over the links and compute poses for each\n        if link_names is not None:\n            links = [l for l in self.links if l.name in link_names]\n        else:\n            links = self.links\n\n        # Compute the pose of each link\n        link_to_pose = { l : None for l in links }\n\n        # Iterate over the links and compute poses for each\n        for link in links:\n            pose = np.eye(4)\n            path = self._paths_to_base[link]\n            for i in range(len(path)-1):\n                child = path[i]\n                parent = path[i+1]\n\n                # Get joint\n                joint = self._graph.get_edge_data(child, parent)['object']\n\n                # Get joint cfg\n                cfg = None\n                if joint.name in joint_cfg:\n                    cfg = joint_cfg[joint.name]\n                elif joint.mimic is not None:\n                    mimic_joint = self._joint_map[joint.mimic.joint]\n                    if mimic_joint.name in joint_cfg:\n                        cfg = joint_cfg[mimic_joint.name]\n                        multiplier = 1.0\n                        offset = 0.0\n                        if joint.mimic.multiplier is not None:\n                            multiplier = joint.mimic.multiplier\n                        if joint.mimic.offset is not None:\n                            offset = joint.mimic.offset\n                        cfg = multiplier * cfg + offset\n                child_pose = joint.get_child_pose(cfg)\n\n                pose = child_pose.dot(pose)\n\n                if parent in link_to_pose and link_to_pose[parent] is not None:\n                    pose = link_to_pose[parent].dot(pose)\n                    break\n\n            link_to_pose[link] = pose\n\n        return link_to_pose"}
{"input": "def _make_new_fasta_no_multi_match_lines(self, scrapped_fasta_as_list):\n        new_scrapped_fasta = []\n        for i in range(0, len(scrapped_fasta_as_list), 2):\n            if not 'multipleMatches' in scrapped_fasta_as_list[i]:\n                new_scrapped_fasta.extend([scrapped_fasta_as_list[i], scrapped_fasta_as_list[i + 1]])\n        return new_scrapped_fasta", "output": "def _make_new_fasta_no_multi_match_lines(self, scrapped_fasta_as_list):\n        new_scrapped_fasta = []\n        for i in range(0, len(scrapped_fasta_as_list), 2):\n            if not 'multipleMatches' in scrapped_fasta_as_list[i] and len(scrapped_fasta_as_list[i + 1]) > 1:\n                new_scrapped_fasta.extend([scrapped_fasta_as_list[i], scrapped_fasta_as_list[i + 1]])\n        return new_scrapped_fasta"}
{"input": "def _combine_lines(lines):\n    newlines = []\n    lastline = '.'\n    for line in lines:\n        if lastline[-1] in '.,:':\n            newlines.append(line)\n            lastline = line\n        else:\n            lastline = newlines[-1] = newlines[-1] + ' ' + line\n    return newlines", "output": "def _combine_lines(lines):\n    newlines = []\n    lastline = '.'\n    for line in lines:\n        if lastline and lastline[-1] in '.,:':\n            newlines.append(line)\n            lastline = line\n        else:\n            lastline = newlines[-1] = newlines[-1] + ' ' + line\n    return newlines"}
{"input": "def execute_payment():\n    req = request.json\n\n    try:\n        validate_execute_request(req)\n    except Exception:\n        traceback.print_exc(file=sys.stderr)\n        raise\n\n    if int(request.args.get(\"validate_only\", 0)) and app.config[\"APP_MODE\"] == \"development\":\n        return jsonify({\"success\": \"Provided JSON looks good\"})\n\n    query = Transaction.query.filter_by(payment_id=req[\"paymentID\"])\n    donation = query[0].donation\n\n    try:\n        body = {\n            \"payer_id\": req[\"payerID\"]\n        }\n\n        payment_execute_request = payments.PaymentExecuteRequest(req[\"paymentID\"])\n        payment_execute_request.request_body(body)\n\n        try:\n            payment_execute_response = pp_client.execute(payment_execute_request, parse=False)\n            r = payment_execute_response\n            if r.status_code < 200 or r.status_code > 299:\n                raise braintreehttp.http_error.HttpError(r.text, r.status_code, r.headers)\n        except IOError as ioe:\n            ioe.body = body\n            if isinstance(ioe, braintreehttp.http_error.HttpError):\n                donation.transaction.state = json.loads(ioe.message)[\"name\"]\n            else:\n                donation.transaction.state = str(ioe.__class__.__name__)\n            db.session.commit()\n            raise\n\n        jresult = r.json()\n\n        # Store payment execution into database\n        database.register_transaction(req[\"paymentID\"], req[\"payerID\"], jresult, jresult[\"state\"])\n\n        # Send confirmation email\n        mail_error = False\n        if app.config[\"APP_MAILER\"] is not None:\n            to_addrs = set()\n            if donation.reference:\n                to_addrs.update({donation.reference.email})\n\n            paypal_addr = get_paypal_email(jresult)\n            if paypal_addr:\n                to_addrs.update({paypal_addr})\n            to_addrs = list(to_addrs)\n\n            lang = \"en\"\n            if \"lang\" in req:\n                lang = req[\"lang\"]\n            elif donation.reference and donation.reference.lang:\n                lang = donation.reference.lang\n\n            if len(to_addrs) == 0:\n                warnings.warn(\"User has no email address! Donation ID: {}\".format(donation.pretty_id))\n            else:\n                try:\n                    mail.send_confirmation_email(to_addrs, donation, lang)\n                except Exception as e:\n                    mail_error = True\n                    app.log_exception(e)\n                    app.log_exception(DonationError(donation, e))\n\n        rjson = {\n            \"result\": jresult[\"state\"],\n            \"donation_id\": donation.pretty_id\n        }\n\n        if mail_error:\n            rjson[\"error\"] = {\n                \"type\": \"_MAIL_ERROR\",\n                \"message\": \"The donation has been recorded, however an error occurred while trying to send you a \"\n                           \"confirmation email. As the email is required to pick up the gadgets, you should contact \"\n                           \"us at info@poliedro-polimi.it and send us the donation ID ({}), so we can send it to you \"\n                           \"manually.\".format(\n                    donation.pretty_id)\n            }\n\n        response = jsonify(rjson)\n        response.status_code = r.status_code\n        return response\n\n    except Exception as e:\n        raise DonationError(donation, e)", "output": "def execute_payment():\n    req = request.json\n\n    try:\n        validate_execute_request(req)\n    except Exception:\n        traceback.print_exc(file=sys.stderr)\n        raise\n\n    if int(request.args.get(\"validate_only\", 0)) and app.config[\"APP_MODE\"] == \"development\":\n        return jsonify({\"success\": \"Provided JSON looks good\"})\n\n    query = Transaction.query.filter_by(payment_id=req[\"paymentID\"])\n    donation = query[0].donation\n\n    try:\n        body = {\n            \"payer_id\": req[\"payerID\"]\n        }\n\n        payment_execute_request = payments.PaymentExecuteRequest(req[\"paymentID\"])\n        payment_execute_request.request_body(body)\n\n        try:\n            payment_execute_response = pp_client.execute(payment_execute_request, parse=False)\n            r = payment_execute_response\n            if r.status_code < 200 or r.status_code > 299:\n                raise braintreehttp.http_error.HttpError(r.text, r.status_code, r.headers)\n        except IOError as ioe:\n            ioe.body = body\n            if isinstance(ioe, braintreehttp.http_error.HttpError) and json.loads(ioe.message).get(\"name\"):\n                donation.transaction.state = json.loads(ioe.message)[\"name\"]\n            else:\n                donation.transaction.state = str(ioe.__class__.__name__)\n            db.session.commit()\n            raise\n\n        jresult = r.json()\n\n        # Store payment execution into database\n        database.register_transaction(req[\"paymentID\"], req[\"payerID\"], jresult, jresult[\"state\"])\n\n        # Send confirmation email\n        mail_error = False\n        if app.config[\"APP_MAILER\"] is not None:\n            to_addrs = set()\n            if donation.reference:\n                to_addrs.update({donation.reference.email})\n\n            paypal_addr = get_paypal_email(jresult)\n            if paypal_addr:\n                to_addrs.update({paypal_addr})\n            to_addrs = list(to_addrs)\n\n            lang = \"en\"\n            if \"lang\" in req:\n                lang = req[\"lang\"]\n            elif donation.reference and donation.reference.lang:\n                lang = donation.reference.lang\n\n            if len(to_addrs) == 0:\n                warnings.warn(\"User has no email address! Donation ID: {}\".format(donation.pretty_id))\n            else:\n                try:\n                    mail.send_confirmation_email(to_addrs, donation, lang)\n                except Exception as e:\n                    mail_error = True\n                    app.log_exception(e)\n                    app.log_exception(DonationError(donation, e))\n\n        rjson = {\n            \"result\": jresult[\"state\"],\n            \"donation_id\": donation.pretty_id\n        }\n\n        if mail_error:\n            rjson[\"error\"] = {\n                \"type\": \"_MAIL_ERROR\",\n                \"message\": \"The donation has been recorded, however an error occurred while trying to send you a \"\n                           \"confirmation email. As the email is required to pick up the gadgets, you should contact \"\n                           \"us at info@poliedro-polimi.it and send us the donation ID ({}), so we can send it to you \"\n                           \"manually.\".format(\n                    donation.pretty_id)\n            }\n\n        response = jsonify(rjson)\n        response.status_code = r.status_code\n        return response\n\n    except Exception as e:\n        raise DonationError(donation, e)"}
{"input": "def set_extension(self):\n        i_accept = self.setting('input-extensions')\n        i_output = self.setting('output-extensions')\n\n        if self.prev_filter:\n            prev_ext = self.prev_filter.ext\n        else:\n            prev_ext = self.doc.ext\n\n        # Check that we can handle input extension\n        if set([prev_ext, \".*\"]).isdisjoint(set(i_accept)):\n            msg = \"Filter '%s' in '%s' can't handle file extension %s, supported extensions are %s\"\n            params = (self.filter_alias, self.key, prev_ext, \", \".join(i_accept))\n            raise dexy.exceptions.UserFeedback(msg % params)\n\n        # Figure out output extension\n        ext = self.setting('ext')\n        if ext:\n            # User has specified desired extension\n            if not ext.startswith('.'):\n                ext = '.%s' % ext\n\n            # Make sure it's a valid one\n            if (not ext in i_output) and (not \".*\" in i_output):\n                msg = \"You have requested file extension %s in %s but filter %s can't generate that.\"\n                raise dexy.exceptions.UserFeedback(msg % (ext, self.key, self.filter_alias))\n\n            self.ext = ext\n\n        elif \".*\" in i_output:\n            self.ext = prev_ext\n\n        else:\n            # User has not specified desired extension, and we don't output wildcards,\n            # figure out extension based on next filter in sequence, if any.\n            if self.next_filter:\n                next_filter_accepts = self.next_filter.setting('input-extensions')\n\n                if \".*\" in next_filter_accepts:\n                    self.ext = i_output[0]\n                else:\n                    if set(i_output).isdisjoint(set(next_filter_accepts)):\n                        msg = \"Filter %s can't go after filter %s, no file extensions in common.\"\n                        raise dexy.exceptions.UserFeedback(msg % (self.next_filter_alias, self.filter_alias))\n\n                    for e in i_output:\n                        if e in next_filter_accepts:\n                            self.ext = e\n\n                    if not self.ext:\n                        msg = \"no file extension found but checked already for disjointed, should not be here\"\n                        raise dexy.exceptions.InternalDexyProblem(msg)\n            else:\n                self.ext = i_output[0]", "output": "def set_extension(self):\n        i_accept = self.setting('input-extensions')\n        i_output = self.setting('output-extensions')\n\n        if self.prev_filter:\n            prev_ext = self.prev_filter.ext\n        else:\n            prev_ext = self.doc.ext\n\n        # Check that we can handle input extension\n        if set([prev_ext, \".*\"]).isdisjoint(set(i_accept)):\n            msg = \"Filter '%s' in '%s' can't handle file extension %s, supported extensions are %s\"\n            params = (self.filter_alias, self.key, prev_ext, \", \".join(i_accept))\n            raise dexy.exceptions.UserFeedback(msg % params)\n\n        # Figure out output extension\n        ext = self.setting('ext')\n        if ext:\n            # User has specified desired extension\n            if not ext.startswith('.'):\n                ext = '.%s' % ext\n\n            # Make sure it's a valid one\n            if (not ext in i_output) and (not \".*\" in i_output):\n                msg = \"You have requested file extension %s in %s but filter %s can't generate that.\"\n                raise dexy.exceptions.UserFeedback(msg % (ext, self.key, self.filter_alias))\n\n            self.ext = ext\n\n        elif \".*\" in i_output:\n            self.ext = prev_ext\n\n        else:\n            # User has not specified desired extension, and we don't output wildcards,\n            # figure out extension based on next filter in sequence, if any.\n            if self.next_filter:\n                next_filter_accepts = self.next_filter.setting('input-extensions')\n\n                if \".*\" in next_filter_accepts:\n                    self.ext = i_output[0]\n                else:\n                    if set(i_output).isdisjoint(set(next_filter_accepts)):\n                        msg = \"Filter %s can't go after filter %s, no file extensions in common.\"\n                        raise dexy.exceptions.UserFeedback(msg % (self.next_filter.alias, self.alias))\n\n                    for e in i_output:\n                        if e in next_filter_accepts:\n                            self.ext = e\n\n                    if not self.ext:\n                        msg = \"no file extension found but checked already for disjointed, should not be here\"\n                        raise dexy.exceptions.InternalDexyProblem(msg)\n            else:\n                self.ext = i_output[0]"}
{"input": "def connect_to_db(self):\n        '''Connects to the database'''\n        if self.db_type == DB_MYSQL:\n            try:\n                import pymysql\n            except ImportError as _err:\n                # TODO: Add python3-mysql when available or consider mysql-connector when available for python3\n                raise Exception(import_error_to_help(_module=\"pymysql\", _err_obj=_err, _pip_package=\"pymysql3\",\n                                                     _apt_package=None, _win_package=None))\n\n            _connection = pymysql.connect (host = self.db_server,\n                            db = self.db_databasename,\n                            user = self.db_username,\n                            passwd = self.db_password,\n                            )\n            \n\n        elif self.db_type == DB_POSTGRESQL:\n\n            try:\n                import postgresql.driver as pg_driver\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"postgresql\", _err_obj=_err,\n                                                     _pip_package=\"py-postgresql\",\n                                                     _apt_package=\"python3-postgresql\",\n                                                     _win_package=None,\n                                                     _import_comment=\"2014-04-16: If using apt-get, \" +\n                                                                     \"check so version is > 1.0.3-2\" +\n                                                     \" as there is a severe bug in the 1.02 version. \" +\n                                                     \"See https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=724597\"))\n\n            if self.db_port == None or self.db_port == \"\" or self.db_port == 0:\n                _port = 5432\n            else:\n                _port = self.db_port\n            _connection = pg_driver.connect(host = self.db_server,\n                                                database =  self.db_databasename, \n                                                user = self.db_username, \n                                                password = self.db_password,\n                                                port = _port)\n                            \n        elif self.db_type in [DB_SQLSERVER, DB_DB2]:\n            _connection_string = None\n            try:\n                import pyodbc\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"pyodbc\", _err_obj=_err,\n                                                     _pip_package=\"pyodbc\",\n                                                     _apt_package=\"None\",\n                                                     _win_package=None,\n                                                     _import_comment=\"2014-04-16: \" +\n                                                                     \"No apt package (python3-pyodbc)\"+\n                                                                     \" available at this time.\"))\n            import platform\n\n            #TODO: Investigate if there is any more adapting needed, platform.release() can also be used.\n\n            if self.db_type == DB_SQLSERVER:\n                if platform.system().lower() == 'linux':\n                    _connection_string = \"DRIVER={FreeTDS};SERVER=\" + self.db_server + \";DATABASE=\" + \\\n                                         self.db_databasename +\";TDS_VERSION=8.0;UID=\" + self.db_username + \";PWD=\" + \\\n                                         self.db_password + \";PORT=\"+str(self.db_port) + \";Trusted_Connection=no;\"\n                elif platform.system().lower() == 'windows':\n                    _connection_string = \"Driver={SQL Server};Server=\" + self.db_server + \";DATABASE=\" + \\\n                                         self.db_databasename +\";UID=\" + self.db_username + \";PWD=\" + self.db_password +\\\n                                         \";PORT=\"+str(self.db_port) + \";Trusted_Connection=no\"\n                else:\n                    raise Exception(\"connect_to_db: ODBC connections on \" + platform.system() + \" not supported yet.\")\n\n\n            elif self.db_type == DB_DB2:\n\n                if platform.system().lower() == 'linux':\n                    drivername = \"DB2\"\n                elif platform.system().lower() == 'windows':\n                    drivername = \"{IBM DATA SERVER DRIVER for ODBC - C:/PROGRA~1/IBM}\"\n                else:\n                    raise Exception(\"connect_to_db: DB2 connections on \" + platform.system() + \" not supported yet.\")\n\n                # DSN-less?{IBM DB2 ODBC DRIVER} ?? http://www.webmasterworld.com/forum88/4434.htm\n                _connection_string =  \"Driver=\" + drivername + \";Database=\" + self.db_databasename +\";hostname=\" + \\\n                                      self.db_server + \";port=\"+str(self.db_port) + \";protocol=TCPIP; uid=\" + \\\n                                      self.db_username + \"; pwd=\" + self.db_password\n                #_connection_string = \"DSN=\" + self.db_server + \";UID=\" + self.db_username + \";PWD=\" + self.db_password\n                _connection = pyodbc.connect(_connection_string, autocommit=self.db_autocommit)\n\n\n                # DSN-less?{IBM DB2 ODBC DRIVER} ?? http://www.webmasterworld.com/forum88/4434.htm\n                _connection_string =  \"Driver=\" + drivername + \";Database=\" + self.db_databasename +\";hostname=\" + \\\n                                      self.db_server + \";port=\"+str(self.db_port) + \";protocol=TCPIP; uid=\" + \\\n                                      self.db_username + \"; pwd=\" + self.db_password\n                #_connection_string = \"DSN=\" + self.db_server + \";UID=\" + self.db_username + \";PWD=\" + self.db_password\n\n            print(\"Connect to database using connection string:  \" + _connection_string)\n            _connection = pyodbc.connect(_connection_string, autocommit=self.db_autocommit)\n        \n        # cx_Oracle in python 3.X not checked yet.\n        elif self.db_type == DB_ORACLE:\n            try:\n                import cx_Oracle\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"cx_Oracle\", _err_obj=_err,\n                                                     _pip_package=\"cx_Oracle\",\n                                                     _apt_package=\"None\",\n                                                     _win_package=\"Download and install binary .msi package from \" +\n                                                                  \"http://cx-oracle.sourceforge.net/ and install.\",\n                                                     _import_comment=\"2014-04-16: No python3-pyodbc available at\" +\n                                                                     \" build time.\"))\n\n            _connection_string = self.db_username + '/' +  self.db_password + '@' + self.db_server + ':' + \\\n                                 str(self.db_port) + '/' + self.db_instance\n            print(\"Connect to database using connection string:  \" + _connection_string)\n            _connection = cx_Oracle.connect(_connection_string)\n            _connection.autocommit=self.db_autocommit\n                  \n        else:\n            raise Exception(\"connect_to_db: Invalid database type.\")              \n      \n        \n        self.db_connection = _connection\n        \n        if self.on_connect:\n            self.on_connect() \n        self.connected = True\n            \n        return _connection", "output": "def connect_to_db(self):\n        '''Connects to the database'''\n        if self.db_type == DB_MYSQL:\n            try:\n                import pymysql\n            except ImportError as _err:\n                # TODO: Add python3-mysql when available or consider mysql-connector when available for python3\n                raise Exception(import_error_to_help(_module=\"pymysql\", _err_obj=_err, _pip_package=\"pymysql3\",\n                                                     _apt_package=None, _win_package=None))\n\n            _connection = pymysql.connect (host = self.db_server,\n                            db = self.db_databasename,\n                            user = self.db_username,\n                            passwd = self.db_password,\n                            )\n            \n\n        elif self.db_type == DB_POSTGRESQL:\n\n            try:\n                import postgresql.driver as pg_driver\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"postgresql\", _err_obj=_err,\n                                                     _pip_package=\"py-postgresql\",\n                                                     _apt_package=\"python3-postgresql\",\n                                                     _win_package=None,\n                                                     _import_comment=\"2014-04-16: If using apt-get, \" +\n                                                                     \"check so version is > 1.0.3-2\" +\n                                                     \" as there is a severe bug in the 1.02 version. \" +\n                                                     \"See https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=724597\"))\n\n            if self.db_port == None or self.db_port == \"\" or self.db_port == 0:\n                _port = 5432\n            else:\n                _port = self.db_port\n            _connection = pg_driver.connect(host = self.db_server,\n                                                database =  self.db_databasename, \n                                                user = self.db_username, \n                                                password = self.db_password,\n                                                port = _port)\n                            \n        elif self.db_type in [DB_SQLSERVER, DB_DB2]:\n            _connection_string = None\n            try:\n                import pyodbc\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"pyodbc\", _err_obj=_err,\n                                                     _pip_package=\"pyodbc\",\n                                                     _apt_package=\"None\",\n                                                     _win_package=None,\n                                                     _import_comment=\"Linux 2014-04-16: \" +\n                                                                     \"No apt package (python3-pyodbc)\"+\n                                                                     \" available at this time.\"))\n            import platform\n\n            #TODO: Investigate if there is any more adapting needed, platform.release() can also be used.\n\n            if self.db_type == DB_SQLSERVER:\n                if platform.system().lower() == 'linux':\n                    _connection_string = \"DRIVER={FreeTDS};SERVER=\" + self.db_server + \";DATABASE=\" + \\\n                                         self.db_databasename +\";TDS_VERSION=8.0;UID=\" + self.db_username + \";PWD=\" + \\\n                                         self.db_password + \";PORT=\"+str(self.db_port) + \";Trusted_Connection=no;\"\n                elif platform.system().lower() == 'windows':\n                    _connection_string = \"Driver={SQL Server};Server=\" + self.db_server + \";DATABASE=\" + \\\n                                         self.db_databasename +\";UID=\" + self.db_username + \";PWD=\" + self.db_password +\\\n                                         \";PORT=\"+str(self.db_port) + \";Trusted_Connection=no\"\n                else:\n                    raise Exception(\"connect_to_db: ODBC connections on \" + platform.system() + \" not supported yet.\")\n\n\n            elif self.db_type == DB_DB2:\n\n                if platform.system().lower() == 'linux':\n                    drivername = \"DB2\"\n                elif platform.system().lower() == 'windows':\n                    drivername = \"{IBM DATA SERVER DRIVER for ODBC - C:/PROGRA~1/IBM}\"\n                else:\n                    raise Exception(\"connect_to_db: DB2 connections on \" + platform.system() + \" not supported yet.\")\n\n                # DSN-less?{IBM DB2 ODBC DRIVER} ?? http://www.webmasterworld.com/forum88/4434.htm\n                _connection_string =  \"Driver=\" + drivername + \";Database=\" + self.db_databasename +\";hostname=\" + \\\n                                      self.db_server + \";port=\"+str(self.db_port) + \";protocol=TCPIP; uid=\" + \\\n                                      self.db_username + \"; pwd=\" + self.db_password\n                #_connection_string = \"DSN=\" + self.db_server + \";UID=\" + self.db_username + \";PWD=\" + self.db_password\n                _connection = pyodbc.connect(_connection_string, autocommit=self.db_autocommit)\n\n\n                # DSN-less?{IBM DB2 ODBC DRIVER} ?? http://www.webmasterworld.com/forum88/4434.htm\n                _connection_string =  \"Driver=\" + drivername + \";Database=\" + self.db_databasename +\";hostname=\" + \\\n                                      self.db_server + \";port=\"+str(self.db_port) + \";protocol=TCPIP; uid=\" + \\\n                                      self.db_username + \"; pwd=\" + self.db_password\n                #_connection_string = \"DSN=\" + self.db_server + \";UID=\" + self.db_username + \";PWD=\" + self.db_password\n\n            print(\"Connect to database using connection string:  \" + _connection_string)\n            _connection = pyodbc.connect(_connection_string, autocommit=self.db_autocommit)\n        \n        # cx_Oracle in python 3.X not checked yet.\n        elif self.db_type == DB_ORACLE:\n            try:\n                import cx_Oracle\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"cx_Oracle\", _err_obj=_err,\n                                                     _pip_package=\"cx_Oracle\",\n                                                     _apt_package=\"None\",\n                                                     _win_package=\"Download and install binary .msi package from \" +\n                                                                  \"http://cx-oracle.sourceforge.net/ and install.\",\n                                                     _import_comment=\"2014-04-16: No python3-pyodbc available at\" +\n                                                                     \" build time.\"))\n\n            _connection_string = self.db_username + '/' +  self.db_password + '@' + self.db_server + ':' + \\\n                                 str(self.db_port) + '/' + self.db_instance\n            print(\"Connect to database using connection string:  \" + _connection_string)\n            _connection = cx_Oracle.connect(_connection_string)\n            _connection.autocommit=self.db_autocommit\n                  \n        else:\n            raise Exception(\"connect_to_db: Invalid database type.\")              \n      \n        \n        self.db_connection = _connection\n        \n        if self.on_connect:\n            self.on_connect() \n        self.connected = True\n            \n        return _connection"}
{"input": "def collect_static(self):\n        \"\"\"\n        calls collect static files\n        \"\"\"\n        config_path = (\"%(path)s/%(project_name)s/config/%(step)/\" % env)\n        result = with_virtualenv_remote(\"cd %s;python manage.py collectstatic\" % config_path)", "output": "def collect_static(self):\n        \"\"\"\n        calls collect static files\n        \"\"\"\n        config_path = (\"%(path)s%(project_name)s/config/%(step)s/\" % env)\n        result = with_virtualenv_remote(\"cd %s;python manage.py collectstatic\" % config_path)"}
{"input": "def get_pr(repo, owner, sha):\n    url = (\"https://api.github.com/search/issues?q=type:pr+is:merged+repo:%s/%s+%s\" %\n           (repo, owner, sha))\n    try:\n        resp = urllib2.urlopen(url)\n        body = resp.read()\n    except Exception as e:\n        logger.error(e)\n        return None\n\n    if resp.code != 200:\n        logger.error(\"Got HTTP status %s. Response:\" % resp.code)\n        logger.error(body)\n        return None\n\n    try:\n        data = json.loads(body)\n    except ValueError:\n        logger.error(\"Failed to read response as JSON:\")\n        logger.error(body)\n        return None\n\n    items = data[\"items\"]\n    if len(items) == 0:\n        logger.error(\"No PR found for %s\" % sha)\n        return None\n    if len(items) > 1:\n        logger.warning(\"Found multiple PRs for %s\" % sha)\n\n    pr = items[0]\n\n    return pr[\"number\"]", "output": "def get_pr(repo, owner, sha):\n    url = (\"https://api.github.com/search/issues?q=type:pr+is:merged+repo:%s/%s+sha:%s\" %\n           (repo, owner, sha))\n    try:\n        resp = urllib2.urlopen(url)\n        body = resp.read()\n    except Exception as e:\n        logger.error(e)\n        return None\n\n    if resp.code != 200:\n        logger.error(\"Got HTTP status %s. Response:\" % resp.code)\n        logger.error(body)\n        return None\n\n    try:\n        data = json.loads(body)\n    except ValueError:\n        logger.error(\"Failed to read response as JSON:\")\n        logger.error(body)\n        return None\n\n    items = data[\"items\"]\n    if len(items) == 0:\n        logger.error(\"No PR found for %s\" % sha)\n        return None\n    if len(items) > 1:\n        logger.warning(\"Found multiple PRs for %s\" % sha)\n\n    pr = items[0]\n\n    return pr[\"number\"]"}
{"input": "def __repr__(self):\n\t\treturn \"<SimPath with %d runs>\" % self.length", "output": "def __repr__(self):\n\t\treturn \"<SimPath with %d runs>\" % (0 if not hasattr(self, 'length') else self.length)"}
{"input": "def name_is_webdriver(self):\n        \"\"\"Check if the file name matches the conditions for the file to\n        be a webdriver spec test file\"\"\"\n        # wdspec tests are in subdirectories of /webdriver excluding __init__.py\n        # files.\n        rel_dir_tree = self.rel_path.split(os.path.sep)\n        return (rel_dir_tree[0] == \"webdriver\" and\n                len(rel_dir_tree) > 1 and\n                self.filename != \"__init__.py\" and\n                fnmatch(self.filename, wd_pattern))", "output": "def name_is_webdriver(self):\n        \"\"\"Check if the file name matches the conditions for the file to\n        be a webdriver spec test file\"\"\"\n        # wdspec tests are in subdirectories of /webdriver excluding __init__.py\n        # files.\n        rel_dir_tree = self.rel_path.split(os.path.sep)\n        return (rel_dir_tree[0] == \"webdriver\" and\n                len(rel_dir_tree) > 1 and\n                self.filename not in (\"__init__.py\", \"conftest.py\") and\n                fnmatch(self.filename, wd_pattern))"}
{"input": "def _print_Mul(self, expr):\n        # print complex numbers nicely in Octave\n        if (expr.is_number and expr.is_imaginary and\n                expr.as_coeff_Mul()[0].is_integer):\n            return \"%si\" % self._print(-S.ImaginaryUnit*expr)\n\n        # cribbed from str.py\n        prec = precedence(expr)\n\n        c, e = expr.as_coeff_Mul()\n        if c < 0:\n            expr = _keep_coeff(-c, e)\n            sign = \"-\"\n        else:\n            sign = \"\"\n\n        a = []  # items in the numerator\n        b = []  # items that are in the denominator (if any)\n\n        if self.order not in ('old', 'none'):\n            args = expr.as_ordered_factors()\n        else:\n            # use make_args in case expr was something like -x -> x\n            args = Mul.make_args(expr)\n\n        # Gather args for numerator/denominator\n        for item in args:\n            if (item.is_commutative and item.is_Pow and item.exp.is_Rational\n                    and item.exp.is_negative):\n                if item.exp != -1:\n                    b.append(Pow(item.base, -item.exp, evaluate=False))\n                else:\n                    b.append(Pow(item.base, -item.exp))\n            elif item.is_Rational and item is not S.Infinity:\n                if item.p != 1:\n                    a.append(Rational(item.p))\n                if item.q != 1:\n                    b.append(Rational(item.q))\n            else:\n                a.append(item)\n\n        a = a or [S.One]\n\n        a_str = [self.parenthesize(x, prec) for x in a]\n        b_str = [self.parenthesize(x, prec) for x in b]\n\n        # from here it differs from str.py to deal with \"*\" and \".*\"\n        def multjoin(a, a_str):\n            # here we probably are assuming the constants will come first\n            r = a_str[0]\n            for i in range(1, len(a)):\n                mulsym = '*' if a[i-1].is_number else '.*'\n                r = r + mulsym + a_str[i]\n            return r\n\n        if len(b) == 0:\n            return sign + multjoin(a, a_str)\n        elif len(b) == 1:\n            divsym = '/' if b[0].is_number else './'\n            return sign + multjoin(a, a_str) + divsym + b_str[0]\n        else:\n            divsym = '/' if all([bi.is_number for bi in b]) else './'\n            return (sign + multjoin(a, a_str) +\n                    divsym + \"(%s)\" % multjoin(b, b_str))", "output": "def _print_Mul(self, expr):\n        # print complex numbers nicely in Octave\n        if (expr.is_number and expr.is_imaginary and\n                expr.as_coeff_Mul()[0].is_integer):\n            return \"%s*i\" % self._print(-S.ImaginaryUnit*expr)\n\n        # cribbed from str.py\n        prec = precedence(expr)\n\n        c, e = expr.as_coeff_Mul()\n        if c < 0:\n            expr = _keep_coeff(-c, e)\n            sign = \"-\"\n        else:\n            sign = \"\"\n\n        a = []  # items in the numerator\n        b = []  # items that are in the denominator (if any)\n\n        if self.order not in ('old', 'none'):\n            args = expr.as_ordered_factors()\n        else:\n            # use make_args in case expr was something like -x -> x\n            args = Mul.make_args(expr)\n\n        # Gather args for numerator/denominator\n        for item in args:\n            if (item.is_commutative and item.is_Pow and item.exp.is_Rational\n                    and item.exp.is_negative):\n                if item.exp != -1:\n                    b.append(Pow(item.base, -item.exp, evaluate=False))\n                else:\n                    b.append(Pow(item.base, -item.exp))\n            elif item.is_Rational and item is not S.Infinity:\n                if item.p != 1:\n                    a.append(Rational(item.p))\n                if item.q != 1:\n                    b.append(Rational(item.q))\n            else:\n                a.append(item)\n\n        a = a or [S.One]\n\n        a_str = [self.parenthesize(x, prec) for x in a]\n        b_str = [self.parenthesize(x, prec) for x in b]\n\n        # from here it differs from str.py to deal with \"*\" and \".*\"\n        def multjoin(a, a_str):\n            # here we probably are assuming the constants will come first\n            r = a_str[0]\n            for i in range(1, len(a)):\n                mulsym = '*' if a[i-1].is_number else '.*'\n                r = r + mulsym + a_str[i]\n            return r\n\n        if len(b) == 0:\n            return sign + multjoin(a, a_str)\n        elif len(b) == 1:\n            divsym = '/' if b[0].is_number else './'\n            return sign + multjoin(a, a_str) + divsym + b_str[0]\n        else:\n            divsym = '/' if all([bi.is_number for bi in b]) else './'\n            return (sign + multjoin(a, a_str) +\n                    divsym + \"(%s)\" % multjoin(b, b_str))"}
{"input": "def eval(cls, z):\n        if z is S.NaN:\n            return S.NaN\n        elif z is S.NegativeOne:\n            return S.NegativeInfinity\n        elif z is S.Zero:\n            return S.Zero\n        elif z is S.One:\n            return S.Infinity\n\n        if (isinstance(z, erf)) and z.args[0].is_real:\n            return z.args[0]\n\n        # Try to pull out factors of -1\n        nz = z.extract_multiplicatively(-1)\n        if nz is not None and ((isinstance(nz, erf)) and (nz.args[0]).is_real):\n            return -nz.args[0]", "output": "def eval(cls, z):\n        if z is S.NaN:\n            return S.NaN\n        elif z is S.NegativeOne:\n            return S.NegativeInfinity\n        elif z is S.Zero:\n            return S.Zero\n        elif z is S.One:\n            return S.Infinity\n\n        if isinstance(z, erf) and z.args[0].is_real:\n            return z.args[0]\n\n        # Try to pull out factors of -1\n        nz = z.extract_multiplicatively(-1)\n        if nz is not None and ((isinstance(nz, erf)) and (nz.args[0]).is_real):\n            return -nz.args[0]"}
{"input": "def mk_url(comp, tp):\n        _comp = '-' + comp if comp else ''\n        return '%s/%s/bokeh-%s%s.%s' % (base_url, container, _comp, version, _min, tp)", "output": "def mk_url(comp, tp):\n        _comp = '-' + comp if comp else ''\n        return '%s/%s/bokeh%s-%s%s.%s' % (base_url, container, _comp, version, _min, tp)"}
{"input": "def upgrade(migrate_engine):\n    # Upgrade operations go here. Don't create your own engine; bind migrate_engine\n    # to your metadata \n    meta = MetaData(migrate_engine)\n    \n    # change db defaults\n    sql_update_db = \"alter database `changebyus_demo` DEFAULT CHARACTER SET = 'utf8' DEFAULT COLLATE = 'utf8_general_ci'\"\n    migrate_engine.execute(sql_update_db)\n    \n    # populate ALL tables in metadata\n    meta.reflect(migrate_engine)\n    \n    #change table defaults\n    tbls = meta.tables.keys()\n    \n    for item in tbls:\n        sql_update_table = \"alter table `%s` DEFAULT CHARACTER SET 'utf8' DEFAULT COLLATE 'utf8_general_ci'\" % item\n        migrate_engine.execute(sql_update_table)", "output": "def upgrade(migrate_engine):\n    # Upgrade operations go here. Don't create your own engine; bind migrate_engine\n    # to your metadata \n    meta = MetaData(migrate_engine)\n    \n    # change db defaults\n    sql_update_db = \"alter database `changebyus_demo` DEFAULT CHARACTER SET = 'utf8' DEFAULT COLLATE = 'utf8_general_ci'\"\n    migrate_engine.execute(sql_update_db)\n    \n    # populate ALL tables in metadata\n    meta.reflect(migrate_engine)\n    \n    #change table defaults\n    tbls = meta.tables.keys()\n    \n    for item in tbls:\n        sql_update_table = \"alter table `%s` CHARACTER SET 'utf8' COLLATE 'utf8_general_ci'\" % item\n        migrate_engine.execute(sql_update_table)"}
{"input": "def _diop_quadratic(var, coeff, t):\n\n    x, y = var\n\n    A = coeff[x**2]\n    B = coeff[x*y]\n    C = coeff[y**2]\n    D = coeff[x]\n    E = coeff[y]\n    F = coeff[1]\n\n    A, B, C, D, E, F = [as_int(i) for i in _remove_gcd(A, B, C, D, E, F)]\n\n    # (1) Simple-Hyperbolic case: A = C = 0, B != 0\n    # In this case equation can be converted to (Bx + E)(By + D) = DE - BF\n    # We consider two cases; DE - BF = 0 and DE - BF != 0\n    # More details, http://www.alpertron.com.ar/METHODS.HTM#SHyperb\n\n    sol = set([])\n    discr = B**2 - 4*A*C\n    if A == 0 and C == 0 and B != 0:\n\n        if D*E - B*F == 0:\n            q, r = divmod(E, B)\n            if not r:\n                sol.add((-q, t))\n            q, r = divmod(D, B)\n            if not r:\n                sol.add((t, -q))\n        else:\n            div = divisors(D*E - B*F)\n            div = div + [-term for term in div]\n            for d in div:\n                x0, r = divmod(d - E, B)\n                if not r:\n                    q, r = divmod(D*E - B*F, d)\n                    if not r:\n                        y0, r = divmod(q - D, B)\n                        if not r:\n                            sol.add((x0, y0))\n\n    # (2) Parabolic case: B**2 - 4*A*C = 0\n    # There are two subcases to be considered in this case.\n    # sqrt(c)D - sqrt(a)E = 0 and sqrt(c)D - sqrt(a)E != 0\n    # More Details, http://www.alpertron.com.ar/METHODS.HTM#Parabol\n\n    elif discr == 0:\n\n        if A == 0:\n            s = _diop_quadratic([y, x], coeff, t)\n            for soln in s:\n                sol.add((soln[1], soln[0]))\n\n        else:\n            g = sign(A)*igcd(A, C)\n            a = A // g\n            c = C // g\n            e = sign(B/A)\n\n            sqa = isqrt(a)\n            sqc = isqrt(c)\n            _c = e*sqc*D - sqa*E\n            if not _c:\n                z = symbols(\"z\", real=True)\n                eq = sqa*g*z**2 + D*z + sqa*F\n                roots = solveset_real(eq, z).intersect(S.Integers)\n                for root in roots:\n                    ans = diop_solve(sqa*x + e*sqc*y - root)\n                    sol.add((ans[0], ans[1]))\n\n            elif _is_int(c):\n                solve_x = lambda u: -e*sqc*g*_c*t**2 - (E + 2*e*sqc*g*u)*t\\\n                    - (e*sqc*g*u**2 + E*u + e*sqc*F) // _c\n\n                solve_y = lambda u: sqa*g*_c*t**2 + (D + 2*sqa*g*u)*t \\\n                    + (sqa*g*u**2 + D*u + sqa*F) // _c\n\n                for z0 in range(0, abs(_c)):\n                    # Check if the coefficients of y and x obtained are integers or not\n                    if (divisible(sqa*g*z0**2 + D*z0 + sqa*F, _c) and\n                            divisible(e*sqc**g*z0**2 + E*z0 + e*sqc*F, _c)):\n                        sol.add((solve_x(z0), solve_y(z0)))\n\n    # (3) Method used when B**2 - 4*A*C is a square, is described in p. 6 of the below paper\n    # by John P. Robertson.\n    # http://www.jpr2718.org/ax2p.pdf\n\n    elif is_square(discr):\n        if A != 0:\n            r = sqrt(discr)\n            u, v = symbols(\"u, v\", integer=True)\n            eq = _mexpand(\n                4*A*r*u*v + 4*A*D*(B*v + r*u + r*v - B*u) +\n                2*A*4*A*E*(u - v) + 4*A*r*4*A*F)\n\n            solution = diop_solve(eq, t)\n\n            for s0, t0 in solution:\n\n                num = B*t0 + r*s0 + r*t0 - B*s0\n                x_0 = S(num)/(4*A*r)\n                y_0 = S(s0 - t0)/(2*r)\n                if isinstance(s0, Symbol) or isinstance(t0, Symbol):\n                    if check_param(x_0, y_0, 4*A*r, t) != (None, None):\n                        ans = check_param(x_0, y_0, 4*A*r, t)\n                        sol.add((ans[0], ans[1]))\n                elif x_0.is_Integer and y_0.is_Integer:\n                    if is_solution_quad(var, coeff, x_0, y_0):\n                        sol.add((x_0, y_0))\n\n        else:\n            s = _diop_quadratic(var[::-1], coeff, t)  # Interchange x and y\n            while s:                                  #         |\n                sol.add(s.pop()[::-1])  # and solution <--------+\n\n\n    # (4) B**2 - 4*A*C > 0 and B**2 - 4*A*C not a square or B**2 - 4*A*C < 0\n\n    else:\n\n        P, Q = _transformation_to_DN(var, coeff)\n        D, N = _find_DN(var, coeff)\n        solns_pell = diop_DN(D, N)\n\n        if D < 0:\n            for x0, y0 in solns_pell:\n                for x in [-x0, x0]:\n                    for y in [-y0, y0]:\n                        s = P*Matrix([x, y]) + Q\n                        try:\n                            sol.add(tuple([as_int(_) for _ in s]))\n                        except ValueError:\n                            pass\n        else:\n            # In this case equation can be transformed into a Pell equation\n\n            solns_pell = set(solns_pell)\n            for X, Y in list(solns_pell):\n                solns_pell.add((-X, -Y))\n\n            a = diop_DN(D, 1)\n            T = a[0][0]\n            U = a[0][1]\n\n            if all(_is_int(_) for _ in P[:4] + Q[:2]):\n                for r, s in solns_pell:\n                    _a = (r + s*sqrt(D))*(T + U*sqrt(D))**t\n                    _b = (r - s*sqrt(D))*(T - U*sqrt(D))**t\n                    x_n = _mexpand(S(_a + _b)/2)\n                    y_n = _mexpand(S(_a - _b)/(2*sqrt(D)))\n                    s = P*Matrix([x_n, y_n]) + Q\n                    sol.add(tuple(s))\n\n            else:\n                L = ilcm(*[_.q for _ in P[:4] + Q[:2]])\n\n                k = 1\n\n                T_k = T\n                U_k = U\n\n                while (T_k - 1) % L != 0 or U_k % L != 0:\n                    T_k, U_k = T_k*T + D*U_k*U, T_k*U + U_k*T\n                    k += 1\n\n                for X, Y in solns_pell:\n\n                    for i in range(k):\n                        if all(_is_int(_) for _ in P*Matrix([X, Y]) + Q):\n                            _a = (X + sqrt(D)*Y)*(T_k + sqrt(D)*U_k)**t\n                            _b = (X - sqrt(D)*Y)*(T_k - sqrt(D)*U_k)**t\n                            Xt = S(_a + _b)/2\n                            Yt = S(_a - _b)/(2*sqrt(D))\n                            s = P*Matrix([Xt, Yt]) + Q\n                            sol.add(tuple(s))\n\n                        X, Y = X*T + D*U*Y, X*U + Y*T\n\n    return sol", "output": "def _diop_quadratic(var, coeff, t):\n\n    x, y = var\n\n    A = coeff[x**2]\n    B = coeff[x*y]\n    C = coeff[y**2]\n    D = coeff[x]\n    E = coeff[y]\n    F = coeff[1]\n\n    A, B, C, D, E, F = [as_int(i) for i in _remove_gcd(A, B, C, D, E, F)]\n\n    # (1) Simple-Hyperbolic case: A = C = 0, B != 0\n    # In this case equation can be converted to (Bx + E)(By + D) = DE - BF\n    # We consider two cases; DE - BF = 0 and DE - BF != 0\n    # More details, http://www.alpertron.com.ar/METHODS.HTM#SHyperb\n\n    sol = set([])\n    discr = B**2 - 4*A*C\n    if A == 0 and C == 0 and B != 0:\n\n        if D*E - B*F == 0:\n            q, r = divmod(E, B)\n            if not r:\n                sol.add((-q, t))\n            q, r = divmod(D, B)\n            if not r:\n                sol.add((t, -q))\n        else:\n            div = divisors(D*E - B*F)\n            div = div + [-term for term in div]\n            for d in div:\n                x0, r = divmod(d - E, B)\n                if not r:\n                    q, r = divmod(D*E - B*F, d)\n                    if not r:\n                        y0, r = divmod(q - D, B)\n                        if not r:\n                            sol.add((x0, y0))\n\n    # (2) Parabolic case: B**2 - 4*A*C = 0\n    # There are two subcases to be considered in this case.\n    # sqrt(c)D - sqrt(a)E = 0 and sqrt(c)D - sqrt(a)E != 0\n    # More Details, http://www.alpertron.com.ar/METHODS.HTM#Parabol\n\n    elif discr == 0:\n\n        if A == 0:\n            s = _diop_quadratic([y, x], coeff, t)\n            for soln in s:\n                sol.add((soln[1], soln[0]))\n\n        else:\n            g = sign(A)*igcd(A, C)\n            a = A // g\n            c = C // g\n            e = sign(B/A)\n\n            sqa = isqrt(a)\n            sqc = isqrt(c)\n            _c = e*sqc*D - sqa*E\n            if not _c:\n                z = symbols(\"z\", real=True)\n                eq = sqa*g*z**2 + D*z + sqa*F\n                roots = solveset_real(eq, z).intersect(S.Integers)\n                for root in roots:\n                    ans = diop_solve(sqa*x + e*sqc*y - root)\n                    sol.add((ans[0], ans[1]))\n\n            elif _is_int(c):\n                solve_x = lambda u: -e*sqc*g*_c*t**2 - (E + 2*e*sqc*g*u)*t\\\n                    - (e*sqc*g*u**2 + E*u + e*sqc*F) // _c\n\n                solve_y = lambda u: sqa*g*_c*t**2 + (D + 2*sqa*g*u)*t \\\n                    + (sqa*g*u**2 + D*u + sqa*F) // _c\n\n                for z0 in range(0, abs(_c)):\n                    # Check if the coefficients of y and x obtained are integers or not\n                    if (divisible(sqa*g*z0**2 + D*z0 + sqa*F, _c) and\n                            divisible(e*sqc*g*z0**2 + E*z0 + e*sqc*F, _c)):\n                        sol.add((solve_x(z0), solve_y(z0)))\n\n    # (3) Method used when B**2 - 4*A*C is a square, is described in p. 6 of the below paper\n    # by John P. Robertson.\n    # http://www.jpr2718.org/ax2p.pdf\n\n    elif is_square(discr):\n        if A != 0:\n            r = sqrt(discr)\n            u, v = symbols(\"u, v\", integer=True)\n            eq = _mexpand(\n                4*A*r*u*v + 4*A*D*(B*v + r*u + r*v - B*u) +\n                2*A*4*A*E*(u - v) + 4*A*r*4*A*F)\n\n            solution = diop_solve(eq, t)\n\n            for s0, t0 in solution:\n\n                num = B*t0 + r*s0 + r*t0 - B*s0\n                x_0 = S(num)/(4*A*r)\n                y_0 = S(s0 - t0)/(2*r)\n                if isinstance(s0, Symbol) or isinstance(t0, Symbol):\n                    if check_param(x_0, y_0, 4*A*r, t) != (None, None):\n                        ans = check_param(x_0, y_0, 4*A*r, t)\n                        sol.add((ans[0], ans[1]))\n                elif x_0.is_Integer and y_0.is_Integer:\n                    if is_solution_quad(var, coeff, x_0, y_0):\n                        sol.add((x_0, y_0))\n\n        else:\n            s = _diop_quadratic(var[::-1], coeff, t)  # Interchange x and y\n            while s:                                  #         |\n                sol.add(s.pop()[::-1])  # and solution <--------+\n\n\n    # (4) B**2 - 4*A*C > 0 and B**2 - 4*A*C not a square or B**2 - 4*A*C < 0\n\n    else:\n\n        P, Q = _transformation_to_DN(var, coeff)\n        D, N = _find_DN(var, coeff)\n        solns_pell = diop_DN(D, N)\n\n        if D < 0:\n            for x0, y0 in solns_pell:\n                for x in [-x0, x0]:\n                    for y in [-y0, y0]:\n                        s = P*Matrix([x, y]) + Q\n                        try:\n                            sol.add(tuple([as_int(_) for _ in s]))\n                        except ValueError:\n                            pass\n        else:\n            # In this case equation can be transformed into a Pell equation\n\n            solns_pell = set(solns_pell)\n            for X, Y in list(solns_pell):\n                solns_pell.add((-X, -Y))\n\n            a = diop_DN(D, 1)\n            T = a[0][0]\n            U = a[0][1]\n\n            if all(_is_int(_) for _ in P[:4] + Q[:2]):\n                for r, s in solns_pell:\n                    _a = (r + s*sqrt(D))*(T + U*sqrt(D))**t\n                    _b = (r - s*sqrt(D))*(T - U*sqrt(D))**t\n                    x_n = _mexpand(S(_a + _b)/2)\n                    y_n = _mexpand(S(_a - _b)/(2*sqrt(D)))\n                    s = P*Matrix([x_n, y_n]) + Q\n                    sol.add(tuple(s))\n\n            else:\n                L = ilcm(*[_.q for _ in P[:4] + Q[:2]])\n\n                k = 1\n\n                T_k = T\n                U_k = U\n\n                while (T_k - 1) % L != 0 or U_k % L != 0:\n                    T_k, U_k = T_k*T + D*U_k*U, T_k*U + U_k*T\n                    k += 1\n\n                for X, Y in solns_pell:\n\n                    for i in range(k):\n                        if all(_is_int(_) for _ in P*Matrix([X, Y]) + Q):\n                            _a = (X + sqrt(D)*Y)*(T_k + sqrt(D)*U_k)**t\n                            _b = (X - sqrt(D)*Y)*(T_k - sqrt(D)*U_k)**t\n                            Xt = S(_a + _b)/2\n                            Yt = S(_a - _b)/(2*sqrt(D))\n                            s = P*Matrix([Xt, Yt]) + Q\n                            sol.add(tuple(s))\n\n                        X, Y = X*T + D*U*Y, X*U + Y*T\n\n    return sol"}
{"input": "def kill(pid, signal):\n    \"\"\"Kill a process with a signal.\"\"\"\n    for chld in reversed(_fetch_child(pid)[1:]):\n        try:\n            os.kill(chld, signal)\n        except OSError as error:\n            message = ('failed to send signal to process %d '\n                       'with error message: %s') % (chld, error)\n            warnings.showwarning(message, ResourceWarning, __file__, 34)", "output": "def kill(pid, signal):\n    \"\"\"Kill a process with a signal.\"\"\"\n    for chld in reversed(_fetch_child(pid)[1:]):\n        try:\n            os.kill(chld, signal)\n        except OSError as error:\n            message = ('failed to send signal to process %d '\n                       'with error message: %s') % (chld, error)\n            warnings.showwarning(message, ResourceWarning, __file__, 33)"}
{"input": "def generate_criteria(self, words):\n        \"\"\"Determine search criteria per word length.\"\"\"\n        counts = {}\n        for word in words:\n            l = len(word)\n            if l in counts:\n                counts[l] += 1\n            else:\n                counts[l] = 0\n        return {\"counts\": counts, \"words\": words}", "output": "def generate_criteria(self, words):\n        \"\"\"Determine search criteria per word length.\"\"\"\n        counts = {}\n        for word in words:\n            l = len(word)\n            if l in counts:\n                counts[l] += 1\n            else:\n                counts[l] = 1\n        return {\"counts\": counts, \"words\": words}"}
{"input": "def __init__(self, project, start=None, starts=None, max_concurrency=None, max_active=None, pickle_paths=None, find=(), avoid=(), restrict=(), min_depth=0, max_depth=None, max_repeats=10, num_find=1, num_avoid=None, num_deviate=1, num_loop=None, cut_lost=None):\n\t\t'''\n\t\tExplores the path space until a block containing a specified address is\n\t\tfound. Parameters (other than for Surveyor):\n\n\t\t@param find: a tuple containing the addresses to search for\n\t\t@param avoid: a tuple containing the addresses to avoid\n\t\t@param restrict: a tuple containing the addresses to restrict the\n\t\t\t\t\t\t analysis to (i.e., avoid all others)\n\t\t@param min_depth: the minimum number of SimRuns in the resulting path\n\t\t@param max_depth: the maximum number of SimRuns in the resulting path\n\n\t\t@param num_find: the minimum number of paths to find (default: 1)\n\t\t@param num_avoid: the minimum number of paths to avoid\n\t\t\t\t\t\t  (default: infinite)\n\t\t@param num_deviate: the minimum number of paths to deviate\n\t\t\t\t\t\t\t(default: infinite)\n\t\t@param num_loop: the minimum number of paths to loop\n\t\t\t\t\t\t (default: infinite)\n\t\t@param cut_lost: cut any paths that have no chance of going to the target\n\t\t'''\n\t\tSurveyor.__init__(self, project, start=start, starts=starts, max_concurrency=max_concurrency, max_active=max_active, pickle_paths=pickle_paths)\n\n\t\t# initialize the counter\n\t\tself._instruction_counter = collections.Counter()\n\n\t\tself._find = self._arg_to_set(find)\n\t\tself._avoid = self._arg_to_set(avoid)\n\t\tself._restrict = self._arg_to_set(restrict)\n\t\tself._max_repeats = max_repeats\n\t\tself._max_depth = max_depth\n\t\tself._min_depth = min_depth\n\n\t\tself.found = [ ]\n\t\tself.avoided = [ ]\n\t\tself.deviating = [ ]\n\t\tself.looping = [ ]\n\t\tself.lost = [ ]\n\n\t\tself._num_find = num_find\n\t\tself._num_avoid = num_avoid\n\t\tself._num_deviate = num_deviate\n\t\tself._num_loop = num_loop\n\n\t\tself._cut_lost = len(self._find) == 0 and self._project._cfg is not None if cut_lost is None else cut_lost\n\n\t\tif self._cut_lost and self._project._cfg is None:\n\t\t\traise AngrSurveyorError(\"cut_lost requires a CFG\")\n\t\tif self._cut_lost:\n\t\t\tgood_find = set()\n\t\t\tfor f in self._find:\n\t\t\t\tif self._project._cfg.get_any_irsb(f) is None:\n\t\t\t\t\tl.warning(\"No node 0x%x in CFG. This will be automatically cut.\", f)\n\t\t\t\telse:\n\t\t\t\t\tgood_find.add(f)\n\t\t\tself._find = good_find", "output": "def __init__(self, project, start=None, starts=None, max_concurrency=None, max_active=None, pickle_paths=None, find=(), avoid=(), restrict=(), min_depth=0, max_depth=None, max_repeats=10000000, num_find=1, num_avoid=None, num_deviate=1, num_loop=None, cut_lost=None):\n\t\t'''\n\t\tExplores the path space until a block containing a specified address is\n\t\tfound. Parameters (other than for Surveyor):\n\n\t\t@param find: a tuple containing the addresses to search for\n\t\t@param avoid: a tuple containing the addresses to avoid\n\t\t@param restrict: a tuple containing the addresses to restrict the\n\t\t\t\t\t\t analysis to (i.e., avoid all others)\n\t\t@param min_depth: the minimum number of SimRuns in the resulting path\n\t\t@param max_depth: the maximum number of SimRuns in the resulting path\n\n\t\t@param num_find: the minimum number of paths to find (default: 1)\n\t\t@param num_avoid: the minimum number of paths to avoid\n\t\t\t\t\t\t  (default: infinite)\n\t\t@param num_deviate: the minimum number of paths to deviate\n\t\t\t\t\t\t\t(default: infinite)\n\t\t@param num_loop: the minimum number of paths to loop\n\t\t\t\t\t\t (default: infinite)\n\t\t@param cut_lost: cut any paths that have no chance of going to the target\n\t\t'''\n\t\tSurveyor.__init__(self, project, start=start, starts=starts, max_concurrency=max_concurrency, max_active=max_active, pickle_paths=pickle_paths)\n\n\t\t# initialize the counter\n\t\tself._instruction_counter = collections.Counter()\n\n\t\tself._find = self._arg_to_set(find)\n\t\tself._avoid = self._arg_to_set(avoid)\n\t\tself._restrict = self._arg_to_set(restrict)\n\t\tself._max_repeats = max_repeats\n\t\tself._max_depth = max_depth\n\t\tself._min_depth = min_depth\n\n\t\tself.found = [ ]\n\t\tself.avoided = [ ]\n\t\tself.deviating = [ ]\n\t\tself.looping = [ ]\n\t\tself.lost = [ ]\n\n\t\tself._num_find = num_find\n\t\tself._num_avoid = num_avoid\n\t\tself._num_deviate = num_deviate\n\t\tself._num_loop = num_loop\n\n\t\tself._cut_lost = len(self._find) == 0 and self._project._cfg is not None if cut_lost is None else cut_lost\n\n\t\tif self._cut_lost and self._project._cfg is None:\n\t\t\traise AngrSurveyorError(\"cut_lost requires a CFG\")\n\t\tif self._cut_lost:\n\t\t\tgood_find = set()\n\t\t\tfor f in self._find:\n\t\t\t\tif self._project._cfg.get_any_irsb(f) is None:\n\t\t\t\t\tl.warning(\"No node 0x%x in CFG. This will be automatically cut.\", f)\n\t\t\t\telse:\n\t\t\t\t\tgood_find.add(f)\n\t\t\tself._find = good_find"}
{"input": "def __init__(self, context_sensitivity_level=2, start=None, avoid_runs=None, enable_function_hints=False, call_depth=None, initial_state=None,\n                 text_base=None, # Temporary\n                 text_size=None # Temporary\n                ):\n        '''\n\n        :param project: The project object.\n        :param context_sensitivity_level: The level of context-sensitivity of this CFG.\n                                        It ranges from 1 to infinity.\n        :return:\n        '''\n        CFGBase.__init__(self, self._p, context_sensitivity_level)\n\n        self._symbolic_function_initial_state = {}\n\n        self._unresolvable_runs = set()\n        self._start = start\n        self._avoid_runs = avoid_runs\n        self._enable_function_hints = enable_function_hints\n        self._call_depth = call_depth\n        self._initial_state = initial_state\n\n        if self._enable_function_hints:\n            # FIXME: As we don't have section info, we have to hardcode where executable sections are.\n            # FIXME: PLEASE MANUALLY MODIFY THE FOLLOWING CHECK BEFORE YOU RUN CFG GENERATION TO YOUR BINARY\n            # FIXME: IF YOU DON'T DO IT, DON'T COMPLAIN TO ME - GO FUCK YOURSELF IN THE CORNER\n            # Once we get the information from cle, these ugly lines can be eventually removed.\n            self.text_base = 0x404ad0 if text_base is None else text_base\n            self.text_size = 0x4fea0 if text_size is None else text_size\n            l.warning('Current section information of the main binary: .text base is 0x%x, size is 0x%x.', self.text_base, self.text_size)\n            l.warning('You do want to modify it manually if you rely on function hints to generate CFG.')\n            l.warning('Otherwise function hints will not work.')\n\n        self.construct()", "output": "def __init__(self, context_sensitivity_level=1, start=None, avoid_runs=None, enable_function_hints=False, call_depth=None, initial_state=None,\n                 text_base=None, # Temporary\n                 text_size=None # Temporary\n                ):\n        '''\n\n        :param project: The project object.\n        :param context_sensitivity_level: The level of context-sensitivity of this CFG.\n                                        It ranges from 1 to infinity.\n        :return:\n        '''\n        CFGBase.__init__(self, self._p, context_sensitivity_level)\n\n        self._symbolic_function_initial_state = {}\n\n        self._unresolvable_runs = set()\n        self._start = start\n        self._avoid_runs = avoid_runs\n        self._enable_function_hints = enable_function_hints\n        self._call_depth = call_depth\n        self._initial_state = initial_state\n\n        if self._enable_function_hints:\n            # FIXME: As we don't have section info, we have to hardcode where executable sections are.\n            # FIXME: PLEASE MANUALLY MODIFY THE FOLLOWING CHECK BEFORE YOU RUN CFG GENERATION TO YOUR BINARY\n            # FIXME: IF YOU DON'T DO IT, DON'T COMPLAIN TO ME - GO FUCK YOURSELF IN THE CORNER\n            # Once we get the information from cle, these ugly lines can be eventually removed.\n            self.text_base = 0x404ad0 if text_base is None else text_base\n            self.text_size = 0x4fea0 if text_size is None else text_size\n            l.warning('Current section information of the main binary: .text base is 0x%x, size is 0x%x.', self.text_base, self.text_size)\n            l.warning('You do want to modify it manually if you rely on function hints to generate CFG.')\n            l.warning('Otherwise function hints will not work.')\n\n        self.construct()"}
{"input": "def _generate_ca(self):\n        # Generate key\n        self.key = OpenSSL.crypto.PKey()\n        self.key.generate_key(OpenSSL.crypto.TYPE_RSA, 2048)\n\n        # Generate certificate\n        self.cert = OpenSSL.crypto.X509()\n        self.cert.set_version(3)\n        # avoid sec_error_reused_issuer_and_serial\n        self.cert.set_serial_number(random.randint(0,2**64-1))\n        self.cert.get_subject().CN = 'Warcprox CA on {}'.format(socket.gethostname())[:64]\n        self.cert.gmtime_adj_notBefore(0)                # now\n        self.cert.gmtime_adj_notAfter(100*365*24*60*60)  # 100 yrs in future\n        self.cert.set_issuer(self.cert.get_subject())\n        self.cert.set_pubkey(self.key)\n        self.cert.add_extensions([\n            OpenSSL.crypto.X509Extension(b\"basicConstraints\", True, b\"CA:TRUE, pathlen:0\"),\n            OpenSSL.crypto.X509Extension(b\"keyUsage\", True, b\"keyCertSign, cRLSign\"),\n            OpenSSL.crypto.X509Extension(b\"subjectKeyIdentifier\", False, b\"hash\", subject=self.cert),\n            ])\n        self.cert.sign(self.key, \"sha1\")\n\n        with open(self.ca_file, 'wb+') as f:\n            f.write(OpenSSL.crypto.dump_privatekey(OpenSSL.SSL.FILETYPE_PEM, self.key))\n            f.write(OpenSSL.crypto.dump_certificate(OpenSSL.SSL.FILETYPE_PEM, self.cert))\n\n        self.logger.info('generated CA key+cert and wrote to {}'.format(self.ca_file))", "output": "def _generate_ca(self):\n        # Generate key\n        self.key = OpenSSL.crypto.PKey()\n        self.key.generate_key(OpenSSL.crypto.TYPE_RSA, 2048)\n\n        # Generate certificate\n        self.cert = OpenSSL.crypto.X509()\n        self.cert.set_version(2)\n        # avoid sec_error_reused_issuer_and_serial\n        self.cert.set_serial_number(random.randint(0,2**64-1))\n        self.cert.get_subject().CN = 'Warcprox CA on {}'.format(socket.gethostname())[:64]\n        self.cert.gmtime_adj_notBefore(0)                # now\n        self.cert.gmtime_adj_notAfter(100*365*24*60*60)  # 100 yrs in future\n        self.cert.set_issuer(self.cert.get_subject())\n        self.cert.set_pubkey(self.key)\n        self.cert.add_extensions([\n            OpenSSL.crypto.X509Extension(b\"basicConstraints\", True, b\"CA:TRUE, pathlen:0\"),\n            OpenSSL.crypto.X509Extension(b\"keyUsage\", True, b\"keyCertSign, cRLSign\"),\n            OpenSSL.crypto.X509Extension(b\"subjectKeyIdentifier\", False, b\"hash\", subject=self.cert),\n            ])\n        self.cert.sign(self.key, \"sha1\")\n\n        with open(self.ca_file, 'wb+') as f:\n            f.write(OpenSSL.crypto.dump_privatekey(OpenSSL.SSL.FILETYPE_PEM, self.key))\n            f.write(OpenSSL.crypto.dump_certificate(OpenSSL.SSL.FILETYPE_PEM, self.cert))\n\n        self.logger.info('generated CA key+cert and wrote to {}'.format(self.ca_file))"}
{"input": "def index(request, page=1): \n    if not request.user.is_authenticated():\n        return redirect('/')\n    return render_to_response('staticbook.html',{'page':int(page)})", "output": "def index(request, page=0): \n    if not request.user.is_authenticated():\n        return redirect('/')\n    return render_to_response('staticbook.html',{'page':int(page)})"}
{"input": "def _make_table(start=-200000, end=100000):\n    G = EcGroup(nid=713)\n    g = G.generator()\n    o = G.order()\n\n    i_table = {}\n    n_table = {}\n    ix = start * g\n\n    for i in range(start, end):\n        i_table[str(ix)] = str(i)\n        n_table[str((o + i) % o)] = str(ix)\n        ix = ix + g\n        #print type(ix)\n        #print type(ix.export())\n\n    return i_table, n_table", "output": "def _make_table(start=-200000, end=2000):\n    G = EcGroup(nid=713)\n    g = G.generator()\n    o = G.order()\n\n    i_table = {}\n    n_table = {}\n    ix = start * g\n\n    for i in range(start, end):\n        i_table[str(ix)] = str(i)\n        n_table[str((o + i) % o)] = str(ix)\n        ix = ix + g\n        #print type(ix)\n        #print type(ix.export())\n\n    return i_table, n_table"}
{"input": "def set_dont_include(self, friends=None):\n        \"\"\"Defines which accounts should not be unfollowed\"\"\"\n        if self.aborting:\n            return self\n\n        self.dont_include = set(friends) or {}\n\n        return self", "output": "def set_dont_include(self, friends=None):\n        \"\"\"Defines which accounts should not be unfollowed\"\"\"\n        if self.aborting:\n            return self\n\n        self.dont_include = set(friends) or set()\n\n        return self"}
{"input": "def __init__(self):\n        super(SpotifySkill, self).__init__()\n        self.index = 0\n        self.spotify = None\n        self.process = None\n        self.device_name = None\n        self.dev_id = None\n        self.idle_count = 0\n        self.ducking = False\n        self.mouth_text = None\n\n        self.__device_list = None\n        self.__devices_fetched = 0\n        self.OAUTH_ID = 1\n        self.DEFAULT_VOLUME = 65\n        self._playlists = None", "output": "def __init__(self):\n        super(SpotifySkill, self).__init__()\n        self.index = 0\n        self.spotify = None\n        self.process = None\n        self.device_name = None\n        self.dev_id = None\n        self.idle_count = 0\n        self.ducking = False\n        self.mouth_text = None\n\n        self.__device_list = None\n        self.__devices_fetched = 0\n        self.OAUTH_ID = 1\n        self.DEFAULT_VOLUME = 80\n        self._playlists = None"}
{"input": "def Graph(data,\n          n_pca=None,\n          sample_idx=None,\n          adaptive_k='sqrt',\n          precomputed=None,\n          knn=5,\n          decay=None,\n          distance='euclidean',\n          thresh=1e-5,\n          n_landmark=None,\n          n_svd=100,\n          beta=1,\n          gamma=0.5,\n          n_jobs=-1,\n          verbose=False,\n          random_state=None,\n          graphtype='auto',\n          **kwargs):\n    \"\"\"Create a graph built on data.\n\n    Automatically selects the appropriate DataGraph subclass based on\n    chosen parameters.\n    Selection criteria:\n    - if `graphtype` is given, this will be respected\n    - otherwise:\n    -- if `sample_idx` is given, an MNNGraph will be created\n    -- if `precomputed` is not given, and either `decay` is `None` or `thresh`\n    is given, a kNNGraph will be created\n    - otherwise, a TraditionalGraph will be created.\n\n    Incompatibilities:\n    - MNNGraph and kNNGraph cannot be precomputed\n    - kNNGraph and TraditionalGraph do not accept sample indices\n\n    Parameters\n    ----------\n    data : array-like, shape=[n_samples,n_features]\n        accepted types: `numpy.ndarray`, `scipy.sparse.spmatrix`.\n        TODO: accept pandas dataframes\n\n    n_pca : `int` or `None`, optional (default: `None`)\n        number of PC dimensions to retain for graph building.\n        If `None`, uses the original data.\n        Note: if data is sparse, uses SVD instead of PCA\n        TODO: should we subtract and store the mean?\n\n    knn : `int`, optional (default: 5)\n        Number of nearest neighbors (including self) to use to build the graph\n\n    decay : `int` or `None`, optional (default: `None`)\n        Rate of alpha decay to use. If `None`, alpha decay is not used.\n\n    distance : `str`, optional (default: `'euclidean'`)\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph.\n        TODO: actually sklearn.neighbors has even more choices\n\n    thresh : `float`, optional (default: `1e-5`)\n        Threshold above which to calculate alpha decay kernel.\n        All affinities below `thresh` will be set to zero in order to save\n        on time and memory constraints.\n\n    precomputed : {'distance', 'affinity', 'adjacency', `None`}, optional (default: `None`)\n        If the graph is precomputed, this variable denotes which graph\n        matrix is provided as `data`.\n        Only one of `precomputed` and `n_pca` can be set.\n\n    beta: float, optional(default: 1)\n        Multiply within - batch connections by(1 - beta)\n\n    gamma: float or {'+', '*'} (default: 0.99)\n        Symmetrization method. If '+', use `(K + K.T) / 2`,\n        if '*', use `K * K.T`, if a float, use\n        `gamma * min(K, K.T) + (1 - gamma) * max(K, K.T)`\n\n    sample_idx: array-like\n        Batch index for MNN kernel\n\n    adaptive_k : `{'min', 'mean', 'sqrt', 'none'}` (default: 'sqrt')\n        Weights MNN kernel adaptively using the number of cells in\n        each sample according to the selected method.\n\n    n_landmark : `int`, optional (default: 2000)\n        number of landmarks to use\n\n    n_svd : `int`, optional (default: 100)\n        number of SVD components to use for spectral clustering\n\n    random_state : `int` or `None`, optional (default: `None`)\n        Random state for random PCA\n\n    verbose : `bool`, optional (default: `True`)\n        Verbosity.\n        TODO: should this be an integer instead to allow multiple\n        levels of verbosity?\n\n    n_jobs : `int`, optional (default : 1)\n        The number of jobs to use for the computation.\n        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n        used at all, which is useful for debugging.\n        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for\n        n_jobs = -2, all CPUs but one are used\n\n    Returns\n    -------\n    G : `DataGraph`\n\n    Raises\n    ------\n    ValueError : if selected parameters are incompatible.\n    \"\"\"\n    if sample_idx is not None and len(np.unique(sample_idx)) == 1:\n        warnings.warn(\"Only one unique sample. \"\n                      \"Not using MNNGraph\")\n        sample_idx = None\n        if graphtype == 'mnn':\n            graphtype = 'auto'\n    if graphtype == 'auto':\n        # automatic graph selection\n        if sample_idx is not None:\n            # only mnn does batch correction\n            graphtype = \"mnn\"\n        elif precomputed is None and (decay is None or thresh > 0):\n            # precomputed requires exact graph\n            # no decay or threshold decay require knngraph\n            graphtype = \"knn\"\n        else:\n            graphtype = \"exact\"\n\n    # set base graph type\n    if graphtype == \"knn\":\n        base = kNNGraph\n        if precomputed is not None:\n            raise ValueError(\"kNNGraph does not support precomputed \"\n                             \"values. Use `graphtype='exact'` or \"\n                             \"`precomputed=None`\")\n        if sample_idx is not None:\n            raise ValueError(\"kNNGraph does not support batch \"\n                             \"correction. Use `graphtype='mnn'` or \"\n                             \"`sample_idx=None`\")\n\n    elif graphtype == \"mnn\":\n        base = MNNGraph\n        if precomputed is not None:\n            raise ValueError(\"MNNGraph does not support precomputed \"\n                             \"values. Use `graphtype='exact'` and \"\n                             \"`sample_idx=None` or `precomputed=None`\")\n    elif graphtype == \"exact\":\n        base = TraditionalGraph\n        if sample_idx is not None:\n            raise ValueError(\"TraditionalGraph does not support batch \"\n                             \"correction. Use `graphtype='mnn'` or \"\n                             \"`sample_idx=None`\")\n    else:\n        raise ValueError(\"graphtype '{}' not recognized. Choose from \"\n                         \"['knn', 'mnn', 'exact', 'auto']\")\n\n    # set add landmarks if necessary\n    if n_landmark is not None:\n        class Graph(base, LandmarkGraph):\n            pass\n    else:\n        class Graph(base):\n            pass\n\n    # build graph and return\n    return Graph(data,\n                 n_pca=n_pca,\n                 sample_idx=sample_idx,\n                 adaptive_k=adaptive_k,\n                 precomputed=precomputed,\n                 knn=knn,\n                 decay=decay,\n                 distance=distance,\n                 thresh=thresh,\n                 n_landmark=n_landmark,\n                 n_svd=n_svd,\n                 beta=beta,\n                 gamma=gamma,\n                 n_jobs=n_jobs,\n                 verbose=verbose,\n                 random_state=random_state,\n                 **kwargs)", "output": "def Graph(data,\n          n_pca=None,\n          sample_idx=None,\n          adaptive_k='sqrt',\n          precomputed=None,\n          knn=5,\n          decay=None,\n          distance='euclidean',\n          thresh=0,\n          n_landmark=None,\n          n_svd=100,\n          beta=1,\n          gamma=0.5,\n          n_jobs=-1,\n          verbose=False,\n          random_state=None,\n          graphtype='auto',\n          **kwargs):\n    \"\"\"Create a graph built on data.\n\n    Automatically selects the appropriate DataGraph subclass based on\n    chosen parameters.\n    Selection criteria:\n    - if `graphtype` is given, this will be respected\n    - otherwise:\n    -- if `sample_idx` is given, an MNNGraph will be created\n    -- if `precomputed` is not given, and either `decay` is `None` or `thresh`\n    is given, a kNNGraph will be created\n    - otherwise, a TraditionalGraph will be created.\n\n    Incompatibilities:\n    - MNNGraph and kNNGraph cannot be precomputed\n    - kNNGraph and TraditionalGraph do not accept sample indices\n\n    Parameters\n    ----------\n    data : array-like, shape=[n_samples,n_features]\n        accepted types: `numpy.ndarray`, `scipy.sparse.spmatrix`.\n        TODO: accept pandas dataframes\n\n    n_pca : `int` or `None`, optional (default: `None`)\n        number of PC dimensions to retain for graph building.\n        If `None`, uses the original data.\n        Note: if data is sparse, uses SVD instead of PCA\n        TODO: should we subtract and store the mean?\n\n    knn : `int`, optional (default: 5)\n        Number of nearest neighbors (including self) to use to build the graph\n\n    decay : `int` or `None`, optional (default: `None`)\n        Rate of alpha decay to use. If `None`, alpha decay is not used.\n\n    distance : `str`, optional (default: `'euclidean'`)\n        Any metric from `scipy.spatial.distance` can be used\n        distance metric for building kNN graph.\n        TODO: actually sklearn.neighbors has even more choices\n\n    thresh : `float`, optional (default: `1e-5`)\n        Threshold above which to calculate alpha decay kernel.\n        All affinities below `thresh` will be set to zero in order to save\n        on time and memory constraints.\n\n    precomputed : {'distance', 'affinity', 'adjacency', `None`}, optional (default: `None`)\n        If the graph is precomputed, this variable denotes which graph\n        matrix is provided as `data`.\n        Only one of `precomputed` and `n_pca` can be set.\n\n    beta: float, optional(default: 1)\n        Multiply within - batch connections by(1 - beta)\n\n    gamma: float or {'+', '*'} (default: 0.99)\n        Symmetrization method. If '+', use `(K + K.T) / 2`,\n        if '*', use `K * K.T`, if a float, use\n        `gamma * min(K, K.T) + (1 - gamma) * max(K, K.T)`\n\n    sample_idx: array-like\n        Batch index for MNN kernel\n\n    adaptive_k : `{'min', 'mean', 'sqrt', 'none'}` (default: 'sqrt')\n        Weights MNN kernel adaptively using the number of cells in\n        each sample according to the selected method.\n\n    n_landmark : `int`, optional (default: 2000)\n        number of landmarks to use\n\n    n_svd : `int`, optional (default: 100)\n        number of SVD components to use for spectral clustering\n\n    random_state : `int` or `None`, optional (default: `None`)\n        Random state for random PCA\n\n    verbose : `bool`, optional (default: `True`)\n        Verbosity.\n        TODO: should this be an integer instead to allow multiple\n        levels of verbosity?\n\n    n_jobs : `int`, optional (default : 1)\n        The number of jobs to use for the computation.\n        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n        used at all, which is useful for debugging.\n        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for\n        n_jobs = -2, all CPUs but one are used\n\n    Returns\n    -------\n    G : `DataGraph`\n\n    Raises\n    ------\n    ValueError : if selected parameters are incompatible.\n    \"\"\"\n    if sample_idx is not None and len(np.unique(sample_idx)) == 1:\n        warnings.warn(\"Only one unique sample. \"\n                      \"Not using MNNGraph\")\n        sample_idx = None\n        if graphtype == 'mnn':\n            graphtype = 'auto'\n    if graphtype == 'auto':\n        # automatic graph selection\n        if sample_idx is not None:\n            # only mnn does batch correction\n            graphtype = \"mnn\"\n        elif precomputed is None and (decay is None or thresh > 0):\n            # precomputed requires exact graph\n            # no decay or threshold decay require knngraph\n            graphtype = \"knn\"\n        else:\n            graphtype = \"exact\"\n\n    # set base graph type\n    if graphtype == \"knn\":\n        base = kNNGraph\n        if precomputed is not None:\n            raise ValueError(\"kNNGraph does not support precomputed \"\n                             \"values. Use `graphtype='exact'` or \"\n                             \"`precomputed=None`\")\n        if sample_idx is not None:\n            raise ValueError(\"kNNGraph does not support batch \"\n                             \"correction. Use `graphtype='mnn'` or \"\n                             \"`sample_idx=None`\")\n\n    elif graphtype == \"mnn\":\n        base = MNNGraph\n        if precomputed is not None:\n            raise ValueError(\"MNNGraph does not support precomputed \"\n                             \"values. Use `graphtype='exact'` and \"\n                             \"`sample_idx=None` or `precomputed=None`\")\n    elif graphtype == \"exact\":\n        base = TraditionalGraph\n        if sample_idx is not None:\n            raise ValueError(\"TraditionalGraph does not support batch \"\n                             \"correction. Use `graphtype='mnn'` or \"\n                             \"`sample_idx=None`\")\n    else:\n        raise ValueError(\"graphtype '{}' not recognized. Choose from \"\n                         \"['knn', 'mnn', 'exact', 'auto']\")\n\n    # set add landmarks if necessary\n    if n_landmark is not None:\n        class Graph(base, LandmarkGraph):\n            pass\n    else:\n        class Graph(base):\n            pass\n\n    # build graph and return\n    return Graph(data,\n                 n_pca=n_pca,\n                 sample_idx=sample_idx,\n                 adaptive_k=adaptive_k,\n                 precomputed=precomputed,\n                 knn=knn,\n                 decay=decay,\n                 distance=distance,\n                 thresh=thresh,\n                 n_landmark=n_landmark,\n                 n_svd=n_svd,\n                 beta=beta,\n                 gamma=gamma,\n                 n_jobs=n_jobs,\n                 verbose=verbose,\n                 random_state=random_state,\n                 **kwargs)"}
{"input": "def as_docker_auths(self):\n        '''\n        returns a representation of the credentials from this registry-cfg as \"docker-auths\",\n        which can be used to populate a docker-cfg file ($HOME/.docker/config.json) below the\n        `auths` attr\n        '''\n        auth_str = f'{self.credentials().username()}:{self.credentials().passwd()}'\n        auth_str = base64.b64encode(auth_str.encode('utf-8'))\n\n        auths = {\n            host: {'auth': auth_str} for host in self.image_reference_prefixes()\n        }\n\n        return auths", "output": "def as_docker_auths(self):\n        '''\n        returns a representation of the credentials from this registry-cfg as \"docker-auths\",\n        which can be used to populate a docker-cfg file ($HOME/.docker/config.json) below the\n        `auths` attr\n        '''\n        auth_str = f'{self.credentials().username()}:{self.credentials().passwd()}'\n        auth_str = base64.b64encode(auth_str.encode('utf-8')).decode('utf-8')\n\n        auths = {\n            host: {'auth': auth_str} for host in self.image_reference_prefixes()\n        }\n\n        return auths"}
{"input": "def __setitem__(self, name, value):\n        if name in self.keys:\n            if not PY3K and isinstance(value, unicode):\n                value = value.encode(\"latin1\",\"ignore\")\n            elif value is None:\n                value = \"\"\n            else:\n                value = str(value)\n            self.texts[self.pg_no][name.lower()] = value", "output": "def __setitem__(self, name, value):\n        if name.lower() in self.keys:\n            if not PY3K and isinstance(value, unicode):\n                value = value.encode(\"latin1\",\"ignore\")\n            elif value is None:\n                value = \"\"\n            else:\n                value = str(value)\n            self.texts[self.pg_no][name.lower()] = value"}
{"input": "def find_title(url):\n    \"\"\"Return the title for the given URL.\"\"\"\n    content = web.get(url)\n    # Some cleanup that I don't really grok, but was in the original, so\n    # we'll keep it (with the compiled regexes made global) for now.\n    content = title_tag_data.sub(r'<\\1title>', content)\n    content = quoted_title.sub('', content)\n\n    start = content.find('<title>')\n    end = content.find('</title>')\n    if start == -1 or end == -1:\n        return\n    title = content[start + 7:end]\n    title = title.strip()[:200]\n\n    def get_unicode_entity(match):\n        entity = match.group()\n        if entity.startswith('&#x'):\n            cp = int(entity[3:-1], 16)\n        elif entity.startswith('&#'):\n            cp = int(entity[2:-1])\n        else:\n            cp = name2codepoint[entity[1:-1]]\n        return unichr(cp)\n\n    title = r_entity.sub(get_unicode_entity, title)\n\n    title = ' '.join(title.split())  # cleanly remove multiple spaces\n\n    # More cryptic regex substitutions. This one looks to be myano's invention.\n    title = re_dcc.sub('', title)\n\n    return title or None", "output": "def find_title(url):\n    \"\"\"Return the title for the given URL.\"\"\"\n    content = web.get(url).decode('utf8')\n    # Some cleanup that I don't really grok, but was in the original, so\n    # we'll keep it (with the compiled regexes made global) for now.\n    content = title_tag_data.sub(r'<\\1title>', content)\n    content = quoted_title.sub('', content)\n\n    start = content.find('<title>')\n    end = content.find('</title>')\n    if start == -1 or end == -1:\n        return\n    title = content[start + 7:end]\n    title = title.strip()[:200]\n\n    def get_unicode_entity(match):\n        entity = match.group()\n        if entity.startswith('&#x'):\n            cp = int(entity[3:-1], 16)\n        elif entity.startswith('&#'):\n            cp = int(entity[2:-1])\n        else:\n            cp = name2codepoint[entity[1:-1]]\n        return unichr(cp)\n\n    title = r_entity.sub(get_unicode_entity, title)\n\n    title = ' '.join(title.split())  # cleanly remove multiple spaces\n\n    # More cryptic regex substitutions. This one looks to be myano's invention.\n    title = re_dcc.sub('', title)\n\n    return title or None"}
{"input": "def align_against(self, ref_col, other_cols=None):\n        \"\"\"\n        Determine what columns can be sliced against another column.\n\n        This matters because not all columns can be filled and would\n        result in getting back non-dense events.\n\n        Currently this just decides based on if the column can be filled,\n        but this might need to be made smarter to deal with synchronous\n        collection of multiple un-fillable measurements.\n\n        Parameters\n        ----------\n        ref_col : str\n            The name of the proposed reference column\n        other_cols : list\n            The names of the columns to test for alignment\n\n        Returns\n        -------\n        dict\n            Keyed on column name, True if that column can be sliced at\n            the times of the input column.\n        \"\"\"\n        if ref_col not in self._dataframe:\n            raise ValueError(\"non-existent columnn: [[{}]]\".format(ref_col))\n        ref_index = self._dataframe[ref_col].dropna().index\n        tmp_dict = {}\n        for col_name, col_fill_type in six.iteritems(self._col_fill):\n            if col_name == ref_col:\n                tmp_dict[col_name] = True\n            elif other_cols and not col_name in other_cols:\n                # skip column names that are not in other_cols, if it passed in\n                continue\n            elif col_fill_type is None:\n                tmp_dict[col_name] = False\n            else:\n                algnable = self._dataframe[col_name][ref_index].notnull().all()\n                tmp_dict[col_name] = bool(algnable)\n        return tmp_dict", "output": "def align_against(self, ref_col, other_cols=None):\n        \"\"\"\n        Determine what columns can be sliced against another column.\n\n        This matters because not all columns can be filled and would\n        result in getting back non-dense events.\n\n        Currently this just decides based on if the column can be filled,\n        but this might need to be made smarter to deal with synchronous\n        collection of multiple un-fillable measurements.\n\n        Parameters\n        ----------\n        ref_col : str\n            The name of the proposed reference column\n        other_cols : list\n            The names of the columns to test for alignment\n\n        Returns\n        -------\n        dict\n            Keyed on column name, True if that column can be sliced at\n            the times of the input column.\n        \"\"\"\n        if ref_col not in self._dataframe:\n            raise ValueError(\"non-existent columnn: [[{}]]\".format(ref_col))\n        ref_index = self._dataframe[ref_col].dropna().index\n        tmp_dict = {}\n        for col_name, col_fill_type in six.iteritems(self._col_fill):\n            if col_name == ref_col:\n                tmp_dict[col_name] = True\n            elif other_cols and not col_name in other_cols:\n                # skip column names that are not in other_cols, if it passed in\n                continue\n            elif col_fill_type is None:\n                tmp_dict[col_name] = False\n            else:\n                algnable = self._dataframe[col_name].fillna(col_fill_type)[ref_index].notnull().all()\n                tmp_dict[col_name] = bool(algnable)\n        return tmp_dict"}
{"input": "def _build_context(self):\n        ctx = self._context.copy()\n\n        versions = ctx[\"versions\"]\n        for name, mod in sys.modules.items():\n            if name.startswith(\"_\"):\n                continue\n            if hasattr(mod, \"__version__\"):\n                versions[name] = mod.__version__\n\n        return ctx", "output": "def _build_context(self):\n        ctx = self._context.copy()\n\n        versions = ctx[\"versions\"]\n        for name, mod in sys.modules.copy().items():\n            if name.startswith(\"_\"):\n                continue\n            if hasattr(mod, \"__version__\"):\n                versions[name] = mod.__version__\n\n        return ctx"}
{"input": "def activeExtruderStackId(self):\n        if not UM.Application.getInstance().getGlobalContainerStack():\n            return None #No active machine, so no active extruder.\n        try:\n            return self._extruder_trains[UM.Application.getInstance().getGlobalContainerStack().getId()][str(self._active_extruder_index)]\n        except KeyError: #Extruder index could be -1 if the global tab is selected, or the entry doesn't exist if the machine definition is wrong.\n            return None", "output": "def activeExtruderStackId(self):\n        if not UM.Application.getInstance().getGlobalContainerStack():\n            return None #No active machine, so no active extruder.\n        try:\n            return self._extruder_trains[UM.Application.getInstance().getGlobalContainerStack().getBottom().getId()][str(self._active_extruder_index)]\n        except KeyError: #Extruder index could be -1 if the global tab is selected, or the entry doesn't exist if the machine definition is wrong.\n            return None"}
{"input": "def render(self, retries: int = 8) -> None:\n        \"\"\"Reloads the response in Chromium, and replaces HTML content\n        with an updated version, with JavaScript executed.\n\n        Warning: the first time you run this method, it will download\n        Chromium into your home directory (``~/.pyppeteer``).\n        \"\"\"\n        async def _async_render(url: str):\n            try:\n                browser = pyppeteer.launch(headless=True)\n                page = await browser.newPage()\n\n                # Load the given page (GET request, obviously.)\n                await page.goto(url)\n\n                # Return the content of the page, JavaScript evaluated.\n                return await page.content()\n            except TimeoutError:\n                return None\n\n        loop = asyncio.get_event_loop()\n        content = None\n\n        for i in range(retries):\n            if not content:\n                try:\n                    content = loop.run_until_complete(_async_render(url=self.url))\n                except TimeoutError:\n                    pass\n\n        html = HTML(url=self.url, html=content, default_encoding=DEFAULT_ENCODING)\n        self.__dict__.update(html.__dict__)", "output": "def render(self, retries: int = 8) -> None:\n        \"\"\"Reloads the response in Chromium, and replaces HTML content\n        with an updated version, with JavaScript executed.\n\n        Warning: the first time you run this method, it will download\n        Chromium into your home directory (``~/.pyppeteer``).\n        \"\"\"\n        async def _async_render(url: str):\n            try:\n                browser = pyppeteer.launch(headless=True)\n                page = await browser.newPage()\n\n                # Load the given page (GET request, obviously.)\n                await page.goto(url)\n\n                # Return the content of the page, JavaScript evaluated.\n                return await page.content()\n            except TimeoutError:\n                return None\n\n        loop = asyncio.get_event_loop()\n        content = None\n\n        for i in range(retries):\n            if not content:\n                try:\n                    content = loop.run_until_complete(_async_render(url=self.url))\n                except TimeoutError:\n                    pass\n\n        html = HTML(url=self.url, html=content.encode(DEFAULT_ENCODING), default_encoding=DEFAULT_ENCODING)\n        self.__dict__.update(html.__dict__)"}
{"input": "def format(self, record):\n        try:\n            try:\n                host_name = settings.local.host.name\n            except AttributeError:\n                host_name = 'undefined'\n\n            formatted_record = '[' + host_name + ']' + \\\n                logging.Formatter.format(self, record)\n\n            if hasattr(record, 'data') and record.data:\n                traceback = record.data.pop('traceback', None)\n                stdout = record.data.pop('stdout', None)\n                stderr = record.data.pop('stderr', None)\n\n                if record.data:\n                    width = len(max(record.data, key=len))\n                    for key, val in record.data.items():\n                        formatted_record += '\\n  %s = %r' % (\n                            key.ljust(width), val)\n                    if stdout:\n                        formatted_record += '\\nProcess stdout:'\n                        stdout_lines = stdout.split('\\n')\n                        if stdout_lines and not stdout_lines[-1]:\n                            stdout_lines.pop()\n                        for line in stdout_lines:\n                            formatted_record += '\\n  ' + line\n                    if stderr:\n                        formatted_record += '\\nProcess stderr:'\n                        stderr_lines = stderr.split('\\n')\n                        if stderr_lines and not stderr_lines[-1]:\n                            stderr_lines.pop()\n                        for line in stderr_lines:\n                            formatted_record += '\\n  ' + line\n                    if traceback:\n                        formatted_record += \\\n                            '\\nTraceback (most recent call last):\\n'\n                        formatted_record += ''.join(traceback).rstrip('\\n')\n        except:\n            from pritunl import logger\n            logger.exception('Log format error')\n\n        return formatted_record", "output": "def format(self, record):\n        try:\n            try:\n                host_name = settings.local.host.name\n            except AttributeError:\n                host_name = 'undefined'\n\n            formatted_record = '[' + host_name + ']' + \\\n                logging.Formatter.format(self, record)\n\n            if hasattr(record, 'data') and record.data:\n                traceback = record.data.pop('traceback', None)\n                stdout = record.data.pop('stdout', None)\n                stderr = record.data.pop('stderr', None)\n\n                if record.data:\n                    width = len(max(record.data, key=len))\n                    for key, val in record.data.items():\n                        formatted_record += '\\n  %s = %r' % (\n                            key.ljust(width), val)\n                    if stdout:\n                        formatted_record += '\\nProcess stdout:'\n                        stdout_lines = stdout.split('\\n')\n                        if stdout_lines and not stdout_lines[-1]:\n                            stdout_lines.pop()\n                        for line in stdout_lines:\n                            formatted_record += '\\n  ' + line\n                    if stderr:\n                        formatted_record += '\\nProcess stderr:'\n                        stderr_lines = stderr.split('\\n')\n                        if stderr_lines and not stderr_lines[-1]:\n                            stderr_lines.pop()\n                        for line in stderr_lines:\n                            formatted_record += '\\n  ' + line.decode('utf-8')\n                    if traceback:\n                        formatted_record += \\\n                            '\\nTraceback (most recent call last):\\n'\n                        formatted_record += ''.join(traceback).rstrip('\\n')\n        except:\n            from pritunl import logger\n            logger.exception('Log format error')\n\n        return formatted_record"}
{"input": "def run(self):\n        if 'alt' in self.options and self.ignore_alt:\n            LOGGER.warning(\"Graphviz: the :alt: option is ignored, it's better to set the title of your graph.\")\n        if self.arguments:\n            if self.content:\n                LOGGER.warning(\"Graphviz: this directive can't have both content and a filename argument. Ignoring content.\")\n            f_name = self.arguments[0]\n            # TODO: be smart about where exactly that file is located\n            with open(f_name, 'rb') as inf:\n                data = inf.read().decode('utf-8')\n        else:\n            data = '\\n'.join(self.content)\n        node_list = []\n        try:\n            p = Popen([self.dot_path, '-Tsvg'], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            svg_data, errors = p.communicate(input=data)\n            code = p.wait()\n            if code:  # Some error\n                document = self.state.document\n                return [document.reporter.error(\n                        'Error processing graph: {0}'.format(errors), line=self.lineno)]\n            if self.embed_graph:  # SVG embedded in the HTML\n                if 'inline' in self.options:\n                    svg_data = '<span class=\"graphviz\">{0}</span>'.format(svg_data)\n                else:\n                    svg_data = '<p class=\"graphviz\">{0}</p>'.format(svg_data)\n\n            else:  # External SVG file\n                # TODO: there is no reason why this branch needs to be a raw\n                # directive. It could generate regular docutils nodes and\n                # be useful for any writer.\n                makedirs(self.output_folder)\n                f_name = hashlib.md5(svg_data).hexdigest() + '.svg'\n                img_path = self.graph_path + f_name\n                f_path = os.path.join(self.output_folder, f_name)\n                alt = self.options.get('alt', '')\n                with open(f_path, 'wb+') as outf:\n                    outf.write(svg_data)\n                    self.state.document.settings.record_dependencies.add(f_path)\n                if 'inline' in self.options:\n                    svg_data = '<span class=\"graphviz\"><img src=\"{0}\" alt=\"{1}\"></span>'.format(img_path, alt)\n                else:\n                    svg_data = '<p class=\"graphviz\"><img src=\"{0}\" alt=\"{1}\"></p>'.format(img_path, alt)\n\n            node_list.append(nodes.raw('', svg_data, format='html'))\n            if 'caption' in self.options and 'inline' not in self.options:\n                node_list.append(\n                    nodes.raw('', '<p class=\"caption\">{0}</p>'.format(self.options['caption']),\n                              format='html'))\n            return node_list\n        except OSError:\n            LOGGER.error(\"Can't execute 'dot'\")\n            raise", "output": "def run(self):\n        if 'alt' in self.options and self.ignore_alt:\n            LOGGER.warning(\"Graphviz: the :alt: option is ignored, it's better to set the title of your graph.\")\n        if self.arguments:\n            if self.content:\n                LOGGER.warning(\"Graphviz: this directive can't have both content and a filename argument. Ignoring content.\")\n            f_name = self.arguments[0]\n            # TODO: be smart about where exactly that file is located\n            with open(f_name, 'rb') as inf:\n                data = inf.read().decode('utf-8')\n        else:\n            data = '\\n'.join(self.content)\n        node_list = []\n        try:\n            p = Popen([self.dot_path, '-Tsvg'], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            svg_data, errors = p.communicate(input=data.encode('utf8'))\n            code = p.wait()\n            if code:  # Some error\n                document = self.state.document\n                return [document.reporter.error(\n                        'Error processing graph: {0}'.format(errors), line=self.lineno)]\n            if self.embed_graph:  # SVG embedded in the HTML\n                if 'inline' in self.options:\n                    svg_data = '<span class=\"graphviz\">{0}</span>'.format(svg_data)\n                else:\n                    svg_data = '<p class=\"graphviz\">{0}</p>'.format(svg_data)\n\n            else:  # External SVG file\n                # TODO: there is no reason why this branch needs to be a raw\n                # directive. It could generate regular docutils nodes and\n                # be useful for any writer.\n                makedirs(self.output_folder)\n                f_name = hashlib.md5(svg_data).hexdigest() + '.svg'\n                img_path = self.graph_path + f_name\n                f_path = os.path.join(self.output_folder, f_name)\n                alt = self.options.get('alt', '')\n                with open(f_path, 'wb+') as outf:\n                    outf.write(svg_data)\n                    self.state.document.settings.record_dependencies.add(f_path)\n                if 'inline' in self.options:\n                    svg_data = '<span class=\"graphviz\"><img src=\"{0}\" alt=\"{1}\"></span>'.format(img_path, alt)\n                else:\n                    svg_data = '<p class=\"graphviz\"><img src=\"{0}\" alt=\"{1}\"></p>'.format(img_path, alt)\n\n            node_list.append(nodes.raw('', svg_data, format='html'))\n            if 'caption' in self.options and 'inline' not in self.options:\n                node_list.append(\n                    nodes.raw('', '<p class=\"caption\">{0}</p>'.format(self.options['caption']),\n                              format='html'))\n            return node_list\n        except OSError:\n            LOGGER.error(\"Can't execute 'dot'\")\n            raise"}
{"input": "def __init__(\n            self,\n            env,\n            policy,\n            baseline,\n            n_itr=500,\n            start_itr=0,\n            batch_size=5000,\n            max_path_length=500,\n            discount=0.99,\n            gae_lambda=1,\n            plot=False,\n            pause_for_plot=False,\n            center_adv=True,\n            positive_adv=False,\n            store_paths=False,\n            whole_paths=False,\n            **kwargs\n    ):\n        \"\"\"\n        :param env: Environment\n        :param policy: Policy\n        :param baseline: Baseline\n        :param n_itr: Number of iterations.\n        :param start_itr: Starting iteration.\n        :param batch_size: Number of samples per iteration.\n        :param max_path_length: Maximum length of a single rollout.\n        :param discount: Discount.\n        :param gae_lambda: Lambda used for generalized advantage estimation.\n        :param plot: Plot evaluation run after each iteration.\n        :param pause_for_plot: Whether to pause before contiuing when plotting.\n        :param center_adv: Whether to rescale the advantages so that they have mean 0 and standard deviation 1.\n        :param positive_adv: Whether to shift the advantages so that they are always positive. When used in\n        conjunction with center_adv the advantages will be standardized before shifting.\n        :param store_paths: Whether to save all paths data to the snapshot.\n        :return:\n        \"\"\"\n        self.env = env\n        self.policy = policy\n        self.baseline = baseline\n        self.n_itr = n_itr\n        self.start_itr = start_itr\n        self.batch_size = batch_size\n        self.max_path_length = max_path_length\n        self.discount = discount\n        self.gae_lambda = gae_lambda\n        self.plot = plot\n        self.pause_for_plot = pause_for_plot\n        self.center_adv = center_adv\n        self.positive_adv = positive_adv\n        self.store_paths = store_paths\n        self.whole_paths = whole_paths", "output": "def __init__(\n            self,\n            env,\n            policy,\n            baseline,\n            n_itr=500,\n            start_itr=0,\n            batch_size=5000,\n            max_path_length=500,\n            discount=0.99,\n            gae_lambda=1,\n            plot=False,\n            pause_for_plot=False,\n            center_adv=True,\n            positive_adv=False,\n            store_paths=False,\n            whole_paths=True,\n            **kwargs\n    ):\n        \"\"\"\n        :param env: Environment\n        :param policy: Policy\n        :param baseline: Baseline\n        :param n_itr: Number of iterations.\n        :param start_itr: Starting iteration.\n        :param batch_size: Number of samples per iteration.\n        :param max_path_length: Maximum length of a single rollout.\n        :param discount: Discount.\n        :param gae_lambda: Lambda used for generalized advantage estimation.\n        :param plot: Plot evaluation run after each iteration.\n        :param pause_for_plot: Whether to pause before contiuing when plotting.\n        :param center_adv: Whether to rescale the advantages so that they have mean 0 and standard deviation 1.\n        :param positive_adv: Whether to shift the advantages so that they are always positive. When used in\n        conjunction with center_adv the advantages will be standardized before shifting.\n        :param store_paths: Whether to save all paths data to the snapshot.\n        :return:\n        \"\"\"\n        self.env = env\n        self.policy = policy\n        self.baseline = baseline\n        self.n_itr = n_itr\n        self.start_itr = start_itr\n        self.batch_size = batch_size\n        self.max_path_length = max_path_length\n        self.discount = discount\n        self.gae_lambda = gae_lambda\n        self.plot = plot\n        self.pause_for_plot = pause_for_plot\n        self.center_adv = center_adv\n        self.positive_adv = positive_adv\n        self.store_paths = store_paths\n        self.whole_paths = whole_paths"}
{"input": "def __init__(self, filename, mode='r', force_overwrite=True, compression='zlib'):\n        self._open = False  # is the file handle currently open?\n        self.mode = mode  # the mode in which the file was opened?\n\n        if not mode in ['r', 'w', 'a']:\n            raise ValueError(\"mode must be one of ['r', 'w', 'a']\")\n\n        if mode == 'w' and not force_overwrite and os.path.exists(filename):\n            raise IOError('\"%s\" already exists' % filename)\n\n        # import tables\n        self.tables = import_('tables')\n\n        if compression == 'zlib':\n            compression = self.tables.Filters(complib='zlib', shuffle=True, complevel=1)\n        elif compression is None:\n            compression = None\n        else:\n            raise ValueError('compression must be either \"zlib\" or None')\n\n        self._handle = self._open_file(filename, mode=mode, filters=compression)\n        self._open = True\n\n        if mode == 'w':\n            # what frame are we currently reading or writing at?\n            self._frame_index = 0\n            # do we need to write the header information?\n            self._needs_initialization = True\n            if not filename.endswith('.h5'):\n                warnings.warn('The .h5 extension is recommended.')\n\n        elif mode == 'a':\n            try:\n                self._frame_index = len(self._handle.root.coordinates)\n                self._needs_initialization = False\n            except self.tables.NoSuchNodeError:\n                self._frame_index = 0\n                self._needs_initialization = False\n        elif mode == 'r':\n            self._frame_index = 0\n            self._needs_initialization = False", "output": "def __init__(self, filename, mode='r', force_overwrite=True, compression='zlib'):\n        self._open = False  # is the file handle currently open?\n        self.mode = mode  # the mode in which the file was opened?\n\n        if not mode in ['r', 'w', 'a']:\n            raise ValueError(\"mode must be one of ['r', 'w', 'a']\")\n\n        if mode == 'w' and not force_overwrite and os.path.exists(filename):\n            raise IOError('\"%s\" already exists' % filename)\n\n        # import tables\n        self.tables = import_('tables')\n\n        if compression == 'zlib':\n            compression = self.tables.Filters(complib='zlib', shuffle=True, complevel=1)\n        elif compression is None:\n            compression = None\n        else:\n            raise ValueError('compression must be either \"zlib\" or None')\n\n        self._handle = self._open_file(filename, mode=mode, filters=compression)\n        self._open = True\n\n        if mode == 'w':\n            # what frame are we currently reading or writing at?\n            self._frame_index = 0\n            # do we need to write the header information?\n            self._needs_initialization = True\n            if not filename.endswith('.h5'):\n                warnings.warn('The .h5 extension is recommended.')\n\n        elif mode == 'a':\n            try:\n                self._frame_index = len(self._handle.root.coordinates)\n                self._needs_initialization = False\n            except self.tables.NoSuchNodeError:\n                self._frame_index = 0\n                self._needs_initialization = True\n        elif mode == 'r':\n            self._frame_index = 0\n            self._needs_initialization = False"}
{"input": "def main():\n    if sc.rtm_connect():\n        status = 'idle'\n        job_queue = []\n        process = None\n        logfile = None\n        enable_build_on_push = False\n        current_job = CurrentJob('')\n        e_tag = ''  # to avoid getting unchanged data back from github\n        e_tag = poll_github(job_queue, e_tag, initial_call=True)\n        # Poll slack once every 2 seconds and github once every 30 seconds\n        while True:\n            if status == 'idle' and len(job_queue) > 0:\n                process, logfile, status, current_job = start_build_commit(job_queue)\n            else:\n                process, logfile, status = poll_for_process_end(process, logfile, status, current_job)\n            for _ in range(15):\n                time.sleep(2)\n                enable_build_on_push = poll_slack(job_queue, enable_build_on_push)\n            if enable_build_on_push:\n                e_tag = poll_github(job_queue, e_tag)\n    else:\n        print('Failed to connect to Slack bot, check token and bot ID')", "output": "def main():\n    if sc.rtm_connect():\n        status = 'idle'\n        job_queue = []\n        process = None\n        logfile = None\n        enable_build_on_push = True\n        current_job = CurrentJob('')\n        e_tag = ''  # to avoid getting unchanged data back from github\n        e_tag = poll_github(job_queue, e_tag, initial_call=True)\n        # Poll slack once every 2 seconds and github once every 30 seconds\n        while True:\n            if status == 'idle' and len(job_queue) > 0:\n                process, logfile, status, current_job = start_build_commit(job_queue)\n            else:\n                process, logfile, status = poll_for_process_end(process, logfile, status, current_job)\n            for _ in range(15):\n                time.sleep(2)\n                enable_build_on_push = poll_slack(job_queue, enable_build_on_push)\n            if enable_build_on_push:\n                e_tag = poll_github(job_queue, e_tag)\n    else:\n        print('Failed to connect to Slack bot, check token and bot ID')"}
{"input": "def get_or_create_user(self, username, ldap_user):\n        # type: (str, _LDAPUser) -> Tuple[UserProfile, bool]\n        try:\n            user_profile = get_user_profile_by_email(username)\n            if not user_profile.is_active or user_profile.realm.deactivated:\n                raise ZulipLDAPException(\"Realm has been deactivated\")\n            return user_profile, False\n        except UserProfile.DoesNotExist:\n            domain = resolve_email_to_domain(username)\n            realm = get_realm(domain)\n            # No need to check for an inactive user since they don't exist yet\n            if realm.deactivated:\n                raise ZulipLDAPException(\"Realm has been deactivated\")\n\n            full_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"full_name\"]\n            short_name = full_name = ldap_user.attrs[full_name_attr][0]\n            if \"short_name\" in settings.AUTH_LDAP_USER_ATTR_MAP:\n                short_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"short_name\"]\n                short_name = ldap_user.attrs[short_name_attr][0]\n\n            user_profile = do_create_user(username, None, realm, full_name, short_name)\n            return user_profile, False", "output": "def get_or_create_user(self, username, ldap_user):\n        # type: (str, _LDAPUser) -> Tuple[UserProfile, bool]\n        try:\n            user_profile = get_user_profile_by_email(username)\n            if not user_profile.is_active or user_profile.realm.deactivated:\n                raise ZulipLDAPException(\"Realm has been deactivated\")\n            return user_profile, False\n        except UserProfile.DoesNotExist:\n            domain = resolve_email_to_domain(username)\n            realm = get_realm(domain)\n            # No need to check for an inactive user since they don't exist yet\n            if realm.deactivated:\n                raise ZulipLDAPException(\"Realm has been deactivated\")\n\n            full_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"full_name\"]\n            short_name = full_name = ldap_user.attrs[full_name_attr][0]\n            if \"short_name\" in settings.AUTH_LDAP_USER_ATTR_MAP:\n                short_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"short_name\"]\n                short_name = ldap_user.attrs[short_name_attr][0]\n\n            user_profile = do_create_user(username, None, realm, full_name, short_name)\n            return user_profile, True"}
{"input": "def main():\n    parser = argparse.ArgumentParser(\n        \"simulator\", description=\"Simulation and result plotting\")\n\n    parser.register('type', 'bool', str2bool)  # add type keyword to registries\n\n    parser.add_argument('action', choices=['simulate', 'testAI', 'testDataset', 'plot', 'train', 'create_dataset'],\n                        default=\"simulate\",\n                        help='Action requested')\n    parser.add_argument('source', type=str,\n                        default=\"./results_8w_with_sizes_csv\",\n                        help='The folder where the json results are stored [DEFAULT: \"./results_8w_with_sizes_csv\"]')\n    parser.add_argument('--cache-types', type=str,\n                        default=\"lru,weightedLRU\",\n                        help='Comma separated list of cache to simulate [DEFAULT: \"lru,weightedLRU\"]')\n    parser.add_argument('--out-folder', type=str,\n                        default=\"./simulation_results\",\n                        help='The folder where the simulation results will be stored [DEFAULT: \"simulation_results\"]')\n    parser.add_argument('--read-on-hit', type='bool',\n                        default=True,\n                        help='Use read on hit data [DEFAULT: True]')\n    parser.add_argument('--simulation-steps', type=str,\n                        default='single,normal,nextW,nextP',\n                        help='Select the simulation steps [DEFAULT: \"single,normal,nextW,next\"]')\n    parser.add_argument('-FEB', '--force-exe-build', type='bool',\n                        default=True,\n                        help='Force to build the simulation executable [DEFAULT: True]')\n    parser.add_argument('-CS', '--cache-size', type=int,\n                        default=104857600,\n                        help='Size of the cache to simulate in Mega Bytes [DEFAULT: 104857600]')\n    parser.add_argument('-R', '--region', type=str,\n                        default=\"all\",\n                        help='Region of the data to simulate [DEFAULT: \"all\"]')\n    parser.add_argument('-WS', '--window-size', type=int,\n                        default=7,\n                        help='Size of the window to simulate [DEFAULT: 7]')\n    parser.add_argument('-WSTA', '--window-start', type=int,\n                        default=0,\n                        help='Window where to start from [DEFAULT: 0]')\n    parser.add_argument('-WSTO', '--window-stop', type=int,\n                        default=4,\n                        help='Window where to stop [DEFAULT: 4]')\n    parser.add_argument('--population-size', type=int,\n                        default=2000,\n                        help='Num. of individuals in the GA [DEFAULT: 100]')\n    parser.add_argument('--num-generations', type=int,\n                        default=1000,\n                        help='Num. of generations of GA [DEFAULT: 200]')\n    parser.add_argument('--out-html', type='bool',\n                        default=True,\n                        help='Plot the output as a html [DEFAULT: True]')\n    parser.add_argument('--out-png', type='bool',\n                        default=False,\n                        help='Plot the output as a png (requires phantomjs-prebuilt installed with npm) [DEFAULT: False]')\n    parser.add_argument('--plot-filters', type=str,\n                        default=\"\",\n                        help='A comma separate string to search as filters')\n    parser.add_argument('--only-CPU', type='bool',\n                        default=True,\n                        help='Force to use only CPU with TensorFlow [DEFAULT: True]')\n    parser.add_argument('--insert-best-greedy', type='bool',\n                        default=False,\n                        help='Force to use insert 1 individual equal to the greedy composition [DEFAULT: False]')\n    parser.add_argument('--dataset-creation-method', type=str,\n                        choices=['greedy', 'ga'], default=\"greedy\",\n                        help='The method used to create the dataset [DEFAULT: \"greedy\"]')\n    parser.add_argument('--dataset-folder', type=str,\n                        default=\"./datasets\",\n                        help='Folder where datasets are stored [DEFAULT: \"./datasets\"]')\n    parser.add_argument('--dataset-prefix', type=str,\n                        default=\"dataset_best_solution\",\n                        help='The dataset file name prefix [DEFAULT: \"dataset_best_solution\"]')\n    parser.add_argument('--plot-resolution', type=str,\n                        default=\"800,600\",\n                        help='A comma separate string representing the target resolution of each plot [DEFAULT: 640,480]')\n    parser.add_argument('--ai-model-basename', type=str,\n                        default=\"./models/donkey_model\",\n                        help='Ai Model basename and path [DEFAULT: \"./models/donkey_model\"]')\n    parser.add_argument('--feature-prefix', type=str,\n                        default=\"featureConverter\",\n                        help='Ai Model feature converter name prefix [DEFAULT: \"featureConverter\"]')\n    parser.add_argument('--use-qlearn', type='bool',\n                        default=False,\n                        help='Force to use Q-Learning method [DEFAULT: False]')\n\n    args, _ = parser.parse_known_args()\n\n    if args.only_CPU:\n        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n    else:\n        # Make visible only first device\n        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n    if args.action in [\"simulate\", \"testAI\", \"testDataset\"]:\n        if not os.path.exists(args.source):\n            print(f\"Path '{args.source}' does not exist!\")\n            exit(-1)\n\n        simulator_exe = get_simulator_exe(force_creation=args.force_exe_build)\n        cache_types = args.cache_types.split(\",\")\n        simulation_steps = args.simulation_steps.split(\",\")\n\n        base_dir = path.abspath(path.join(os.getcwd(), args.out_folder))\n        os.makedirs(base_dir, exist_ok=True)\n\n        with open(path.join(base_dir, \"simulator.version\"), \"w\") as ver_file:\n            output = subprocess.check_output(\n                \" \".join([simulator_exe, 'version']),\n                shell=True,\n            )\n            ver_file.write(output.decode('ascii'))\n\n        processes = []\n\n        ##\n        # Single Window runs\n        single_window_run_dir = path.join(\n            base_dir,\n            \"run_single_window\"\n        )\n        os.makedirs(single_window_run_dir, exist_ok=True)\n\n        if 'single' in simulation_steps:\n            for window_idx in range(args.window_start, args.window_stop):\n                for cache_type in cache_types:\n                    working_dir = path.join(\n                        single_window_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"window_{window_idx}\",\n                    )\n                    os.makedirs(working_dir, exist_ok=True)\n                    # Create base command\n                    exe_args = [\n                        simulator_exe,\n                        args.action,\n                        cache_type,\n                        path.abspath(args.source),\n                        f\"--size={args.cache_size}\",\n                        f\"--simRegion={args.region}\",\n                        f\"--simWindowSize={args.window_size}\",\n                        f\"--simStartFromWindow={window_idx}\",\n                        f\"--simStopWindow={window_idx+1}\",\n                        \"--simDump=true\",\n                        \"--simDumpFileName=dump.json.gz\",\n                    ]\n                    # Add custom cache parameters\n                    if cache_type == 'aiLRU':\n                        feature_map_file = path.abspath(\n                            path.join(\n                                path.dirname(args.ai_model_basename),\n                                f\"{args.feature_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        model_weights_file = path.abspath(\n                            f\"{args.ai_model_basename.split('.h5')[0]}-window_{window_idx:02d}.dump.json.gz\"\n                        )\n                        exe_args.append(f\"--aiFeatureMap={feature_map_file}\")\n                        if args.use_qlearn:\n                            exe_args.append(\"--aiQLearn=true\")\n                        else:\n                            exe_args.append(\"--aiHost=127.0.0.1\")\n                            exe_args.append(f\"--aiPort=4242\")\n                            exe_args.append(f\"--aiModel={model_weights_file}\")\n                    elif cache_type == 'lruDatasetVerifier':\n                        dataset_file = path.abspath(\n                            path.join(\n                                args.dataset_folder,\n                                f\"{args.dataset_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        exe_args.append(f\"--dataset2TestPath={dataset_file}\")\n                    # Create the task\n                    cur_process = subprocess.Popen(\n                        \" \".join(exe_args),\n                        shell=True,\n                        cwd=working_dir,\n                        stdin=subprocess.PIPE,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                    )\n                    processes.append((\"Single Window\", cur_process))\n\n            wait_jobs(processes)\n\n        ##\n        # Normal runs\n        normal_run_dir = path.join(\n            base_dir,\n            \"run_full_normal\"\n        )\n        os.makedirs(normal_run_dir, exist_ok=True)\n\n        if 'normal' in simulation_steps:\n            for cache_type in cache_types:\n                working_dir = path.join(\n                    normal_run_dir,\n                    f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\"\n                )\n                os.makedirs(working_dir, exist_ok=True)\n                # Create base command\n                exe_args = [\n                    simulator_exe,\n                    args.action,\n                    cache_type,\n                    path.abspath(args.source),\n                    f\"--size={args.cache_size}\",\n                    f\"--simRegion={args.region}\",\n                    f\"--simWindowSize={args.window_size}\",\n                    f\"--simStartFromWindow={args.window_start}\",\n                    f\"--simStopWindow={args.window_stop}\",\n                ]\n                # Add custom cache parameters\n                if cache_type == 'aiLRU':\n                    feature_map_file = path.abspath(\n                        path.join(\n                            path.dirname(args.ai_model_basename),\n                            f\"{args.feature_prefix}-window_00.json.gz\"\n                        )\n                    )\n                    model_weights_file = path.abspath(\n                        f\"{args.ai_model_basename.split('.h5')[0]}-window_00.dump.json.gz\"\n                    )\n                    exe_args.append(f\"--aiFeatureMap={feature_map_file}\")\n                    if args.use_qlearn:\n                        exe_args.append(\"--aiQLearn=true\")\n                    else:\n                        exe_args.append(\"--aiHost=127.0.0.1\")\n                        exe_args.append(f\"--aiPort=4242\")\n                        exe_args.append(f\"--aiModel={model_weights_file}\")\n                elif cache_type == 'lruDatasetVerifier':\n                    dataset_file = path.abspath(\n                        path.join(\n                            args.dataset_folder,\n                            f\"{args.dataset_prefix}-window_00.json.gz\"\n                        )\n                    )\n                    exe_args.append(f\"--dataset2TestPath={dataset_file}\")\n                cur_process = subprocess.Popen(\n                    \" \".join(exe_args),\n                    shell=True,\n                    cwd=working_dir,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n                processes.append((\"Full Run\", cur_process))\n                # Add custom cache parameters\n                if cache_type == 'aiLRU':\n                    wait_jobs(processes)\n\n            wait_jobs(processes)\n\n        ##\n        # Next windows\n        nexxt_window_run_dir = path.join(\n            base_dir,\n            \"run_next_window\"\n        )\n        os.makedirs(nexxt_window_run_dir, exist_ok=True)\n\n        if 'nextW' in simulation_steps:\n            for window_idx in range(args.window_start, args.window_stop):\n                for cache_type in cache_types:\n                    working_dir = path.join(\n                        nexxt_window_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"window_{window_idx+1}\",\n                    )\n                    dump_dir = path.join(\n                        single_window_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"window_{window_idx}\",\n                    )\n                    os.makedirs(working_dir, exist_ok=True)\n                    # Create base command\n                    exe_args = [\n                        simulator_exe,\n                        args.action,\n                        cache_type,\n                        path.abspath(args.source),\n                        f\"--size={args.cache_size}\",\n                        f\"--simRegion={args.region}\",\n                        f\"--simWindowSize={args.window_size}\",\n                        f\"--simStartFromWindow={window_idx+1}\",\n                        f\"--simStopWindow={window_idx+2}\",\n                        \"--simLoadDump=true\",\n                        f\"--simLoadDumpFileName={path.join(dump_dir, 'dump.json.gz')}\",\n                    ]\n                    # Add custom cache parameters\n                    if cache_type == 'aiLRU':\n                        feature_map_file = path.abspath(\n                            path.join(\n                                path.dirname(args.ai_model_basename),\n                                f\"{args.feature_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        model_weights_file = path.abspath(\n                            f\"{args.ai_model_basename.split('.h5')[0]}-window_{window_idx:02d}.dump.json.gz\"\n                        )\n                        exe_args.append(f\"--aiFeatureMap={feature_map_file}\")\n                        if args.use_qlearn:\n                            exe_args.append(\"--aiQLearn=true\")\n                        else:\n                            exe_args.append(\"--aiHost=127.0.0.1\")\n                            exe_args.append(f\"--aiPort=4242\")\n                            exe_args.append(f\"--aiModel={model_weights_file}\")\n                    elif cache_type == 'lruDatasetVerifier':\n                        dataset_file = path.abspath(\n                            path.join(\n                                args.dataset_folder,\n                                f\"{args.dataset_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        exe_args.append(f\"--dataset2TestPath={dataset_file}\")\n                    # Create the task\n                    cur_process = subprocess.Popen(\n                        \" \".join(exe_args),\n                        shell=True,\n                        cwd=working_dir,\n                        stdin=subprocess.PIPE,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                    )\n                    processes.append((\"Next Window\", cur_process))\n\n            wait_jobs(processes)\n\n        ##\n        # Next Period\n        next_period_run_dir = path.join(\n            base_dir,\n            \"run_next_period\"\n        )\n        os.makedirs(next_period_run_dir, exist_ok=True)\n\n        if 'nextP' in simulation_steps:\n            for window_idx in range(args.window_start, args.window_stop):\n                for cache_type in cache_types:\n                    working_dir = path.join(\n                        next_period_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"windows_{window_idx+1}-{args.window_stop}\",\n                    )\n                    dump_dir = path.join(\n                        single_window_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"window_{window_idx}\",\n                    )\n                    os.makedirs(working_dir, exist_ok=True)\n                    # Create base command\n                    exe_args = [\n                        simulator_exe,\n                        args.action,\n                        cache_type,\n                        path.abspath(args.source),\n                        f\"--size={args.cache_size}\",\n                        f\"--simRegion={args.region}\",\n                        f\"--simWindowSize={args.window_size}\",\n                        f\"--simStartFromWindow={window_idx+1}\",\n                        f\"--simStopWindow={args.window_stop+1}\",\n                        \"--simLoadDump=true\",\n                        f\"--simLoadDumpFileName={path.join(dump_dir, 'dump.json.gz')}\",\n                    ]\n                    # Add custom cache parameters\n                    if cache_type == 'aiLRU':\n                        feature_map_file = path.abspath(\n                            path.join(\n                                path.dirname(args.ai_model_basename),\n                                f\"{args.feature_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        model_weights_file = path.abspath(\n                            f\"{args.ai_model_basename.split('.h5')[0]}-window_{window_idx:02d}.dump.json.gz\"\n                        )\n                        exe_args.append(f\"--aiFeatureMap={feature_map_file}\")\n                        if args.use_qlearn:\n                            exe_args.append(\"--aiQLearn=true\")\n                        else:\n                            exe_args.append(\"--aiHost=127.0.0.1\")\n                            exe_args.append(f\"--aiPort=4242\")\n                            exe_args.append(f\"--aiModel={model_weights_file}\")\n                    elif cache_type == 'lruDatasetVerifier':\n                        dataset_file = path.abspath(\n                            path.join(\n                                args.dataset_folder,\n                                f\"{args.dataset_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        exe_args.append(f\"--dataset2TestPath={dataset_file}\")\n                    # Create the task\n                    cur_process = subprocess.Popen(\n                        \" \".join(exe_args),\n                        shell=True,\n                        cwd=working_dir,\n                        stdin=subprocess.PIPE,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                    )\n                    processes.append((\"Next Period\", cur_process))\n\n            wait_jobs(processes)\n\n    elif args.action == \"plot\":\n        if not path.exists(args.source):\n            print(f\"Cannot find folder '{args.source}'\")\n            exit(-1)\n        filters = [elm for elm in args.plot_filters.split(\",\") if elm]\n        results = load_results(args.source)\n        plot_width, plot_height = [\n            int(elm) for elm in args.plot_resolution.split(\",\")\n            if elm\n        ]\n        plot_results(\n            args.source, results,\n            window_size=args.window_size,\n            filters=filters,\n            html=args.out_html,\n            png=args.out_png,\n            plot_width=plot_width,\n            plot_height=plot_height,\n            read_on_hit=args.read_on_hit,\n        )\n\n    elif args.action == \"train\":\n        datasets = []\n        for root, dirs, files in walk(args.source):\n            for file_ in files:\n                head, tail = path.splitext(file_)\n                if tail == \".npz\":\n                    datasets.append(\n                        path.join(root, file_)\n                    )\n\n        for dataset_file in datasets:\n            print(f\"[Start training][Dataset: {dataset_file}]\")\n            dataset = SimulatorDatasetReader(\n            ).load_data_and_labels(dataset_file)\n            window_num = dataset_file.split(\"-window_\")[1].split(\".\")[0]\n            model = DonkeyModel()\n            data, labels = dataset.data\n            # print(data.shape)\n            model.train(data, labels)\n            out_path = path.join(\n                path.dirname(dataset_file), f\"donkey_model-window_{window_num}\"\n            )\n            model.save(out_path).export_weights(out_path)\n            print(f\"[Model saved][Output: {out_path}...]\")\n\n    elif args.action == \"create_dataset\":\n        base_dir = path.join(\n            path.dirname(path.abspath(args.source)), \"datasets\"\n        )\n        os.makedirs(base_dir, exist_ok=True)\n\n        day_files = []\n        for root, dirs, files in walk(args.source):\n            for file_ in tqdm(sorted(files), desc=\"Search files\", ascii=True):\n                head, tail = path.splitext(file_)\n                if tail == \".gz\":\n                    _, tail = path.splitext(head)\n                    if tail == \".csv\" or tail == \".feather\":\n                        day_files.append(\n                            path.join(\n                                root, file_\n                            )\n                        )\n\n        windows = []\n        cur_window = []\n        for file_ in day_files:\n            if len(cur_window) < args.window_size:\n                cur_window.append(file_)\n            else:\n                windows.append(cur_window)\n                cur_window = []\n        else:\n            if len(cur_window):\n                windows.append(cur_window)\n                cur_window = []\n\n        for winIdx, window in enumerate(windows):\n            if winIdx == args.window_stop:\n                break\n\n            list_df = []\n            files = {}\n            for file_ in tqdm(window, desc=f\"Create window {winIdx} dataframe\",\n                              ascii=True):\n                head, _ = path.splitext(file_)\n                _, tail = path.splitext(head)\n                with gzip.GzipFile(file_, \"rb\") as cur_file:\n                    if tail == \".csv\":\n                        df = pd.read_csv(cur_file)\n                    elif tail == \".feather\":\n                        df = pd.read_feather(cur_file)\n                    else:\n                        raise Exception(\n                            f\"Error: extension '{tail}' not supported...\")\n                list_df.append(df)\n            cur_df = pd.concat(list_df, ignore_index=True).dropna()\n            # print(cur_df.shape)\n            if args.region != 'all':\n                cur_df = cur_df[cur_df['site_name'].str.contains(\n                    f\"_{args.region}_\", case=False)\n                ]\n            # print(cur_df.shape)\n\n            stat_avg_time = []\n            stat_num_req = []\n            max_history = 64\n\n            for cur_row in tqdm(cur_df.itertuples(), total=cur_df.shape[0],\n                                desc=f\"Parse window {winIdx} dataframe\",\n                                ascii=True):\n                cur_filename = cur_row.filename\n                cur_size = cur_row.size\n                if cur_filename not in files:\n                    data_type, campain, process, file_type = cur_filename.split(\"/\")[2:6]\n                    files[cur_filename] = {\n                        'size': cur_size,\n                        'totReq': 0,\n                        'days': [],\n                        'campain': campain,\n                        'process': process,\n                        'reqHistory': [],\n                        'lastReq': 0,\n                        'fileType': file_type,\n                        'dataType': data_type,\n                    }\n                cur_time = datetime.fromtimestamp(cur_row.day)\n                cur_file_stats = files[cur_filename]\n                cur_file_stats['totReq'] += 1\n                cur_file_stats['lastReq'] = cur_time\n                if len(cur_file_stats['reqHistory']) > max_history:\n                    cur_file_stats['reqHistory'].pop()\n\n                cur_file_stats['reqHistory'].append(cur_time)\n\n                assert cur_file_stats['size'] == cur_size, f\"{cur_file_stats['size']} != {cur_size}\"\n\n                if cur_row.day not in cur_file_stats['days']:\n                    cur_file_stats['days'].append(cur_row.day)\n\n                stat_num_req.append(cur_file_stats['totReq'])\n                stat_avg_time.append(\n                    sum([\n                        (cur_file_stats['lastReq'] - elm).total_seconds() / 60.\n                        for elm in cur_file_stats['reqHistory']\n                    ]) / max_history\n                )\n\n            cur_df['avg_time'] = stat_avg_time\n            cur_df['num_req'] = stat_num_req\n\n            files_df = pd.DataFrame(\n                data={\n                    'filename': [filename\n                                 for filename in files],\n                    'size': [files[filename]['size']\n                             for filename in files],\n                    'totReq': [files[filename]['totReq']\n                               for filename in files],\n                    'fileType': [files[filename]['fileType']\n                                 for filename in files],\n                    'dataType': [files[filename]['dataType']\n                                 for filename in files],\n                    'campain': [files[filename]['campain']\n                               for filename in files],\n                    'process': [files[filename]['process']\n                               for filename in files],\n                }\n            )\n\n            # Remove 1 request files\n            # files_df = files_df.drop(files_df[files_df.totReq == 1].index)\n\n            # TO Megabytes\n            files_df['size'] = files_df['size'] / 1024**2\n\n            # Add value\n            files_df['value'] = (\n                files_df['size'] *\n                files_df['totReq']\n            ) / args.window_size\n\n            # Remove low value files\n            # q1 = files_df.value.describe().quantile(0.25)\n            # files_df = files_df.drop(files_df[files_df.value < q1].index)\n\n            # Sort and reset indexes\n            # Note: greedyValue is prepared for 2 PTAS algorithm\n            files_df['greedyValue'] = files_df['value'] / files_df['size']\n            files_df = files_df.sort_values(\n                by=['greedyValue'], ascending=False)\n            files_df = files_df.reset_index(drop=True)\n            # print(files_df)\n\n            # print(\n            #   sum(files_df['size']), args.cache_size,\n            #   sum(files_df['size'])/args.cache_size\n            # )\n\n            greedy_solution = get2PTAS(\n                files_df, args.cache_size\n            )\n\n            if args.dataset_creation_method == \"ga\":\n                best_selection = get_best_configuration(\n                    files_df, args.cache_size,\n                    population_size=args.population_size,\n                    num_generations=args.num_generations,\n                    insert_best_greedy=args.insert_best_greedy,\n                )\n                compare_greedy_solution(\n                    files_df, args.cache_size, greedy_solution,\n                )\n            else:\n                best_selection = greedy_solution\n                gr_size = sum(files_df[best_selection]['size'].to_list())\n                gr_score = sum(files_df[best_selection]['value'].to_list())\n                print(\"---[Results]---\")\n                print(\n                    f\"[Size: \\t{gr_size:0.2f}][Score: \\t{gr_score:0.2f}][Greedy]\")\n\n            files_df['class'] = best_selection\n\n            dataset_labels_out_file = path.join(\n                base_dir,\n                f\"dataset_labels-window_{winIdx:02d}.feather.gz\"\n            )\n\n            dataset_best_solution_out_file = path.join(\n                base_dir,\n                f\"dataset_best_solution-window_{winIdx:02d}.json.gz\"\n            )\n\n            # get 30% of the requests\n            len_dataset = int(cur_df.shape[0] * 0.30)\n\n            sample = cur_df.sample(n=len_dataset, random_state=42)\n            sample.rename(columns={'size': 'fileSize'}, inplace=True)\n\n            dataset_df = pd.merge(sample, files_df, on='filename')\n            dataset_df = dataset_df[\n                ['site_name', 'user', 'num_req', 'avg_time',\n                 'size', 'fileType', 'dataType', \n                 'campain', 'process', 'class']\n            ]\n            dataset_df.rename(\n                columns={\n                    'site_name': \"siteName\",\n                    'user': \"userID\",\n                    'num_req': \"numReq\",\n                    'avg_time': \"avgTime\",\n                },\n                inplace=True\n            )\n\n            with yaspin(\n                Spinners.bouncingBall,\n                text=f\"[Store labeleled stage dataset][{dataset_labels_out_file}]\"\n            ):\n                with gzip.GzipFile(dataset_labels_out_file, \"wb\") as out_file:\n                    dataset_df.to_feather(out_file)\n\n            with yaspin(\n                Spinners.bouncingBall,\n                text=f\"[Store best stolution][{dataset_best_solution_out_file}]\"\n            ):\n                with gzip.GzipFile(dataset_best_solution_out_file, \"wb\") as out_file:\n                    out_file.write(\n                        json.dumps({\n                            'selected_files': files_df[\n                                files_df['class'] == True\n                            ]['filename'].to_list()\n                        }).encode(\"utf-8\")\n                    )\n\n            # Get some stats\n            # print(dataset_df.describe())\n\n            # Prepare dataset\n            print(f\"[Prepare dataset][Using: '{dataset_labels_out_file}']\")\n            dataset = SimulatorDatasetReader(dataset_labels_out_file)\n            dataset.modify_column(\n                'size',\n                lambda column: (column / 1024**2)\n            ).make_converter_map(\n                [\n                    'class',\n                ],\n                map_type=bool,\n                sort_keys=True,\n            ).make_converter_map(\n                [\n                    'size',\n                ],\n                map_type=int,\n                sort_keys=True,\n                buckets=[5, 10, 50, 100, 250, 500, 1000, 2000, 4000, 10000, '...'],\n            ).make_converter_map(\n                [\n                    'numReq',\n                ],\n                map_type=int,\n                sort_keys=True,\n                buckets=[1, 2, 3, 4, 5, 10, 25, 50, 75, 100, 200, '...'],\n            ).make_converter_map(\n                [\n                    'avgTime',\n                ],\n                map_type=int,\n                sort_keys=True,\n                buckets=list(range(0, 6*1000, 100)) + ['...'],\n            ).make_converter_map(\n                [\n                    'siteName',\n                    'userID',\n                    'fileType',\n                    'dataType',\n                    'campain',\n                    'process',\n                ],\n                unknown_values=True,\n                map_type=str,\n            ).store_converter_map(\n                f\"featureConverter-window_{winIdx:02d}\"\n            ).make_data_and_labels(\n                [\n                    'siteName',\n                    'userID',\n                    'fileType',\n                    'dataType',\n                    'campain',\n                    'process',\n                    'numReq',\n                    'avgTime',\n                    'size',\n                ],\n                'class'\n            ).save_data_and_labels(\n                f\"dataset_converted-window_{winIdx:02d}\"\n            )\n            print(\n                f\"[Dataset created][Name: 'dataset_converted-window_{winIdx:02d}']\"\n            )", "output": "def main():\n    parser = argparse.ArgumentParser(\n        \"simulator\", description=\"Simulation and result plotting\")\n\n    parser.register('type', 'bool', str2bool)  # add type keyword to registries\n\n    parser.add_argument('action', choices=['simulate', 'testAI', 'testDataset', 'plot', 'train', 'create_dataset'],\n                        default=\"simulate\",\n                        help='Action requested')\n    parser.add_argument('source', type=str,\n                        default=\"./results_8w_with_sizes_csv\",\n                        help='The folder where the json results are stored [DEFAULT: \"./results_8w_with_sizes_csv\"]')\n    parser.add_argument('--cache-types', type=str,\n                        default=\"lru,weightedLRU\",\n                        help='Comma separated list of cache to simulate [DEFAULT: \"lru,weightedLRU\"]')\n    parser.add_argument('--out-folder', type=str,\n                        default=\"./simulation_results\",\n                        help='The folder where the simulation results will be stored [DEFAULT: \"simulation_results\"]')\n    parser.add_argument('--read-on-hit', type='bool',\n                        default=False,\n                        help='Use read on hit data [DEFAULT: True]')\n    parser.add_argument('--simulation-steps', type=str,\n                        default='single,normal,nextW,nextP',\n                        help='Select the simulation steps [DEFAULT: \"single,normal,nextW,next\"]')\n    parser.add_argument('-FEB', '--force-exe-build', type='bool',\n                        default=True,\n                        help='Force to build the simulation executable [DEFAULT: True]')\n    parser.add_argument('-CS', '--cache-size', type=int,\n                        default=104857600,\n                        help='Size of the cache to simulate in Mega Bytes [DEFAULT: 104857600]')\n    parser.add_argument('-R', '--region', type=str,\n                        default=\"all\",\n                        help='Region of the data to simulate [DEFAULT: \"all\"]')\n    parser.add_argument('-WS', '--window-size', type=int,\n                        default=7,\n                        help='Size of the window to simulate [DEFAULT: 7]')\n    parser.add_argument('-WSTA', '--window-start', type=int,\n                        default=0,\n                        help='Window where to start from [DEFAULT: 0]')\n    parser.add_argument('-WSTO', '--window-stop', type=int,\n                        default=4,\n                        help='Window where to stop [DEFAULT: 4]')\n    parser.add_argument('--population-size', type=int,\n                        default=2000,\n                        help='Num. of individuals in the GA [DEFAULT: 100]')\n    parser.add_argument('--num-generations', type=int,\n                        default=1000,\n                        help='Num. of generations of GA [DEFAULT: 200]')\n    parser.add_argument('--out-html', type='bool',\n                        default=True,\n                        help='Plot the output as a html [DEFAULT: True]')\n    parser.add_argument('--out-png', type='bool',\n                        default=False,\n                        help='Plot the output as a png (requires phantomjs-prebuilt installed with npm) [DEFAULT: False]')\n    parser.add_argument('--plot-filters', type=str,\n                        default=\"\",\n                        help='A comma separate string to search as filters')\n    parser.add_argument('--only-CPU', type='bool',\n                        default=True,\n                        help='Force to use only CPU with TensorFlow [DEFAULT: True]')\n    parser.add_argument('--insert-best-greedy', type='bool',\n                        default=False,\n                        help='Force to use insert 1 individual equal to the greedy composition [DEFAULT: False]')\n    parser.add_argument('--dataset-creation-method', type=str,\n                        choices=['greedy', 'ga'], default=\"greedy\",\n                        help='The method used to create the dataset [DEFAULT: \"greedy\"]')\n    parser.add_argument('--dataset-folder', type=str,\n                        default=\"./datasets\",\n                        help='Folder where datasets are stored [DEFAULT: \"./datasets\"]')\n    parser.add_argument('--dataset-prefix', type=str,\n                        default=\"dataset_best_solution\",\n                        help='The dataset file name prefix [DEFAULT: \"dataset_best_solution\"]')\n    parser.add_argument('--plot-resolution', type=str,\n                        default=\"800,600\",\n                        help='A comma separate string representing the target resolution of each plot [DEFAULT: 640,480]')\n    parser.add_argument('--ai-model-basename', type=str,\n                        default=\"./models/donkey_model\",\n                        help='Ai Model basename and path [DEFAULT: \"./models/donkey_model\"]')\n    parser.add_argument('--feature-prefix', type=str,\n                        default=\"featureConverter\",\n                        help='Ai Model feature converter name prefix [DEFAULT: \"featureConverter\"]')\n    parser.add_argument('--use-qlearn', type='bool',\n                        default=False,\n                        help='Force to use Q-Learning method [DEFAULT: False]')\n\n    args, _ = parser.parse_known_args()\n\n    if args.only_CPU:\n        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n    else:\n        # Make visible only first device\n        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n    if args.action in [\"simulate\", \"testAI\", \"testDataset\"]:\n        if not os.path.exists(args.source):\n            print(f\"Path '{args.source}' does not exist!\")\n            exit(-1)\n\n        simulator_exe = get_simulator_exe(force_creation=args.force_exe_build)\n        cache_types = args.cache_types.split(\",\")\n        simulation_steps = args.simulation_steps.split(\",\")\n\n        base_dir = path.abspath(path.join(os.getcwd(), args.out_folder))\n        os.makedirs(base_dir, exist_ok=True)\n\n        with open(path.join(base_dir, \"simulator.version\"), \"w\") as ver_file:\n            output = subprocess.check_output(\n                \" \".join([simulator_exe, 'version']),\n                shell=True,\n            )\n            ver_file.write(output.decode('ascii'))\n\n        processes = []\n\n        ##\n        # Single Window runs\n        single_window_run_dir = path.join(\n            base_dir,\n            \"run_single_window\"\n        )\n        os.makedirs(single_window_run_dir, exist_ok=True)\n\n        if 'single' in simulation_steps:\n            for window_idx in range(args.window_start, args.window_stop):\n                for cache_type in cache_types:\n                    working_dir = path.join(\n                        single_window_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"window_{window_idx}\",\n                    )\n                    os.makedirs(working_dir, exist_ok=True)\n                    # Create base command\n                    exe_args = [\n                        simulator_exe,\n                        args.action,\n                        cache_type,\n                        path.abspath(args.source),\n                        f\"--size={args.cache_size}\",\n                        f\"--simRegion={args.region}\",\n                        f\"--simWindowSize={args.window_size}\",\n                        f\"--simStartFromWindow={window_idx}\",\n                        f\"--simStopWindow={window_idx+1}\",\n                        \"--simDump=true\",\n                        \"--simDumpFileName=dump.json.gz\",\n                    ]\n                    # Add custom cache parameters\n                    if cache_type == 'aiLRU':\n                        feature_map_file = path.abspath(\n                            path.join(\n                                path.dirname(args.ai_model_basename),\n                                f\"{args.feature_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        model_weights_file = path.abspath(\n                            f\"{args.ai_model_basename.split('.h5')[0]}-window_{window_idx:02d}.dump.json.gz\"\n                        )\n                        exe_args.append(f\"--aiFeatureMap={feature_map_file}\")\n                        if args.use_qlearn:\n                            exe_args.append(\"--aiQLearn=true\")\n                        else:\n                            exe_args.append(\"--aiHost=127.0.0.1\")\n                            exe_args.append(f\"--aiPort=4242\")\n                            exe_args.append(f\"--aiModel={model_weights_file}\")\n                    elif cache_type == 'lruDatasetVerifier':\n                        dataset_file = path.abspath(\n                            path.join(\n                                args.dataset_folder,\n                                f\"{args.dataset_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        exe_args.append(f\"--dataset2TestPath={dataset_file}\")\n                    # Create the task\n                    cur_process = subprocess.Popen(\n                        \" \".join(exe_args),\n                        shell=True,\n                        cwd=working_dir,\n                        stdin=subprocess.PIPE,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                    )\n                    processes.append((\"Single Window\", cur_process))\n\n            wait_jobs(processes)\n\n        ##\n        # Normal runs\n        normal_run_dir = path.join(\n            base_dir,\n            \"run_full_normal\"\n        )\n        os.makedirs(normal_run_dir, exist_ok=True)\n\n        if 'normal' in simulation_steps:\n            for cache_type in cache_types:\n                working_dir = path.join(\n                    normal_run_dir,\n                    f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\"\n                )\n                os.makedirs(working_dir, exist_ok=True)\n                # Create base command\n                exe_args = [\n                    simulator_exe,\n                    args.action,\n                    cache_type,\n                    path.abspath(args.source),\n                    f\"--size={args.cache_size}\",\n                    f\"--simRegion={args.region}\",\n                    f\"--simWindowSize={args.window_size}\",\n                    f\"--simStartFromWindow={args.window_start}\",\n                    f\"--simStopWindow={args.window_stop}\",\n                ]\n                # Add custom cache parameters\n                if cache_type == 'aiLRU':\n                    feature_map_file = path.abspath(\n                        path.join(\n                            path.dirname(args.ai_model_basename),\n                            f\"{args.feature_prefix}-window_00.json.gz\"\n                        )\n                    )\n                    model_weights_file = path.abspath(\n                        f\"{args.ai_model_basename.split('.h5')[0]}-window_00.dump.json.gz\"\n                    )\n                    exe_args.append(f\"--aiFeatureMap={feature_map_file}\")\n                    if args.use_qlearn:\n                        exe_args.append(\"--aiQLearn=true\")\n                    else:\n                        exe_args.append(\"--aiHost=127.0.0.1\")\n                        exe_args.append(f\"--aiPort=4242\")\n                        exe_args.append(f\"--aiModel={model_weights_file}\")\n                elif cache_type == 'lruDatasetVerifier':\n                    dataset_file = path.abspath(\n                        path.join(\n                            args.dataset_folder,\n                            f\"{args.dataset_prefix}-window_00.json.gz\"\n                        )\n                    )\n                    exe_args.append(f\"--dataset2TestPath={dataset_file}\")\n                cur_process = subprocess.Popen(\n                    \" \".join(exe_args),\n                    shell=True,\n                    cwd=working_dir,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n                processes.append((\"Full Run\", cur_process))\n                # Add custom cache parameters\n                if cache_type == 'aiLRU':\n                    wait_jobs(processes)\n\n            wait_jobs(processes)\n\n        ##\n        # Next windows\n        nexxt_window_run_dir = path.join(\n            base_dir,\n            \"run_next_window\"\n        )\n        os.makedirs(nexxt_window_run_dir, exist_ok=True)\n\n        if 'nextW' in simulation_steps:\n            for window_idx in range(args.window_start, args.window_stop):\n                for cache_type in cache_types:\n                    working_dir = path.join(\n                        nexxt_window_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"window_{window_idx+1}\",\n                    )\n                    dump_dir = path.join(\n                        single_window_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"window_{window_idx}\",\n                    )\n                    os.makedirs(working_dir, exist_ok=True)\n                    # Create base command\n                    exe_args = [\n                        simulator_exe,\n                        args.action,\n                        cache_type,\n                        path.abspath(args.source),\n                        f\"--size={args.cache_size}\",\n                        f\"--simRegion={args.region}\",\n                        f\"--simWindowSize={args.window_size}\",\n                        f\"--simStartFromWindow={window_idx+1}\",\n                        f\"--simStopWindow={window_idx+2}\",\n                        \"--simLoadDump=true\",\n                        f\"--simLoadDumpFileName={path.join(dump_dir, 'dump.json.gz')}\",\n                    ]\n                    # Add custom cache parameters\n                    if cache_type == 'aiLRU':\n                        feature_map_file = path.abspath(\n                            path.join(\n                                path.dirname(args.ai_model_basename),\n                                f\"{args.feature_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        model_weights_file = path.abspath(\n                            f\"{args.ai_model_basename.split('.h5')[0]}-window_{window_idx:02d}.dump.json.gz\"\n                        )\n                        exe_args.append(f\"--aiFeatureMap={feature_map_file}\")\n                        if args.use_qlearn:\n                            exe_args.append(\"--aiQLearn=true\")\n                        else:\n                            exe_args.append(\"--aiHost=127.0.0.1\")\n                            exe_args.append(f\"--aiPort=4242\")\n                            exe_args.append(f\"--aiModel={model_weights_file}\")\n                    elif cache_type == 'lruDatasetVerifier':\n                        dataset_file = path.abspath(\n                            path.join(\n                                args.dataset_folder,\n                                f\"{args.dataset_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        exe_args.append(f\"--dataset2TestPath={dataset_file}\")\n                    # Create the task\n                    cur_process = subprocess.Popen(\n                        \" \".join(exe_args),\n                        shell=True,\n                        cwd=working_dir,\n                        stdin=subprocess.PIPE,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                    )\n                    processes.append((\"Next Window\", cur_process))\n\n            wait_jobs(processes)\n\n        ##\n        # Next Period\n        next_period_run_dir = path.join(\n            base_dir,\n            \"run_next_period\"\n        )\n        os.makedirs(next_period_run_dir, exist_ok=True)\n\n        if 'nextP' in simulation_steps:\n            for window_idx in range(args.window_start, args.window_stop):\n                for cache_type in cache_types:\n                    working_dir = path.join(\n                        next_period_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"windows_{window_idx+1}-{args.window_stop}\",\n                    )\n                    dump_dir = path.join(\n                        single_window_run_dir,\n                        f\"{cache_type}_{int(args.cache_size/1024**2)}T_{args.region}\",\n                        f\"window_{window_idx}\",\n                    )\n                    os.makedirs(working_dir, exist_ok=True)\n                    # Create base command\n                    exe_args = [\n                        simulator_exe,\n                        args.action,\n                        cache_type,\n                        path.abspath(args.source),\n                        f\"--size={args.cache_size}\",\n                        f\"--simRegion={args.region}\",\n                        f\"--simWindowSize={args.window_size}\",\n                        f\"--simStartFromWindow={window_idx+1}\",\n                        f\"--simStopWindow={args.window_stop+1}\",\n                        \"--simLoadDump=true\",\n                        f\"--simLoadDumpFileName={path.join(dump_dir, 'dump.json.gz')}\",\n                    ]\n                    # Add custom cache parameters\n                    if cache_type == 'aiLRU':\n                        feature_map_file = path.abspath(\n                            path.join(\n                                path.dirname(args.ai_model_basename),\n                                f\"{args.feature_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        model_weights_file = path.abspath(\n                            f\"{args.ai_model_basename.split('.h5')[0]}-window_{window_idx:02d}.dump.json.gz\"\n                        )\n                        exe_args.append(f\"--aiFeatureMap={feature_map_file}\")\n                        if args.use_qlearn:\n                            exe_args.append(\"--aiQLearn=true\")\n                        else:\n                            exe_args.append(\"--aiHost=127.0.0.1\")\n                            exe_args.append(f\"--aiPort=4242\")\n                            exe_args.append(f\"--aiModel={model_weights_file}\")\n                    elif cache_type == 'lruDatasetVerifier':\n                        dataset_file = path.abspath(\n                            path.join(\n                                args.dataset_folder,\n                                f\"{args.dataset_prefix}-window_{window_idx:02d}.json.gz\"\n                            )\n                        )\n                        exe_args.append(f\"--dataset2TestPath={dataset_file}\")\n                    # Create the task\n                    cur_process = subprocess.Popen(\n                        \" \".join(exe_args),\n                        shell=True,\n                        cwd=working_dir,\n                        stdin=subprocess.PIPE,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                    )\n                    processes.append((\"Next Period\", cur_process))\n\n            wait_jobs(processes)\n\n    elif args.action == \"plot\":\n        if not path.exists(args.source):\n            print(f\"Cannot find folder '{args.source}'\")\n            exit(-1)\n        filters = [elm for elm in args.plot_filters.split(\",\") if elm]\n        results = load_results(args.source)\n        plot_width, plot_height = [\n            int(elm) for elm in args.plot_resolution.split(\",\")\n            if elm\n        ]\n        plot_results(\n            args.source, results,\n            window_size=args.window_size,\n            filters=filters,\n            html=args.out_html,\n            png=args.out_png,\n            plot_width=plot_width,\n            plot_height=plot_height,\n            read_on_hit=args.read_on_hit,\n        )\n\n    elif args.action == \"train\":\n        datasets = []\n        for root, dirs, files in walk(args.source):\n            for file_ in files:\n                head, tail = path.splitext(file_)\n                if tail == \".npz\":\n                    datasets.append(\n                        path.join(root, file_)\n                    )\n\n        for dataset_file in datasets:\n            print(f\"[Start training][Dataset: {dataset_file}]\")\n            dataset = SimulatorDatasetReader(\n            ).load_data_and_labels(dataset_file)\n            window_num = dataset_file.split(\"-window_\")[1].split(\".\")[0]\n            model = DonkeyModel()\n            data, labels = dataset.data\n            # print(data.shape)\n            model.train(data, labels)\n            out_path = path.join(\n                path.dirname(dataset_file), f\"donkey_model-window_{window_num}\"\n            )\n            model.save(out_path).export_weights(out_path)\n            print(f\"[Model saved][Output: {out_path}...]\")\n\n    elif args.action == \"create_dataset\":\n        base_dir = path.join(\n            path.dirname(path.abspath(args.source)), \"datasets\"\n        )\n        os.makedirs(base_dir, exist_ok=True)\n\n        day_files = []\n        for root, dirs, files in walk(args.source):\n            for file_ in tqdm(sorted(files), desc=\"Search files\", ascii=True):\n                head, tail = path.splitext(file_)\n                if tail == \".gz\":\n                    _, tail = path.splitext(head)\n                    if tail == \".csv\" or tail == \".feather\":\n                        day_files.append(\n                            path.join(\n                                root, file_\n                            )\n                        )\n\n        windows = []\n        cur_window = []\n        for file_ in day_files:\n            if len(cur_window) < args.window_size:\n                cur_window.append(file_)\n            else:\n                windows.append(cur_window)\n                cur_window = []\n        else:\n            if len(cur_window):\n                windows.append(cur_window)\n                cur_window = []\n\n        for winIdx, window in enumerate(windows):\n            if winIdx == args.window_stop:\n                break\n\n            list_df = []\n            files = {}\n            for file_ in tqdm(window, desc=f\"Create window {winIdx} dataframe\",\n                              ascii=True):\n                head, _ = path.splitext(file_)\n                _, tail = path.splitext(head)\n                with gzip.GzipFile(file_, \"rb\") as cur_file:\n                    if tail == \".csv\":\n                        df = pd.read_csv(cur_file)\n                    elif tail == \".feather\":\n                        df = pd.read_feather(cur_file)\n                    else:\n                        raise Exception(\n                            f\"Error: extension '{tail}' not supported...\")\n                list_df.append(df)\n            cur_df = pd.concat(list_df, ignore_index=True).dropna()\n            # print(cur_df.shape)\n            if args.region != 'all':\n                cur_df = cur_df[cur_df['site_name'].str.contains(\n                    f\"_{args.region}_\", case=False)\n                ]\n            # print(cur_df.shape)\n\n            stat_avg_time = []\n            stat_num_req = []\n            max_history = 64\n\n            for cur_row in tqdm(cur_df.itertuples(), total=cur_df.shape[0],\n                                desc=f\"Parse window {winIdx} dataframe\",\n                                ascii=True):\n                cur_filename = cur_row.filename\n                cur_size = cur_row.size\n                if cur_filename not in files:\n                    data_type, campain, process, file_type = cur_filename.split(\"/\")[2:6]\n                    files[cur_filename] = {\n                        'size': cur_size,\n                        'totReq': 0,\n                        'days': [],\n                        'campain': campain,\n                        'process': process,\n                        'reqHistory': [],\n                        'lastReq': 0,\n                        'fileType': file_type,\n                        'dataType': data_type,\n                    }\n                cur_time = datetime.fromtimestamp(cur_row.day)\n                cur_file_stats = files[cur_filename]\n                cur_file_stats['totReq'] += 1\n                cur_file_stats['lastReq'] = cur_time\n                if len(cur_file_stats['reqHistory']) > max_history:\n                    cur_file_stats['reqHistory'].pop()\n\n                cur_file_stats['reqHistory'].append(cur_time)\n\n                assert cur_file_stats['size'] == cur_size, f\"{cur_file_stats['size']} != {cur_size}\"\n\n                if cur_row.day not in cur_file_stats['days']:\n                    cur_file_stats['days'].append(cur_row.day)\n\n                stat_num_req.append(cur_file_stats['totReq'])\n                stat_avg_time.append(\n                    sum([\n                        (cur_file_stats['lastReq'] - elm).total_seconds() / 60.\n                        for elm in cur_file_stats['reqHistory']\n                    ]) / max_history\n                )\n\n            cur_df['avg_time'] = stat_avg_time\n            cur_df['num_req'] = stat_num_req\n\n            files_df = pd.DataFrame(\n                data={\n                    'filename': [filename\n                                 for filename in files],\n                    'size': [files[filename]['size']\n                             for filename in files],\n                    'totReq': [files[filename]['totReq']\n                               for filename in files],\n                    'fileType': [files[filename]['fileType']\n                                 for filename in files],\n                    'dataType': [files[filename]['dataType']\n                                 for filename in files],\n                    'campain': [files[filename]['campain']\n                               for filename in files],\n                    'process': [files[filename]['process']\n                               for filename in files],\n                }\n            )\n\n            # Remove 1 request files\n            # files_df = files_df.drop(files_df[files_df.totReq == 1].index)\n\n            # TO Megabytes\n            files_df['size'] = files_df['size'] / 1024**2\n\n            # Add value\n            files_df['value'] = (\n                files_df['size'] *\n                files_df['totReq']\n            ) / args.window_size\n\n            # Remove low value files\n            # q1 = files_df.value.describe().quantile(0.25)\n            # files_df = files_df.drop(files_df[files_df.value < q1].index)\n\n            # Sort and reset indexes\n            # Note: greedyValue is prepared for 2 PTAS algorithm\n            files_df['greedyValue'] = files_df['value'] / files_df['size']\n            files_df = files_df.sort_values(\n                by=['greedyValue'], ascending=False)\n            files_df = files_df.reset_index(drop=True)\n            # print(files_df)\n\n            # print(\n            #   sum(files_df['size']), args.cache_size,\n            #   sum(files_df['size'])/args.cache_size\n            # )\n\n            greedy_solution = get2PTAS(\n                files_df, args.cache_size\n            )\n\n            if args.dataset_creation_method == \"ga\":\n                best_selection = get_best_configuration(\n                    files_df, args.cache_size,\n                    population_size=args.population_size,\n                    num_generations=args.num_generations,\n                    insert_best_greedy=args.insert_best_greedy,\n                )\n                compare_greedy_solution(\n                    files_df, args.cache_size, greedy_solution,\n                )\n            else:\n                best_selection = greedy_solution\n                gr_size = sum(files_df[best_selection]['size'].to_list())\n                gr_score = sum(files_df[best_selection]['value'].to_list())\n                print(\"---[Results]---\")\n                print(\n                    f\"[Size: \\t{gr_size:0.2f}][Score: \\t{gr_score:0.2f}][Greedy]\")\n\n            files_df['class'] = best_selection\n\n            dataset_labels_out_file = path.join(\n                base_dir,\n                f\"dataset_labels-window_{winIdx:02d}.feather.gz\"\n            )\n\n            dataset_best_solution_out_file = path.join(\n                base_dir,\n                f\"dataset_best_solution-window_{winIdx:02d}.json.gz\"\n            )\n\n            # get 30% of the requests\n            len_dataset = int(cur_df.shape[0] * 0.30)\n\n            sample = cur_df.sample(n=len_dataset, random_state=42)\n            sample.rename(columns={'size': 'fileSize'}, inplace=True)\n\n            dataset_df = pd.merge(sample, files_df, on='filename')\n            dataset_df = dataset_df[\n                ['site_name', 'user', 'num_req', 'avg_time',\n                 'size', 'fileType', 'dataType', \n                 'campain', 'process', 'class']\n            ]\n            dataset_df.rename(\n                columns={\n                    'site_name': \"siteName\",\n                    'user': \"userID\",\n                    'num_req': \"numReq\",\n                    'avg_time': \"avgTime\",\n                },\n                inplace=True\n            )\n\n            with yaspin(\n                Spinners.bouncingBall,\n                text=f\"[Store labeleled stage dataset][{dataset_labels_out_file}]\"\n            ):\n                with gzip.GzipFile(dataset_labels_out_file, \"wb\") as out_file:\n                    dataset_df.to_feather(out_file)\n\n            with yaspin(\n                Spinners.bouncingBall,\n                text=f\"[Store best stolution][{dataset_best_solution_out_file}]\"\n            ):\n                with gzip.GzipFile(dataset_best_solution_out_file, \"wb\") as out_file:\n                    out_file.write(\n                        json.dumps({\n                            'selected_files': files_df[\n                                files_df['class'] == True\n                            ]['filename'].to_list()\n                        }).encode(\"utf-8\")\n                    )\n\n            # Get some stats\n            # print(dataset_df.describe())\n\n            # Prepare dataset\n            print(f\"[Prepare dataset][Using: '{dataset_labels_out_file}']\")\n            dataset = SimulatorDatasetReader(dataset_labels_out_file)\n            dataset.modify_column(\n                'size',\n                lambda column: (column / 1024**2)\n            ).make_converter_map(\n                [\n                    'class',\n                ],\n                map_type=bool,\n                sort_keys=True,\n            ).make_converter_map(\n                [\n                    'size',\n                ],\n                map_type=int,\n                sort_keys=True,\n                buckets=[5, 10, 50, 100, 250, 500, 1000, 2000, 4000, 10000, '...'],\n            ).make_converter_map(\n                [\n                    'numReq',\n                ],\n                map_type=int,\n                sort_keys=True,\n                buckets=[1, 2, 3, 4, 5, 10, 25, 50, 75, 100, 200, '...'],\n            ).make_converter_map(\n                [\n                    'avgTime',\n                ],\n                map_type=int,\n                sort_keys=True,\n                buckets=list(range(0, 6*1000, 100)) + ['...'],\n            ).make_converter_map(\n                [\n                    'siteName',\n                    'userID',\n                    'fileType',\n                    'dataType',\n                    'campain',\n                    'process',\n                ],\n                unknown_values=True,\n                map_type=str,\n            ).store_converter_map(\n                f\"featureConverter-window_{winIdx:02d}\"\n            ).make_data_and_labels(\n                [\n                    'siteName',\n                    'userID',\n                    'fileType',\n                    'dataType',\n                    'campain',\n                    'process',\n                    'numReq',\n                    'avgTime',\n                    'size',\n                ],\n                'class'\n            ).save_data_and_labels(\n                f\"dataset_converted-window_{winIdx:02d}\"\n            )\n            print(\n                f\"[Dataset created][Name: 'dataset_converted-window_{winIdx:02d}']\"\n            )"}
{"input": "def peek(self, draw_limb=True, draw_grid=False, gamma=None,\n                   colorbar=True, basic_plot=False, **matplot_args):\n        \"\"\"Displays the map in a new figure\n\n        Parameters\n        ----------\n        draw_limb : bool\n            Whether the solar limb should be plotted.\n        draw_grid : bool or number\n            Whether solar meridians and parallels are plotted. If float then sets\n            degree difference between parallels and meridians.\n        gamma : float\n            Gamma value to use for the color map\n        colorbar : bool\n            Whether to display a colorbar next to the plot\n        basic_plot : bool\n            If true, the data is plotted by itself at it's natural scale; no\n            title, labels, or axes are shown.\n        **matplot_args : dict\n            Matplotlib Any additional imshow arguments that should be used\n            when plotting the image.\n        \"\"\"\n\n        # Create a figure and add title and axes\n        figure = plt.figure(frameon=not basic_plot)\n\n        # Basic plot\n        if basic_plot:\n            axes = plt.Axes(figure, [0., 0., 1., 1.])\n            axes.set_axis_off()\n            figure.add_axes(axes)\n            matplot_args.update({'annotate':False})\n\n        # Normal plot\n        else:\n            axes = figure.gca()\n\n        im = self.plot(axes=axes,**matplot_args)\n\n        if colorbar and not basic_plot:\n            figure.colorbar(im)\n\n        if draw_limb:\n            self.draw_limb(axes=axes)\n\n        if isinstance(draw_grid, bool):\n            if draw_grid:\n                self.draw_grid(axes=axes)\n        elif isinstance(draw_grid, (int, long, float)):\n            self.draw_grid(axes=axes, grid_spacing=draw_grid)\n        else:\n            raise TypeError(\"draw_grid should be bool, int, long or float\")\n\n        figure.show()\n\n        return figure", "output": "def peek(self, draw_limb=False, draw_grid=False, gamma=None,\n                   colorbar=True, basic_plot=False, **matplot_args):\n        \"\"\"Displays the map in a new figure\n\n        Parameters\n        ----------\n        draw_limb : bool\n            Whether the solar limb should be plotted.\n        draw_grid : bool or number\n            Whether solar meridians and parallels are plotted. If float then sets\n            degree difference between parallels and meridians.\n        gamma : float\n            Gamma value to use for the color map\n        colorbar : bool\n            Whether to display a colorbar next to the plot\n        basic_plot : bool\n            If true, the data is plotted by itself at it's natural scale; no\n            title, labels, or axes are shown.\n        **matplot_args : dict\n            Matplotlib Any additional imshow arguments that should be used\n            when plotting the image.\n        \"\"\"\n\n        # Create a figure and add title and axes\n        figure = plt.figure(frameon=not basic_plot)\n\n        # Basic plot\n        if basic_plot:\n            axes = plt.Axes(figure, [0., 0., 1., 1.])\n            axes.set_axis_off()\n            figure.add_axes(axes)\n            matplot_args.update({'annotate':False})\n\n        # Normal plot\n        else:\n            axes = figure.gca()\n\n        im = self.plot(axes=axes,**matplot_args)\n\n        if colorbar and not basic_plot:\n            figure.colorbar(im)\n\n        if draw_limb:\n            self.draw_limb(axes=axes)\n\n        if isinstance(draw_grid, bool):\n            if draw_grid:\n                self.draw_grid(axes=axes)\n        elif isinstance(draw_grid, (int, long, float)):\n            self.draw_grid(axes=axes, grid_spacing=draw_grid)\n        else:\n            raise TypeError(\"draw_grid should be bool, int, long or float\")\n\n        figure.show()\n\n        return figure"}
{"input": "def dynare_import(filename,names_dict={},full_output=False, debug=True):\n    '''Imports model defined in specified file'''\n    import os\n    basename = os.path.basename(filename)\n    fname = re.compile('(.*)\\.(.*)').match(basename).group(1)\n    f = open(filename)\n    txt = f.read()\n    model = parse_dynare_text(txt,names_dict=names_dict,full_output=full_output, debug=debug)\n    model['name'] = fname\n    return model", "output": "def dynare_import(filename,names_dict={},full_output=False, debug=False):\n    '''Imports model defined in specified file'''\n    import os\n    basename = os.path.basename(filename)\n    fname = re.compile('(.*)\\.(.*)').match(basename).group(1)\n    f = open(filename)\n    txt = f.read()\n    model = parse_dynare_text(txt,names_dict=names_dict,full_output=full_output, debug=debug)\n    model['name'] = fname\n    return model"}
{"input": "def admin_post():\n    if settings.app.demo_mode:\n        return utils.demo_blocked()\n\n    if not flask.g.administrator.super_user:\n        return utils.jsonify({\n            'error': REQUIRES_SUPER_USER,\n            'error_msg': REQUIRES_SUPER_USER_MSG,\n        }, 400)\n\n    username = utils.filter_str(flask.request.json['username']).lower()\n    password = flask.request.json['password']\n    yubikey_id = flask.request.json.get('yubikey_id') or None\n    yubikey_id = yubikey_id[:12] if yubikey_id else None\n    otp_auth = flask.request.json.get('otp_auth', False)\n    auth_api = flask.request.json.get('auth_api', False)\n    disabled = flask.request.json.get('disabled', False)\n    super_user = flask.request.json.get('super_user', False)\n    remote_addr = utils.get_remote_addr()\n\n    try:\n        admin = auth.new_admin(\n            username=username,\n            password=password,\n            yubikey_id=yubikey_id,\n            default=True,\n            otp_auth=otp_auth,\n            auth_api=auth_api,\n            disabled=disabled,\n            super_user=super_user,\n        )\n    except pymongo.errors.DuplicateKeyError:\n        return utils.jsonify({\n            'error': ADMIN_USERNAME_EXISTS,\n            'error_msg': ADMIN_USERNAME_EXISTS_MSG,\n        }, 400)\n\n    admin.audit_event('admin_created',\n        'Administrator created',\n        remote_addr=remote_addr,\n    )\n\n    journal.entry(\n        journal.ADMIN_CREATE,\n        admin.journal_data,\n        event_long='Administrator created',\n        remote_addr=remote_addr,\n    )\n\n    event.Event(type=ADMINS_UPDATED)\n\n    return utils.jsonify(admin.dict())", "output": "def admin_post():\n    if settings.app.demo_mode:\n        return utils.demo_blocked()\n\n    if not flask.g.administrator.super_user:\n        return utils.jsonify({\n            'error': REQUIRES_SUPER_USER,\n            'error_msg': REQUIRES_SUPER_USER_MSG,\n        }, 400)\n\n    username = utils.filter_str(flask.request.json['username']).lower()\n    password = flask.request.json['password']\n    yubikey_id = flask.request.json.get('yubikey_id') or None\n    yubikey_id = yubikey_id[:12] if yubikey_id else None\n    otp_auth = flask.request.json.get('otp_auth', False)\n    auth_api = flask.request.json.get('auth_api', False)\n    disabled = flask.request.json.get('disabled', False)\n    super_user = flask.request.json.get('super_user', False)\n    remote_addr = utils.get_remote_addr()\n\n    try:\n        admin = auth.new_admin(\n            username=username,\n            password=password,\n            yubikey_id=yubikey_id,\n            default=False,\n            otp_auth=otp_auth,\n            auth_api=auth_api,\n            disabled=disabled,\n            super_user=super_user,\n        )\n    except pymongo.errors.DuplicateKeyError:\n        return utils.jsonify({\n            'error': ADMIN_USERNAME_EXISTS,\n            'error_msg': ADMIN_USERNAME_EXISTS_MSG,\n        }, 400)\n\n    admin.audit_event('admin_created',\n        'Administrator created',\n        remote_addr=remote_addr,\n    )\n\n    journal.entry(\n        journal.ADMIN_CREATE,\n        admin.journal_data,\n        event_long='Administrator created',\n        remote_addr=remote_addr,\n    )\n\n    event.Event(type=ADMINS_UPDATED)\n\n    return utils.jsonify(admin.dict())"}
{"input": "def main():\n   app.run(host=\"0.0.0.0\", debug=True)", "output": "def main():\n   app.run(host=\"0.0.0.0\", debug=False)"}
{"input": "def upgrade(full=True):\n    \"\"\"\n    Upgrade all packages.\n    \"\"\"\n    manager = MANAGER\n    cmds = {'pkgin': {False: 'ug', True: 'fug'}}\n    cmd = cmds[manager][full]\n    sudo(\"%(manager)s -y %(cmd)s\" % locals())", "output": "def upgrade(full=False):\n    \"\"\"\n    Upgrade all packages.\n    \"\"\"\n    manager = MANAGER\n    cmds = {'pkgin': {False: 'ug', True: 'fug'}}\n    cmd = cmds[manager][full]\n    sudo(\"%(manager)s -y %(cmd)s\" % locals())"}
{"input": "def empty_temp_db():\n    # create a temp database for testing\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        qc.config[\"core\"][\"db_location\"] = os.path.join(tmpdirname, 'temp.db')\n        qc.config[\"core\"][\"db_debug\"] = False\n        initialise_database()\n        yield", "output": "def empty_temp_db():\n    # create a temp database for testing\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        qc.config[\"core\"][\"db_location\"] = os.path.join(tmpdirname, 'temp.db')\n        qc.config[\"core\"][\"db_debug\"] = True\n        initialise_database()\n        yield"}
{"input": "def quality(wavelength: Union[Quantity, ndarray], flux: Union[Quantity, ndarray]) -> Union[\n    float64, Quantity]:\n    \"\"\"Calculation of the spectral Quality, Q, for a spectrum.\n\n    Parameters\n    ----------\n    wavelength: array-like or Quantity array\n        Wavelength of spectrum.\n    flux: array-like or Quantity array\n        Flux of spectrum.\n\n    Returns\n    -------\n    sqrt{sum{W(i)}}: float or Quantity scalar\n       Spectral quality\n\n    Notes\n    -----\n    Extract from https://arxiv.org/pdf/1511.07468v1.pdf\n\n        Q = sqrt{sum{W(i)}} / sqrt{sum{A_0{i}}\n\n    where, W(i), is the optimal pixel weights\n\n        W(i) = lambda(i)**2 (d'A_0(i) / d'lambda(i))**2 / (A_0(i) + sigma_D**2)\n\n    in which lambda(i) and A_0(i) are the values of each pixel wave-length and\n    flux, respectively. The weight will be proportional to the information\n    content of the spectrum, given by the derivative of the amplitude, and\n    calculated following Connes (1985).\n\n    The spectral quality, Q, is indpendant of the flux level and is only\n    a function of the spectral profile.\n\n    \"\"\"\n    if not isinstance(wavelength, np.ndarray):\n        print(\"Your wavelength and flux should really be numpy arrays! Converting them here.\")\n        wavelength = np.asarray(wavelength)\n    if not isinstance(flux, np.ndarray):\n        flux = np.asarray(flux)\n    \n    flux = flux = u.dimensionless_unscaled # Turn into Quantity if not already\n    flux = flux / flux.unit  # Remove units from flux (sqrt(N_e) is unitless)\n\n    wis = sqrt_sum_wis(wavelength, flux)\n\n    return wis / np.sqrt(np.sum(flux))", "output": "def quality(wavelength: Union[Quantity, ndarray], flux: Union[Quantity, ndarray]) -> Union[\n    float64, Quantity]:\n    \"\"\"Calculation of the spectral Quality, Q, for a spectrum.\n\n    Parameters\n    ----------\n    wavelength: array-like or Quantity array\n        Wavelength of spectrum.\n    flux: array-like or Quantity array\n        Flux of spectrum.\n\n    Returns\n    -------\n    sqrt{sum{W(i)}}: float or Quantity scalar\n       Spectral quality\n\n    Notes\n    -----\n    Extract from https://arxiv.org/pdf/1511.07468v1.pdf\n\n        Q = sqrt{sum{W(i)}} / sqrt{sum{A_0{i}}\n\n    where, W(i), is the optimal pixel weights\n\n        W(i) = lambda(i)**2 (d'A_0(i) / d'lambda(i))**2 / (A_0(i) + sigma_D**2)\n\n    in which lambda(i) and A_0(i) are the values of each pixel wave-length and\n    flux, respectively. The weight will be proportional to the information\n    content of the spectrum, given by the derivative of the amplitude, and\n    calculated following Connes (1985).\n\n    The spectral quality, Q, is indpendant of the flux level and is only\n    a function of the spectral profile.\n\n    \"\"\"\n    if not isinstance(wavelength, np.ndarray):\n        print(\"Your wavelength and flux should really be numpy arrays! Converting them here.\")\n        wavelength = np.asarray(wavelength)\n    if not isinstance(flux, np.ndarray):\n        flux = np.asarray(flux)\n    \n    flux = flux * u.dimensionless_unscaled # Turn into Quantity if not already\n    flux = flux / flux.unit  # Remove units from flux (sqrt(N_e) is unitless)\n\n    wis = sqrt_sum_wis(wavelength, flux)\n\n    return wis / np.sqrt(np.sum(flux))"}
{"input": "def __test(infile):\n        DATA = load(infile)\n\n        # Load the file\n        (y, sr) = librosa.load(DATA['wavfile'][0], sr=None, mono=True)\n\n        if DATA['hann_w'][0, 0] == 0:\n            # Set window to ones, swap back to nfft\n            window = np.ones\n            win_length = None\n\n        else:\n            window = None\n            win_length = DATA['hann_w'][0, 0]\n\n        # Compute the STFT\n        D = librosa.stft(y,\n                         n_fft=DATA['nfft'][0, 0].astype(int),\n                         hop_length=DATA['hop_length'][0, 0].astype(int),\n                         win_length=win_length,\n                         window=window,\n                         center=False)\n\n        assert np.allclose(D, DATA['D'])", "output": "def __test(infile):\n        DATA = load(infile)\n\n        # Load the file\n        (y, sr) = librosa.load(DATA['wavfile'][0], sr=None, mono=True)\n\n        if DATA['hann_w'][0, 0] == 0:\n            # Set window to ones, swap back to nfft\n            window = np.ones\n            win_length = None\n\n        else:\n            window = 'hann'\n            win_length = DATA['hann_w'][0, 0]\n\n        # Compute the STFT\n        D = librosa.stft(y,\n                         n_fft=DATA['nfft'][0, 0].astype(int),\n                         hop_length=DATA['hop_length'][0, 0].astype(int),\n                         win_length=win_length,\n                         window=window,\n                         center=False)\n\n        assert np.allclose(D, DATA['D'])"}
{"input": "def fill_graphs(self):\n        # bin size calculations\n        minimum, maximum = stats.describe(self.data_prices, bias=False, nan_policy=\"omit\")[1]\n        minimum -= .05\n        maximum -= .05\n        difference = maximum - minimum\n        bin_size = difference / self.bins\n\n        # counts for every bin\n        counts = [0] * (self.bins + 1)\n        for price in self.data_prices:\n            counts[int((price - minimum) / bin_size)] += 1\n\n        # puts the bin data into the sheet\n        for cell_index in range(0, self.bins):\n            bin_name = minimum + (((cell_index * bin_size) + ((cell_index + 1) * bin_size)) / 2)\n            self.sheet[\"D\" + str(cell_index + 9)] = bin_name\n            self.sheet[\"E\" + str(cell_index + 9)] = counts[cell_index]\n\n        # puts in the bar chart\n        bar_chart = openpyxl.chart.BarChart()\n        bar_chart.shape = 4\n        bar_chart.type = \"col\"\n        bar_chart.style = 10\n        bar_chart.title = self.sheet.title + \" HISTOGRAM\"\n        bar_chart.x_axis_title = \"BIN AVERAGE\"\n        bar_chart.y_axis_title = \"FREQUENCY\"\n        bar_data = openpyxl.chart.Reference(self.sheet, min_col = 5, min_row = 8, max_row = 9 + self.bins - 1, max_col = 5)\n        bar_categories = openpyxl.chart.Reference(self.sheet, min_col = 4, min_row = 9, max_row = 9 + self.bins - 1)\n        bar_chart.add_data(bar_data, titles_from_data = True)\n        bar_chart.set_categories(bar_categories)\n        self.sheet.add_chart(bar_chart, \"G4\")\n\n        # puts in the 3 month line chart\n        line_chart = openpyxl.chart.LineChart()\n        line_chart.style = 12\n        line_chart.title = self.sheet.title + \" LINECHART\"\n        line_chart.x_axis_title = \"DATE\"\n        line_chart.y_axis_title = \"PRICE\"\n        line_data = openpyxl.chart.Reference(self.sheet, min_col = 2, min_row = 1, max_row = 1 + len(self.data_prices))\n        line_categories = openpyxl.chart.Reference(self.sheet, min_col = 1, min_row = 2, max_row = 1 + len(self.data_prices))\n        line_chart.add_data(line_data)\n        line_chart.set_categories(line_categories)\n        # style the line chart\n        style = line_chart.series[0]\n        style.graphicalProperties.line.solidFill = \"00AAAA\"\n        style.graphicalProperties.line.dashStyle = \"sysDot\"\n        style.graphicalProperties.line.width = 100050\n        self.sheet.add_chart(line_chart, \"G22\")", "output": "def fill_graphs(self):\n        # bin size calculations\n        minimum, maximum = stats.describe(self.data_prices, bias=False, nan_policy=\"omit\")[1]\n        minimum -= .05\n        maximum += .05\n        difference = maximum - minimum\n        bin_size = difference / self.bins\n\n        # counts for every bin\n        counts = [0] * (self.bins + 1)\n        for price in self.data_prices:\n            counts[int((price - minimum) / bin_size)] += 1\n\n        # puts the bin data into the sheet\n        for cell_index in range(0, self.bins):\n            bin_name = minimum + (((cell_index * bin_size) + ((cell_index + 1) * bin_size)) / 2)\n            self.sheet[\"D\" + str(cell_index + 9)] = bin_name\n            self.sheet[\"E\" + str(cell_index + 9)] = counts[cell_index]\n\n        # puts in the bar chart\n        bar_chart = openpyxl.chart.BarChart()\n        bar_chart.shape = 4\n        bar_chart.type = \"col\"\n        bar_chart.style = 10\n        bar_chart.title = self.sheet.title + \" HISTOGRAM\"\n        bar_chart.x_axis_title = \"BIN AVERAGE\"\n        bar_chart.y_axis_title = \"FREQUENCY\"\n        bar_data = openpyxl.chart.Reference(self.sheet, min_col = 5, min_row = 8, max_row = 9 + self.bins - 1, max_col = 5)\n        bar_categories = openpyxl.chart.Reference(self.sheet, min_col = 4, min_row = 9, max_row = 9 + self.bins - 1)\n        bar_chart.add_data(bar_data, titles_from_data = True)\n        bar_chart.set_categories(bar_categories)\n        self.sheet.add_chart(bar_chart, \"G4\")\n\n        # puts in the 3 month line chart\n        line_chart = openpyxl.chart.LineChart()\n        line_chart.style = 12\n        line_chart.title = self.sheet.title + \" LINECHART\"\n        line_chart.x_axis_title = \"DATE\"\n        line_chart.y_axis_title = \"PRICE\"\n        line_data = openpyxl.chart.Reference(self.sheet, min_col = 2, min_row = 1, max_row = 1 + len(self.data_prices))\n        line_categories = openpyxl.chart.Reference(self.sheet, min_col = 1, min_row = 2, max_row = 1 + len(self.data_prices))\n        line_chart.add_data(line_data)\n        line_chart.set_categories(line_categories)\n        # style the line chart\n        style = line_chart.series[0]\n        style.graphicalProperties.line.solidFill = \"00AAAA\"\n        style.graphicalProperties.line.dashStyle = \"sysDot\"\n        style.graphicalProperties.line.width = 100050\n        self.sheet.add_chart(line_chart, \"G22\")"}
{"input": "def __init__(self, **kwargs):\n        \"\"\"\n        :param kwargs: allow to overwrite a default value.\n                       It mainly used in unit tests\n\n        To define a new default value just add an attribute with the default value\n        \"\"\"\n        super().__init__()\n        self.__dict__ = self\n        if __MACSY_DATA__ == '$' + 'MACSYDATA':\n            prefix_data = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', 'data'))\n        else:\n            prefix_data = os.path.join(__MACSY_DATA__, 'data')\n        self.cfg_file = kwargs.get('cfg_file', None)\n        self.coverage_profile = kwargs.get('coverage_profile', 0.5)\n        self.e_value_search = kwargs.get('e_value_search', None)\n        self.cut_ga = kwargs.get('cut_ga', True)\n        self.db_type = kwargs.get('db_type', None)\n        self.hmmer = kwargs.get('hmmer', 'hmmsearch')\n        self.i_evalue_sel = kwargs.get('i_evalue_sel', 0.001)\n        self.idx = kwargs.get('idx', False)\n        self.inter_gene_max_space = kwargs.get('inter_gene_max_space', None)\n        self.log_level = kwargs.get('log_level', logging.INFO)\n        self.log_file = kwargs.get('log_file', 'macsyfinder.log')\n        self.max_nb_genes = kwargs.get('max_nb_genes', None)\n        self.min_genes_required = kwargs.get('min_genes_required', None)\n        self.min_mandatory_genes_required = kwargs.get('min_mandatory_genes_required', None)\n        self.models = kwargs.get('models', [])\n        self.models_dir = kwargs.get('models_dir', os.path.join(prefix_data, 'models'))\n        self.multi_loci = kwargs.get('multi_loci', set())\n        self.mute = kwargs.get('mute', False)\n        self.out_dir = kwargs.get('out_dir', None)\n        self.previous_run = kwargs.get('previous_run', False)\n        self.profile_suffix = kwargs.get('profile_suffix', '.hmm')\n        self.quiet = kwargs.get('quiet', 0)\n        self.relative_path = kwargs.get('relative_path', False)\n        self.replicon_topology = kwargs.get('replicon_topology', 'circular')\n        self.res_extract_suffix = kwargs.get('res_extract_suffix', '.res_hmm_extract')\n        self.res_search_dir = kwargs.get('res_search_dir', os.getcwd())\n        self.res_search_suffix = kwargs.get('res_search_suffix', '.search_hmm.out')\n        self.sequence_db = kwargs.get('sequence_db', None)\n        self.topology_file = kwargs.get('topology_file', None)\n        self.verbosity = kwargs.get('verbosity', 0)\n        self.worker = kwargs.get('worker', 1)", "output": "def __init__(self, **kwargs):\n        \"\"\"\n        :param kwargs: allow to overwrite a default value.\n                       It mainly used in unit tests\n\n        To define a new default value just add an attribute with the default value\n        \"\"\"\n        super().__init__()\n        self.__dict__ = self\n        if __MACSY_DATA__ == '$' + 'MACSYDATA':\n            prefix_data = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', 'data'))\n        else:\n            prefix_data = os.path.join(__MACSY_DATA__, 'data')\n        self.cfg_file = kwargs.get('cfg_file', None)\n        self.coverage_profile = kwargs.get('coverage_profile', 0.5)\n        self.e_value_search = kwargs.get('e_value_search', 0.1)\n        self.cut_ga = kwargs.get('cut_ga', True)\n        self.db_type = kwargs.get('db_type', None)\n        self.hmmer = kwargs.get('hmmer', 'hmmsearch')\n        self.i_evalue_sel = kwargs.get('i_evalue_sel', 0.001)\n        self.idx = kwargs.get('idx', False)\n        self.inter_gene_max_space = kwargs.get('inter_gene_max_space', None)\n        self.log_level = kwargs.get('log_level', logging.INFO)\n        self.log_file = kwargs.get('log_file', 'macsyfinder.log')\n        self.max_nb_genes = kwargs.get('max_nb_genes', None)\n        self.min_genes_required = kwargs.get('min_genes_required', None)\n        self.min_mandatory_genes_required = kwargs.get('min_mandatory_genes_required', None)\n        self.models = kwargs.get('models', [])\n        self.models_dir = kwargs.get('models_dir', os.path.join(prefix_data, 'models'))\n        self.multi_loci = kwargs.get('multi_loci', set())\n        self.mute = kwargs.get('mute', False)\n        self.out_dir = kwargs.get('out_dir', None)\n        self.previous_run = kwargs.get('previous_run', False)\n        self.profile_suffix = kwargs.get('profile_suffix', '.hmm')\n        self.quiet = kwargs.get('quiet', 0)\n        self.relative_path = kwargs.get('relative_path', False)\n        self.replicon_topology = kwargs.get('replicon_topology', 'circular')\n        self.res_extract_suffix = kwargs.get('res_extract_suffix', '.res_hmm_extract')\n        self.res_search_dir = kwargs.get('res_search_dir', os.getcwd())\n        self.res_search_suffix = kwargs.get('res_search_suffix', '.search_hmm.out')\n        self.sequence_db = kwargs.get('sequence_db', None)\n        self.topology_file = kwargs.get('topology_file', None)\n        self.verbosity = kwargs.get('verbosity', 0)\n        self.worker = kwargs.get('worker', 1)"}
{"input": "def add_edge(self, edge, wt=1, label=\"\"):\n        \"\"\"\n        Add an edge to the graph connecting two nodes.\n        An edge, here, is a pair of node like C(m, n) or a tuple\n        with m as head and n as tail :  m -> n\n        \"\"\"\n        u, v = edge\n        if (v not in self.node_neighbors[u]):\n            self.node_neighbors[u][v] = wt\n        else:\n            raise Exception(\"Edge (%s, %s) already added in the graph\" % (u, v))", "output": "def add_edge(self, edge, wt=DEFAULT_WEIGHT, label=\"\"):\n        \"\"\"\n        Add an edge to the graph connecting two nodes.\n        An edge, here, is a pair of node like C(m, n) or a tuple\n        with m as head and n as tail :  m -> n\n        \"\"\"\n        u, v = edge\n        if (v not in self.node_neighbors[u]):\n            self.node_neighbors[u][v] = wt\n        else:\n            raise Exception(\"Edge (%s, %s) already added in the graph\" % (u, v))"}
{"input": "def stream_data_on(self, write_config=True, gcp_mode=True):\n        \"\"\"\n        Turns on streaming data.\n\n        Opt Args:\n        ---------\n        gcp_mode (bool) : Determines whether to write data using the \n            smurf2mce (gcp) mode. Default is True.\n\n        Returns:\n        --------\n        data_filename (string): The fullpath to where the data is stored\n        \"\"\"\n        bands = self.config.get('init').get('bands')\n        \n        # Check if flux ramp is non-zero\n        ramp_max_cnt = self.get_ramp_max_cnt()\n        if ramp_max_cnt == 0:\n            self.log('Flux ramp frequency is zero. Cannot take data.', \n                self.LOG_ERROR)\n        else:\n            # check which flux ramp relay state we're in\n            # read_ac_dc_relay_status() should be 0 in DC mode, 3 in\n            # AC mode.  this check is only possible if you're using\n            # one of the newer C02 cryostat cards.\n            flux_ramp_ac_dc_relay_status=self.C.read_ac_dc_relay_status()\n            if flux_ramp_ac_dc_relay_status == 0:\n                self.log(\"FLUX RAMP IS DC COUPLED.  HOPEFULLY THAT'S WHAT YOU WERE EXPECTING.\".format(flux_ramp_ac_dc_relay_status), self.LOG_USER)\n            elif flux_ramp_ac_dc_relay_status == 3:\n                self.log(\"Flux ramp is AC-coupled.\".format(flux_ramp_ac_dc_relay_status), self.LOG_USER)\n            else:\n                self.log(\"flux_ramp_ac_dc_relay_status = {} - NOT A VALID STATE.\".format(flux_ramp_ac_dc_relay_status), self.LOG_ERROR)\n            \n            # start streaming before opening file to avoid transient filter step\n            self.set_stream_enable(1, write_log=False)\n            time.sleep(1.)\n\n            # Make the data file\n            timestamp = self.get_timestamp()\n            data_filename = os.path.join(self.output_dir, timestamp+'.dat')\n\n            # Optionally write PyRogue configuration\n            if write_config:\n                config_filename=os.path.join(self.output_dir, timestamp+'.yml')\n                self.log('Writing PyRogue configuration to file : {}'.format(config_filename), \n                     self.LOG_USER)\n                self.write_config(config_filename)\n                # short wait\n                time.sleep(5.)\n\n            self.log('Writing to file : {}'.format(data_filename), \n                self.LOG_USER)\n            if gcp_mode:\n                ret = self.make_smurf_to_gcp_config(filename=data_filename)\n                smurf_chans = {}\n                for b in bands:\n                    smurf_chans[b] = self.which_on(b)\n                self.make_gcp_mask(smurf_chans=smurf_chans)\n                shutil.copy(self.smurf_to_mce_mask_file,\n                            os.path.join(self.output_dir, timestamp+'_mask.txt'))\n                self.read_smurf_to_gcp_config()\n            else:\n                self.set_streaming_datafile(data_filename)\n\n            if gcp_mode:\n                self.set_smurf_to_gcp_writer(True, write_log=True)\n            else:\n                self.set_streaming_file_open(1)  # Open the file\n\n            return data_filename", "output": "def stream_data_on(self, write_config=False, gcp_mode=True):\n        \"\"\"\n        Turns on streaming data.\n\n        Opt Args:\n        ---------\n        gcp_mode (bool) : Determines whether to write data using the \n            smurf2mce (gcp) mode. Default is True.\n\n        Returns:\n        --------\n        data_filename (string): The fullpath to where the data is stored\n        \"\"\"\n        bands = self.config.get('init').get('bands')\n        \n        # Check if flux ramp is non-zero\n        ramp_max_cnt = self.get_ramp_max_cnt()\n        if ramp_max_cnt == 0:\n            self.log('Flux ramp frequency is zero. Cannot take data.', \n                self.LOG_ERROR)\n        else:\n            # check which flux ramp relay state we're in\n            # read_ac_dc_relay_status() should be 0 in DC mode, 3 in\n            # AC mode.  this check is only possible if you're using\n            # one of the newer C02 cryostat cards.\n            flux_ramp_ac_dc_relay_status=self.C.read_ac_dc_relay_status()\n            if flux_ramp_ac_dc_relay_status == 0:\n                self.log(\"FLUX RAMP IS DC COUPLED.  HOPEFULLY THAT'S WHAT YOU WERE EXPECTING.\".format(flux_ramp_ac_dc_relay_status), self.LOG_USER)\n            elif flux_ramp_ac_dc_relay_status == 3:\n                self.log(\"Flux ramp is AC-coupled.\".format(flux_ramp_ac_dc_relay_status), self.LOG_USER)\n            else:\n                self.log(\"flux_ramp_ac_dc_relay_status = {} - NOT A VALID STATE.\".format(flux_ramp_ac_dc_relay_status), self.LOG_ERROR)\n            \n            # start streaming before opening file to avoid transient filter step\n            self.set_stream_enable(1, write_log=False)\n            time.sleep(1.)\n\n            # Make the data file\n            timestamp = self.get_timestamp()\n            data_filename = os.path.join(self.output_dir, timestamp+'.dat')\n\n            # Optionally write PyRogue configuration\n            if write_config:\n                config_filename=os.path.join(self.output_dir, timestamp+'.yml')\n                self.log('Writing PyRogue configuration to file : {}'.format(config_filename), \n                     self.LOG_USER)\n                self.write_config(config_filename)\n                # short wait\n                time.sleep(5.)\n\n            self.log('Writing to file : {}'.format(data_filename), \n                self.LOG_USER)\n            if gcp_mode:\n                ret = self.make_smurf_to_gcp_config(filename=data_filename)\n                smurf_chans = {}\n                for b in bands:\n                    smurf_chans[b] = self.which_on(b)\n                self.make_gcp_mask(smurf_chans=smurf_chans)\n                shutil.copy(self.smurf_to_mce_mask_file,\n                            os.path.join(self.output_dir, timestamp+'_mask.txt'))\n                self.read_smurf_to_gcp_config()\n            else:\n                self.set_streaming_datafile(data_filename)\n\n            if gcp_mode:\n                self.set_smurf_to_gcp_writer(True, write_log=True)\n            else:\n                self.set_streaming_file_open(1)  # Open the file\n\n            return data_filename"}
{"input": "def request(self, *args, **kwargs):\n        r = super(BrowserSession, self).request(*args, **kwargs)\n\n        r._content = self.render(r.text).encode(DEFAULT_ENCODING)\n        r.encoding = 'utf-8'\n\n        r.html = HTML(url=r.url, html=r.text, default_encoding=r.encoding)\n\n        return r", "output": "def request(self, *args, **kwargs):\n        r = super(BrowserSession, self).request(*args, **kwargs)\n\n        r._content = self.render(r.text).encode(DEFAULT_ENCODING)\n        r.encoding = DEFAULT_ENCODING\n\n        r.html = HTML(url=r.url, html=r.text, default_encoding=r.encoding)\n\n        return r"}
{"input": "def __init__(self):\n        self.state = RUNNING\n        self.state_lock = threading.Lock()\n        self.running = threading.Event()\n        self.running.set()\n        self.last_check = time.time()\n        self.processes = []", "output": "def __init__(self):\n        self.state = None\n        self.state_lock = threading.Lock()\n        self.running = threading.Event()\n        self.running.set()\n        self.last_check = time.time()\n        self.processes = []"}
{"input": "def get_percentiles(results,sim_number=None):\n    \"\"\"\n    Get 5,25,50,75 and 95 percentiles of your simulations\n    \n    :results: Expects an numpy array which should of an index \"simulation\" for simulations \n    :type: array    \n    \n    :sim_number: Optional, Number of your simulation, needed when working with multiple lists of simulations\n    :type: int\n    \n    :return: Percentiles of simulations \n    :rtype: int and float\n    \"\"\" \n    p5,p25,p50,p75,p95=[],[],[],[],[]\n    fields=[word for word in results.dtype.names if word.startswith('simulation'+str(sim_number))]\n    for i in range(len(fields)):\n        p5.append(np.percentile(list(results[fields[i]]),5))\n        p25.append(np.percentile(list(results[fields[i]]),25))\n        p50.append(np.percentile(list(results[fields[i]]),50))\n        p75.append(np.percentile(list(results[fields[i]]),75))    \n        p95.append(np.percentile(list(results[fields[i]]),95))\n    return p5,p25,p50,p75,p95", "output": "def get_percentiles(results,sim_number=''):\n    \"\"\"\n    Get 5,25,50,75 and 95 percentiles of your simulations\n    \n    :results: Expects an numpy array which should of an index \"simulation\" for simulations \n    :type: array    \n    \n    :sim_number: Optional, Number of your simulation, needed when working with multiple lists of simulations\n    :type: int\n    \n    :return: Percentiles of simulations \n    :rtype: int and float\n    \"\"\" \n    p5,p25,p50,p75,p95=[],[],[],[],[]\n    fields=[word for word in results.dtype.names if word.startswith('simulation'+str(sim_number))]\n    for i in range(len(fields)):\n        p5.append(np.percentile(list(results[fields[i]]),5))\n        p25.append(np.percentile(list(results[fields[i]]),25))\n        p50.append(np.percentile(list(results[fields[i]]),50))\n        p75.append(np.percentile(list(results[fields[i]]),75))    \n        p95.append(np.percentile(list(results[fields[i]]),95))\n    return p5,p25,p50,p75,p95"}
{"input": "def confirm(self, question):\n        while True:\n            with self._log_lock:\n                if self._last_log_prefix != None:\n                    sys.stderr.write(\"\\n\")\n                    self._last_log_prefix = None\n                sys.stderr.write(\"warning: {0} (y/N) \".format(question))\n                if self.auto_response != None:\n                    sys.stderr.write(\"{0}\\n\".format(self.auto_response))\n                    return self.auto_response == \"y\"\n                response = sys.stdin.readline()\n                if response == \"\": return False\n                response = response.rstrip().lower()\n                if response == \"y\": return True\n                if response == \"n\": return False", "output": "def confirm(self, question):\n        while True:\n            with self._log_lock:\n                if self._last_log_prefix != None:\n                    sys.stderr.write(\"\\n\")\n                    self._last_log_prefix = None\n                sys.stderr.write(\"warning: {0} (y/N) \".format(question))\n                if self.auto_response != None:\n                    sys.stderr.write(\"{0}\\n\".format(self.auto_response))\n                    return self.auto_response == \"y\"\n                response = sys.stdin.readline()\n                if response == \"\": return False\n                response = response.rstrip().lower()\n                if response == \"y\": return True\n                if response == \"n\" or response == \"\": return False"}
{"input": "def _get_next_addr_to_search(self, alignment=None):\n        # TODO: Take care of those functions that are already generated\n        curr_addr = self._next_addr\n        # Determine the size of that IRSB\n        # Note: we don't care about SimProcedure at this moment, as we want to\n        # get as many functions as possible\n        # s_irsb = None\n        # while s_irsb is None:\n        #     s_ex = self._project.exit_to(addr=curr_addr, \\\n        #                     state=self._project.initial_state(mode=\"static\"))\n        #     try:\n        #         s_irsb = self._project.sim_block(s_ex)\n        #     except simuvex.s_irsb.SimIRSBError:\n        #         # We cannot build functions there\n        #         # Move on to next possible position\n        #         s_irsb = None\n        #         # TODO: Handle strings\n        #         curr_addr = \\\n        #             self._seg_list.next_free_pos(curr_addr)\n        if self._seg_list.has_blocks:\n            curr_addr = self._seg_list.next_free_pos(curr_addr)\n\n        if alignment is not None:\n            if curr_addr % alignment > 0:\n                curr_addr = curr_addr - curr_addr % alignment + alignment\n        # block_size = s_irsb.irsb.size()\n        # self._next_addr = curr_addr + block_size\n        self._next_addr = curr_addr\n        if curr_addr < self._ending_point:\n            l.debug(\"Returning new recon address: 0x%08x\", curr_addr)\n            return curr_addr\n        else:\n            l.debug(\"0x%08x is beyond the ending point.\", curr_addr)\n            return None", "output": "def _get_next_addr_to_search(self, alignment=None):\n        # TODO: Take care of those functions that are already generated\n        curr_addr = self._next_addr\n        # Determine the size of that IRSB\n        # Note: we don't care about SimProcedure at this moment, as we want to\n        # get as many functions as possible\n        # s_irsb = None\n        # while s_irsb is None:\n        #     s_ex = self._project.exit_to(addr=curr_addr, \\\n        #                     state=self._project.initial_state(mode=\"static\"))\n        #     try:\n        #         s_irsb = self._project.sim_block(s_ex)\n        #     except simuvex.s_irsb.SimIRSBError:\n        #         # We cannot build functions there\n        #         # Move on to next possible position\n        #         s_irsb = None\n        #         # TODO: Handle strings\n        #         curr_addr = \\\n        #             self._seg_list.next_free_pos(curr_addr)\n        if self._seg_list.has_blocks:\n            curr_addr = self._seg_list.next_free_pos(curr_addr)\n\n        if alignment is not None:\n            if curr_addr % alignment > 0:\n                curr_addr = curr_addr - curr_addr % alignment + alignment\n        # block_size = s_irsb.irsb.size()\n        # self._next_addr = curr_addr + block_size\n        self._next_addr = curr_addr\n        if self._ending_point is None or curr_addr < self._ending_point:\n            l.debug(\"Returning new recon address: 0x%08x\", curr_addr)\n            return curr_addr\n        else:\n            l.debug(\"0x%08x is beyond the ending point.\", curr_addr)\n            return None"}
{"input": "def _fix_subord_punct(self, node):\n        # Dot used as the ordinal-number marker (in some languages) or abbreviation marker.\n        # TODO: detect these cases somehow\n        # Numbers can be detected with `node.parent.form.isdigit()`,\n        # but abbreviations are more tricky because the Abbr=Yes feature is not always used.\n        if node.form == '.' and node.parent == node.prev_node:\n            return\n\n        # Even non-paired punctuation like commas and dashes may work as paired.\n        # Detect such cases and try to preserve, but only if projective.\n        p_desc = node.parent.descendants(add_self=1)\n        if node in (p_desc[0], p_desc[-1]) and len(p_desc) == p_desc[-1].ord - p_desc[0].ord + 1:\n            if (p_desc[0].upos == 'PUNCT' and p_desc[-1].upos == 'PUNCT'\n                    and p_desc[0].parent == node.parent and p_desc[-1].parent == node.parent):\n                return\n\n        # Initialize the candidates (left and right) with the nearest nodes excluding punctuation.\n        # Final punctuation should not be attached to any following, so exclude r_cand there.\n        l_cand, r_cand = node.prev_node, node.next_node\n        if node.form in FINAL_PUNCT:\n            r_cand = None\n        while l_cand.ord > 0 and l_cand.upos == \"PUNCT\":\n            if self._punct_type[l_cand.ord] == 'opening':\n                l_cand = None\n                break\n            l_cand = l_cand.prev_node\n        while r_cand is not None and r_cand.upos == \"PUNCT\":\n            if self._punct_type[r_cand.ord] == 'closing':\n                r_cand = None\n                break\n            r_cand = r_cand.next_node\n\n        # Climb up from the candidates, until we would reach the root or \"cross\" the punctuation.\n        # If the candidates' descendants span across the punctuation, we also stop\n        # because climbing higher would cause a non-projectivity (the punct would be the gap).\n        l_path, r_path = [l_cand], [r_cand]\n        if l_cand.is_root():\n            l_cand = None\n        else:\n            while (not l_cand.parent.is_root() and l_cand.parent.precedes(node)\n                   and not node.precedes(l_cand.descendants(add_self=1)[-1])):\n                l_cand = l_cand.parent\n                l_path.append(l_cand)\n        if r_cand is not None:\n            while (not r_cand.parent.is_root() and node.precedes(r_cand.parent)\n                   and not r_cand.descendants(add_self=1)[0].precedes(node)):\n                r_cand = r_cand.parent\n                r_path.append(r_cand)\n\n        # Now select between l_cand and r_cand -- which will be the new parent?\n        # The lower one. Note that if neither is descendant of the other and neither is None\n        # (which can happen in rare non-projective cases), we arbitrarily prefer l_cand,\n        # but if the original parent is either on l_path or r_path, we keep it as acceptable.\n        if l_cand is not None and l_cand.is_descendant_of(r_cand):\n            cand, path = l_cand, l_path\n        elif r_cand is not None and r_cand.is_descendant_of(l_cand):\n            cand, path = r_cand, r_path\n        elif l_cand is not None:\n            cand, path = l_cand, l_path + r_path\n        elif r_cand is not None:\n            cand, path = r_cand, l_path + r_path\n        else:\n            return\n\n        # The guidelines say:\n        #    Within the relevant unit, a punctuation mark is attached\n        #    at the highest possible node that preserves projectivity.\n        # However, sometimes it is difficult to detect the unit (and its head).\n        # E.g. in \"Der Mann, den Sie gestern kennengelernt haben, kam wieder.\"\n        # the second comma should depend on \"kennengelernt\", not on \"Mann\"\n        # because the unit is just the relative clause.\n        # We try to be conservative and keep the parent, unless we are sure it is wrong.\n        if node.parent not in path:\n            node.parent = cand\n        node.deprel = \"punct\"", "output": "def _fix_subord_punct(self, node):\n        # Dot used as the ordinal-number marker (in some languages) or abbreviation marker.\n        # TODO: detect these cases somehow\n        # Numbers can be detected with `node.parent.form.isdigit()`,\n        # but abbreviations are more tricky because the Abbr=Yes feature is not always used.\n        if node.form == '.' and node.parent == node.prev_node:\n            return\n\n        # Even non-paired punctuation like commas and dashes may work as paired.\n        # Detect such cases and try to preserve, but only if projective.\n        p_desc = node.parent.descendants(add_self=1)\n        if node in (p_desc[0], p_desc[-1]) and len(p_desc) == p_desc[-1].ord - p_desc[0].ord + 1:\n            if (p_desc[0].upos == 'PUNCT' and p_desc[-1].upos == 'PUNCT'\n                    and p_desc[0].parent == node.parent and p_desc[-1].parent == node.parent):\n                return\n\n        # Initialize the candidates (left and right) with the nearest nodes excluding punctuation.\n        # Final punctuation should not be attached to any following, so exclude r_cand there.\n        l_cand, r_cand = node.prev_node, node.next_node\n        if node.form in FINAL_PUNCT:\n            r_cand = None\n        while l_cand.ord > 0 and l_cand.upos == \"PUNCT\":\n            if self._punct_type[l_cand.ord] == 'opening':\n                l_cand = None\n                break\n            l_cand = l_cand.prev_node\n        while r_cand is not None and r_cand.upos == \"PUNCT\":\n            if self._punct_type[r_cand.ord] == 'closing':\n                r_cand = None\n                break\n            r_cand = r_cand.next_node\n\n        # Climb up from the candidates, until we would reach the root or \"cross\" the punctuation.\n        # If the candidates' descendants span across the punctuation, we also stop\n        # because climbing higher would cause a non-projectivity (the punct would be the gap).\n        l_path, r_path = [l_cand], [r_cand]\n        if l_cand is None or l_cand.is_root():\n            l_cand = None\n        else:\n            while (not l_cand.parent.is_root() and l_cand.parent.precedes(node)\n                   and not node.precedes(l_cand.descendants(add_self=1)[-1])):\n                l_cand = l_cand.parent\n                l_path.append(l_cand)\n        if r_cand is not None:\n            while (not r_cand.parent.is_root() and node.precedes(r_cand.parent)\n                   and not r_cand.descendants(add_self=1)[0].precedes(node)):\n                r_cand = r_cand.parent\n                r_path.append(r_cand)\n\n        # Now select between l_cand and r_cand -- which will be the new parent?\n        # The lower one. Note that if neither is descendant of the other and neither is None\n        # (which can happen in rare non-projective cases), we arbitrarily prefer l_cand,\n        # but if the original parent is either on l_path or r_path, we keep it as acceptable.\n        if l_cand is not None and l_cand.is_descendant_of(r_cand):\n            cand, path = l_cand, l_path\n        elif r_cand is not None and r_cand.is_descendant_of(l_cand):\n            cand, path = r_cand, r_path\n        elif l_cand is not None:\n            cand, path = l_cand, l_path + r_path\n        elif r_cand is not None:\n            cand, path = r_cand, l_path + r_path\n        else:\n            return\n\n        # The guidelines say:\n        #    Within the relevant unit, a punctuation mark is attached\n        #    at the highest possible node that preserves projectivity.\n        # However, sometimes it is difficult to detect the unit (and its head).\n        # E.g. in \"Der Mann, den Sie gestern kennengelernt haben, kam wieder.\"\n        # the second comma should depend on \"kennengelernt\", not on \"Mann\"\n        # because the unit is just the relative clause.\n        # We try to be conservative and keep the parent, unless we are sure it is wrong.\n        if node.parent not in path:\n            node.parent = cand\n        node.deprel = \"punct\""}
{"input": "def mobile(request, template=None):\n    if not request.MOBILE:\n        return redirect_to(\n            request, 'products.product', slug='mobile', permanent=False)\n\n    docs = MOBILE_DOCS_FOR_MOBILE\n    return jingo.render(request, template,\n                        _data(docs, request.locale, 'mobile', 'mobile'))", "output": "def mobile(request, template=None):\n    if not request.MOBILE or waffle.flag_is_active(request, 'new-theme'):\n        return redirect_to(\n            request, 'products.product', slug='mobile', permanent=False)\n\n    docs = MOBILE_DOCS_FOR_MOBILE\n    return jingo.render(request, template,\n                        _data(docs, request.locale, 'mobile', 'mobile'))"}
{"input": "def validate_response(r, path):\n    \"\"\"\n    Check the requests object r, raise error if it's not ok.\n\n    Parameters\n    ----------\n    r: requests response object\n    path: associated URL path, for error messages\n    \"\"\"\n    if not r.ok:\n        m = str(r.content)\n        error = None\n        try:\n            error = r.json()['error']\n            msg = error['message']\n        except:\n            msg = str(r.content)\n\n        if \"Not Found\" in m:\n            raise FileNotFoundError(path)\n        elif \"forbidden\" in m:\n            raise IOError(\"Forbidden: %s\\n%s\" % (path, msg))\n        elif \"invalid\" in m:\n            raise ValueError(\"Bad Request: %s\\n%s\" % (path, msg))\n        elif error:\n            raise HtmlError(error)\n        else:\n            raise RuntimeError(m)", "output": "def validate_response(r, path):\n    \"\"\"\n    Check the requests object r, raise error if it's not ok.\n\n    Parameters\n    ----------\n    r: requests response object\n    path: associated URL path, for error messages\n    \"\"\"\n    if not r.ok:\n        m = str(r.content)\n        error = None\n        try:\n            error = r.json()['error']\n            msg = error['message']\n        except:\n            msg = str(r.content)\n\n        if \"Not Found\" in m or 'No such object' in m:\n            raise FileNotFoundError(path)\n        elif \"forbidden\" in m:\n            raise IOError(\"Forbidden: %s\\n%s\" % (path, msg))\n        elif \"invalid\" in m:\n            raise ValueError(\"Bad Request: %s\\n%s\" % (path, msg))\n        elif error:\n            raise HtmlError(error)\n        else:\n            raise RuntimeError(m)"}
{"input": "def check(self, instance):\n\n        if 'host' not in instance:\n            instance['host'] = 'localhost'\n        if 'extension_length' not in instance:\n            self.log.error('extension_length not defined, skipping')\n            return\n        if 'manager_user' not in instance:\n            self.log.error('manager_user not defined, skipping')\n            return\n        if 'manager_secret' not in instance:\n            self.log.error('manager_secret not defined, skipping')\n            return\n            \n\n######  Connect\n        mgr = asterisk.manager.Manager()\n        try:\n            if 'port' in instance:\n                mgr.connect(instance['host'],instance['port'])\n            else:\n                mgr.connect(instance['host'])\n            mgr.login(instance['manager_user'],instance['manager_secret'])\n        except asterisk.manager.ManagerSocketException as e:\n            self.log.error('Error connecting to Asterisk Manager Interface')\n            mgr.close()\n            return\n        except asterisk.manager.ManagerAuthException as e:\n            self.log.error('Error Logging in to Asterisk Manager Interface')\n            mgr.close()\n            return\n\n##### Call Volume\n        call_volume = mgr.command('core show calls')\n\n        current_call_vol = call_volume.data.split('\\n')\n\n        procesed_call_vol = current_call_vol[1].replace(' calls processed','')\n        current_call_vol = current_call_vol[0].replace('active call','')\n        current_call_vol = current_call_vol.replace('s','')\n        current_call_vol = current_call_vol.replace(' ','')\n\n        self.gauge('asterisk.callsprocesed',procesed_call_vol)\n        self.gauge('asterisk.callvolume',current_call_vol)\n\n##### Internal, Inbound Outbound Calls\n\n        extensionLength = instance['extension_length']\n\n        current_channels = mgr.command('core show channels verbose')\n        current_channels = current_channels.data.split('\\n')\n        current_channels[0] = None\n        current_channels_size = len(current_channels)\n        current_channels[current_channels_size-1] = None\n        current_channels[current_channels_size-2] = None\n        current_channels[current_channels_size-3] = None\n        current_channels[current_channels_size-4] = None\n        current_channels[current_channels_size-5] = None\n\n        currentChannelsArray = []\n        currentCalls = []\n\n        for chan in current_channels:\n            if chan != None:\n                channel     = re.sub(' +',' ',chan[0:21]).lstrip(' ').rstrip(' ')\n                context     = re.sub(' +',' ',chan[21:42]).lstrip(' ').rstrip(' ')\n                extension   = re.sub(' +',' ',chan[42:59]).lstrip(' ').rstrip(' ')\n                priority    = re.sub(' +',' ',chan[59:64]).lstrip(' ').rstrip(' ')\n                state       = re.sub(' +',' ',chan[64:72]).lstrip(' ').rstrip(' ')\n                application = re.sub(' +',' ',chan[72:85]).lstrip(' ').rstrip(' ')\n                data        = re.sub(' +',' ',chan[85:111]).lstrip(' ').rstrip(' ')\n                callerid    = re.sub(' +',' ',chan[111:127]).lstrip(' ').rstrip(' ')\n                duration    = re.sub(' +',' ',chan[127:136]).lstrip(' ').rstrip(' ')\n                accountcode = re.sub(' +',' ',chan[136:148]).lstrip(' ').rstrip(' ')\n                peeraccount = re.sub(' +',' ',chan[148:160]).lstrip(' ').rstrip(' ')\n                bridgedto   = re.sub(' +',' ',chan[160:181]).lstrip(' ').rstrip(' ')\n                currentChannel = Channel(channel,context,extension,priority,state,application,data,callerid,duration,accountcode,peeraccount,bridgedto)\n                currentChannelsArray.append(currentChannel)\n                \n        internalCalls = 0\n        outboundCalls = 0\n        inboundCalls  = 0\n\n        for currentChannel in currentChannelsArray:\n            caller = \"N/A\"\n            called = \"N/A\"\n            callType = \"N/A\"\n\n            if \"Dial\" == currentChannel.Application:\n                currentCall = Call(\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\")\n                currentCall.Caller = currentChannel.CallerId\n                currentCall.CallerChannel = currentChannel.Channel\n                currentCall.BridgedChannel = currentChannel.BridgedTo\n                currentCalls.append(currentCall)\n\n        for currentCall in currentCalls:\n            caller = \"N/A\"\n            called = \"N/A\"\n            callType = \"N/A\"\n            for currentChannel in currentChannelsArray:\n                if \"None\" not in currentChannel.BridgedTo:\n                    if currentCall.BridgedChannel == currentChannel.Channel:\n                        currentCall.Called = currentChannel.CallerId\n                        currentCall.CalledChannel = currentChannel.Channel\n\n        for currentCall in currentCalls:\n            if len(currentCall.Caller) <= extensionLength and len(currentCall.Called) <= extensionLength:\n                currentCall.CallType = \"Internal\"\n                internalCalls = internalCalls +1\n            if len(currentCall.Caller) > extensionLength and len(currentCall.Called) <= extensionLength:\n                currentCall.CallType = \"Inbound\"\n                inboundCalls = inboundCalls + 1\n            if len(currentCall.Caller) <= extensionLength and len(currentCall.Called) > extensionLength:\n                currentCall.CallType = \"Outbound\"\n                outboundCalls = outboundCalls + 1\n\n        self.gauge('asterisk.calls.internal',internalCalls)\n        self.gauge('asterisk.calls.inbound',inboundCalls)\n        self.gauge('asterisk.calls.outbound',outboundCalls)\n\n##### SIP Peers\n        sip_result = mgr.command('sip show peers')\n\n        sip_results = sip_result.data.split('\\n')\n\n        siptotals = sip_results[len(sip_results)-3]\n\n        siptotal = re.findall(r'([0-9]+) sip peer',siptotals)[0]\n\n        monitored_peers = re.findall(r'Monitored: ([0-9]+) online, ([0-9]+) offline',siptotals)[0]\n        unmonitored_peers = re.findall(r'Unmonitored: ([0-9]+) online, ([0-9]+) offline',siptotals)[0]\n\n        self.gauge('asterisk.sip.peers',siptotal)\n        self.gauge('asterisk.sip.monitored.online',monitored_peers[0])\n        self.gauge('asterisk.sip.monitored.offline',monitored_peers[1])\n        self.gauge('asterisk.sip.unmonitored.online',unmonitored_peers[0])\n        self.gauge('asterisk.sip.unmonitored.offline',unmonitored_peers[1])\n\n##### SIP Trunks (You have to add '-trunk' string into your SIP trunk name to detect it as a Trunk)\n        sip_total_trunks = 0\n        sip_online_trunks = 0\n        sip_offline_trunks = 0\n\n        trunks = re.finditer('^.*-trunk.*([OK|UN].*)', sip_result.data, re.MULTILINE)\n\n        for trunk in trunks:\n            sip_total_trunks +=1\n            if 'OK' in trunk.group():\n                sip_online_trunks += 1\n            else:\n                sip_offline_trunks += 1\n      \n        self.gauge('asterisk.sip.trunks.total',sip_total_trunks)\n        self.gauge('asterisk.sip.trunks.online',sip_online_trunks)\n        self.gauge('asterisk.sip.trunks.offline',sip_offline_trunks)\n\n##### PRI In Use\n\n        pri = mgr.command('pri show channels')\n\n        pri_channels = pri.data.split('\\n')\n\n        pri_channels[0] = None\n        pri_channels[1] = None\n\n        openchannels = 0\n        for chan in pri_channels:\n            if chan != None:\n                chan_data = chan.split()\n                if len(chan_data) > 2 and chan_data[3] == \"No\":\n                    openchannels += 1\n\n        self.gauge('asterisk.pri.channelsinuse',openchannels)\n\n##### IAX2 Peers\n\n        iax_result = mgr.command('iax2 show peers')\n\n        iax_results = iax_result.data.split('\\n')\n\n        iax_total_line = iax_results[len(iax_results)-3]\n\n        iax_peers_total = re.findall(r'([0-9]+) iax2 peers',iax_total_line)[0]\n        iax_peers_online = re.findall(r'\\[([0-9]+) online',iax_total_line)[0]\n        iax_peers_offline = re.findall(r'([0-9]+) offline',iax_total_line)[0]\n        iax_peers_unmonitored = re.findall(r'([0-9]+) unmonitored',iax_total_line)[0]\n\n        self.gauge('asterisk.iax2.peers',iax_peers_total)\n        self.gauge('asterisk.iax2.online',iax_peers_online)\n        self.gauge('asterisk.iax2.offline',iax_peers_offline)\n        self.gauge('asterisk.iax2.unmonitored',iax_peers_unmonitored)\n   \n##### DAHDI Channels  \n    \n        dahdi_result = mgr.command('dahdi show status')\n\n        dahdi_results = dahdi_result.data.split('\\n')\n\n        dahdi_total_trunks = len(dahdi_results)-3\n\n        dahdi_results[0] = None\n\n        dahdi_online_trunks = 0\n        dahdi_offline_trunks = 0\n\n        for chan in dahdi_results:\n            if chan != None:\n                chan_data = chan.split()\n\n                if len(chan_data) > 1:\n                    if \"Wildcard\" in chan_data[0]:\n                        if len(chan_data) > 2 and chan_data[2] == \"OK\":\n                            dahdi_online_trunks += 1\n                        if len(chan_data) > 2 and chan_data[2] == \"RED\":\n                            dahdi_offline_trunks += 1\n\n                    if \"wanpipe\" in chan_data[0]:\n                        if len(chan_data) > 2 and chan_data[3] == \"OK\":\n                            dahdi_online_trunks += 1\n                        if len(chan_data) > 2 and chan_data[3] == \"RED\":\n                            dahdi_offline_trunks += 1\n\n        self.gauge('asterisk.dahdi.total',dahdi_total_trunks)\n        self.gauge('asterisk.dahdi.online',dahdi_online_trunks)\n        self.gauge('asterisk.dahdi.offline',dahdi_offline_trunks)\n        \n##### G729 Codecs \n        \n        g729_result = mgr.command('g729 show licenses')\n\n        g729_results = g729_result.data.split('\\n')\n\n        g729_total_line = g729_results[0]\n\n        g729_total = re.findall(r'([0-9]+) licensed',g729_total_line)[0]\n        g729_encoders = re.split('/',g729_total_line)[0]\n        g729_decoders = re.findall(r'([0-9]+) encoders/decoders',g729_total_line)[0]\n\n        self.gauge('asterisk.g729.total',g729_total)\n        self.gauge('asterisk.g729.encoders',g729_encoders)\n        self.gauge('asterisk.g729.decoders',g729_decoders)\n        \n\n##### Asterisk Uptime\n\n        uptime_result = mgr.command('core show uptime')\n        \n        uptime_results = uptime_result.data.split('\\n')\n        \n        system_total_line = uptime_results[0]\n        asterisk_total_line = uptime_results[1]\n        \n        system_uptime_days = 0\n        system_uptime_hours = 0\n        system_uptime_minutes = 0\n        system_uptime_seconds = 0\n        \n        system_uptime_days = 0\n        system_uptime_hours = 0\n        system_uptime_minutes = 0\n        system_uptime_seconds = 0\n\n        if \"day\" in system_total_line:\n            system_uptime_days = re.findall(r'([0-9]+) day',system_total_line)[0]\n        if \"hour\" in system_total_line:\n            system_uptime_hours = re.findall(r'([0-9]+) hour',system_total_line)[0]\n        if \"minute\" in system_total_line:\n            system_uptime_minutes = re.findall(r'([0-9]+) minute',system_total_line)[0]\n        if \"second\" in system_total_line:\n            system_uptime_seconds = re.findall(r'([0-9]+) second',system_total_line)[0]\n\n        system_uptime = ( int(system_uptime_days) * 86400) +  ( int(system_uptime_hours) * 3600) + ( int(system_uptime_minutes) * 60) + int(system_uptime_seconds)\n        \n        asterisk_last_reload_days = 0\n        asterisk_last_reload_hours = 0\n        asterisk_last_reload_minutes = 0\n        asterisk_last_reload_seconds = 0\n        \n        if \"day\" in asterisk_total_line:\n            asterisk_last_reload_days = re.findall(r'([0-9]+) day',asterisk_total_line)[0]\n        if \"hour\" in asterisk_total_line:\n            asterisk_last_reload_hours = re.findall(r'([0-9]+) hour',asterisk_total_line)[0]\n        if \"minute\" in asterisk_total_line:\n            asterisk_last_reload_minutes = re.findall(r'([0-9]+) minute',asterisk_total_line)[0]\n        if \"second\" in asterisk_total_line:\n            asterisk_last_reload_seconds = re.findall(r' ([0-9]+) second',asterisk_total_line)[0]\n\n        asterisk_last_reload = ( int(asterisk_last_reload_days) * 86400) + ( int(asterisk_last_reload_hours) * 3600) + ( int(asterisk_last_reload_minutes) * 60) + int(asterisk_last_reload_seconds)\n\n        self.gauge('asterisk.system.uptime',system_uptime)\n        self.gauge('asterisk.last.reload',asterisk_last_reload)\n        \n##### MFCR2 Channels\n\n        mfcr2_result = mgr.command('mfcr2 show channels')\n\n        mfcr2_results = mfcr2_result.data.split('\\n')\n\n        mfcr2_total_channels = len(mfcr2_results)-3\n\n        mfcr2_results[0] = None\n\n        mfcr2_inuse_channels = 0\n        mfcr2_available_channels = 0\n        mfcr2_blocked_channels = 0\n\n        for chan in mfcr2_results:\n            if chan != None:\n                chan_data = chan.split()\n                if len(chan_data) > 2:\n                    if \"IDLE\" in chan_data[6] and \"IDLE\" in chan_data[7] :\n                        mfcr2_available_channels += 1\n                    if \"ANSWER\" in chan_data[6] or \"ANSWER\" in chan_data[7] :\n                        mfcr2_inuse_channels += 1\n                    if \"BLOCK\" in chan_data[6] or \"BLOCK\" in chan_data[7] :\n                        mfcr2_blocked_channels += 1\n                        \n        self.gauge('asterisk.mfcr2.total.channels',mfcr2_total_channels)\n        self.gauge('asterisk.mfcr2.available.channels',mfcr2_available_channels)\n        self.gauge('asterisk.mfcr2.inuse.channels',mfcr2_inuse_channels)\n        self.gauge('asterisk.mfcr2.blocked.channels',mfcr2_blocked_channels)\n\n##### SCCP Devices\n\n        sccp_total_devices = 0\n        sccp_online_devices = 0\n        sccp_offline_devices = 0\n        \n        sccp_result = mgr.command('sccp show devices')\n\n        if \"No such command\" not in sccp_result.data:\n            \n            sccp_devices = re.finditer('^.*.SEP.*', sccp_result.data, re.MULTILINE)\n\n            for sccp_device in sccp_devices:\n                sccp_total_devices +=1\n                if '--' in sccp_device.group():\n                    sccp_offline_devices += 1\n                else:\n                    sccp_online_devices += 1\n\n        self.gauge('asterisk.sccp.devices.total',sccp_total_devices)\n        self.gauge('asterisk.sccp.devices.online',sccp_online_devices)\n        self.gauge('asterisk.sccp.devices.offline',sccp_offline_devices)\n                    \n\n##### Close connection\n\n        mgr.close()", "output": "def check(self, instance):\n\n        if 'host' not in instance:\n            instance['host'] = 'localhost'\n        if 'extension_length' not in instance:\n            self.log.error('extension_length not defined, skipping')\n            return\n        if 'manager_user' not in instance:\n            self.log.error('manager_user not defined, skipping')\n            return\n        if 'manager_secret' not in instance:\n            self.log.error('manager_secret not defined, skipping')\n            return\n            \n\n######  Connect\n        mgr = asterisk.manager.Manager()\n        try:\n            if 'port' in instance:\n                mgr.connect(instance['host'],instance['port'])\n            else:\n                mgr.connect(instance['host'])\n            mgr.login(instance['manager_user'],instance['manager_secret'])\n        except asterisk.manager.ManagerSocketException as e:\n            self.log.error('Error connecting to Asterisk Manager Interface')\n            mgr.close()\n            return\n        except asterisk.manager.ManagerAuthException as e:\n            self.log.error('Error Logging in to Asterisk Manager Interface')\n            mgr.close()\n            return\n\n##### Call Volume\n        call_volume = mgr.command('core show calls')\n\n        current_call_vol = call_volume.data.split('\\n')\n\n        procesed_call_vol = current_call_vol[1].replace(' calls processed','')\n        current_call_vol = current_call_vol[0].replace('active call','')\n        current_call_vol = current_call_vol.replace('s','')\n        current_call_vol = current_call_vol.replace(' ','')\n\n        self.gauge('asterisk.callsprocesed',procesed_call_vol)\n        self.gauge('asterisk.callvolume',current_call_vol)\n\n##### Internal, Inbound Outbound Calls\n\n        extensionLength = instance['extension_length']\n\n        current_channels = mgr.command('core show channels verbose')\n        current_channels = current_channels.data.split('\\n')\n        current_channels[0] = None\n        current_channels_size = len(current_channels)\n        current_channels[current_channels_size-1] = None\n        current_channels[current_channels_size-2] = None\n        current_channels[current_channels_size-3] = None\n        current_channels[current_channels_size-4] = None\n        current_channels[current_channels_size-5] = None\n\n        currentChannelsArray = []\n        currentCalls = []\n\n        for chan in current_channels:\n            if chan != None:\n                channel     = re.sub(' +',' ',chan[0:21]).lstrip(' ').rstrip(' ')\n                context     = re.sub(' +',' ',chan[21:42]).lstrip(' ').rstrip(' ')\n                extension   = re.sub(' +',' ',chan[42:59]).lstrip(' ').rstrip(' ')\n                priority    = re.sub(' +',' ',chan[59:64]).lstrip(' ').rstrip(' ')\n                state       = re.sub(' +',' ',chan[64:72]).lstrip(' ').rstrip(' ')\n                application = re.sub(' +',' ',chan[72:85]).lstrip(' ').rstrip(' ')\n                data        = re.sub(' +',' ',chan[85:111]).lstrip(' ').rstrip(' ')\n                callerid    = re.sub(' +',' ',chan[111:127]).lstrip(' ').rstrip(' ')\n                duration    = re.sub(' +',' ',chan[127:136]).lstrip(' ').rstrip(' ')\n                accountcode = re.sub(' +',' ',chan[136:148]).lstrip(' ').rstrip(' ')\n                peeraccount = re.sub(' +',' ',chan[148:160]).lstrip(' ').rstrip(' ')\n                bridgedto   = re.sub(' +',' ',chan[160:181]).lstrip(' ').rstrip(' ')\n                currentChannel = Channel(channel,context,extension,priority,state,application,data,callerid,duration,accountcode,peeraccount,bridgedto)\n                currentChannelsArray.append(currentChannel)\n\n        internalCalls = 0\n        outboundCalls = 0\n        inboundCalls  = 0\n\n        for currentChannel in currentChannelsArray:\n            caller = \"N/A\"\n            called = \"N/A\"\n            callType = \"N/A\"\n\n            if \"Dial\" == currentChannel.Application or \"Queue\" == currentChannel.Application:\n                currentCall = Call(\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\")\n                currentCall.Caller = currentChannel.CallerId\n                currentCall.CallerChannel = currentChannel.Channel\n                currentCall.BridgedChannel = currentChannel.BridgedTo\n                currentCalls.append(currentCall)\n\n        for currentCall in currentCalls:\n            caller = \"N/A\"\n            called = \"N/A\"\n            callType = \"N/A\"\n            for currentChannel in currentChannelsArray:\n                if \"None\" not in currentChannel.BridgedTo:\n                    if currentCall.BridgedChannel == currentChannel.Channel:\n                        currentCall.Called = currentChannel.CallerId\n                        currentCall.CalledChannel = currentChannel.Channel\n\n        for currentCall in currentCalls:\n            if len(currentCall.Caller) <= extensionLength and len(currentCall.Called) <= extensionLength:\n                currentCall.CallType = \"Internal\"\n                internalCalls = internalCalls +1\n            if len(currentCall.Caller) > extensionLength and len(currentCall.Called) <= extensionLength:\n                currentCall.CallType = \"Inbound\"\n                inboundCalls = inboundCalls + 1\n            if len(currentCall.Caller) <= extensionLength and len(currentCall.Called) > extensionLength:\n                currentCall.CallType = \"Outbound\"\n                outboundCalls = outboundCalls + 1\n\n        self.gauge('asterisk.calls.internal',internalCalls)\n        self.gauge('asterisk.calls.inbound',inboundCalls)\n        self.gauge('asterisk.calls.outbound',outboundCalls)\n\n##### SIP Peers\n        sip_result = mgr.command('sip show peers')\n\n        sip_results = sip_result.data.split('\\n')\n\n        siptotals = sip_results[len(sip_results)-3]\n\n        siptotal = re.findall(r'([0-9]+) sip peer',siptotals)[0]\n\n        monitored_peers = re.findall(r'Monitored: ([0-9]+) online, ([0-9]+) offline',siptotals)[0]\n        unmonitored_peers = re.findall(r'Unmonitored: ([0-9]+) online, ([0-9]+) offline',siptotals)[0]\n\n        self.gauge('asterisk.sip.peers',siptotal)\n        self.gauge('asterisk.sip.monitored.online',monitored_peers[0])\n        self.gauge('asterisk.sip.monitored.offline',monitored_peers[1])\n        self.gauge('asterisk.sip.unmonitored.online',unmonitored_peers[0])\n        self.gauge('asterisk.sip.unmonitored.offline',unmonitored_peers[1])\n\n##### SIP Trunks (You have to add '-trunk' string into your SIP trunk name to detect it as a Trunk)\n        sip_total_trunks = 0\n        sip_online_trunks = 0\n        sip_offline_trunks = 0\n\n        trunks = re.finditer('^.*-trunk.*([OK|UN].*)', sip_result.data, re.MULTILINE)\n\n        for trunk in trunks:\n            sip_total_trunks +=1\n            if 'OK' in trunk.group():\n                sip_online_trunks += 1\n            else:\n                sip_offline_trunks += 1\n      \n        self.gauge('asterisk.sip.trunks.total',sip_total_trunks)\n        self.gauge('asterisk.sip.trunks.online',sip_online_trunks)\n        self.gauge('asterisk.sip.trunks.offline',sip_offline_trunks)\n\n##### PRI In Use\n\n        pri = mgr.command('pri show channels')\n\n        pri_channels = pri.data.split('\\n')\n\n        pri_channels[0] = None\n        pri_channels[1] = None\n\n        openchannels = 0\n        for chan in pri_channels:\n            if chan != None:\n                chan_data = chan.split()\n                if len(chan_data) > 2 and chan_data[3] == \"No\":\n                    openchannels += 1\n\n        self.gauge('asterisk.pri.channelsinuse',openchannels)\n\n##### IAX2 Peers\n\n        iax_result = mgr.command('iax2 show peers')\n\n        iax_results = iax_result.data.split('\\n')\n\n        iax_total_line = iax_results[len(iax_results)-3]\n\n        iax_peers_total = re.findall(r'([0-9]+) iax2 peers',iax_total_line)[0]\n        iax_peers_online = re.findall(r'\\[([0-9]+) online',iax_total_line)[0]\n        iax_peers_offline = re.findall(r'([0-9]+) offline',iax_total_line)[0]\n        iax_peers_unmonitored = re.findall(r'([0-9]+) unmonitored',iax_total_line)[0]\n\n        self.gauge('asterisk.iax2.peers',iax_peers_total)\n        self.gauge('asterisk.iax2.online',iax_peers_online)\n        self.gauge('asterisk.iax2.offline',iax_peers_offline)\n        self.gauge('asterisk.iax2.unmonitored',iax_peers_unmonitored)\n   \n##### DAHDI Channels  \n    \n        dahdi_result = mgr.command('dahdi show status')\n\n        dahdi_results = dahdi_result.data.split('\\n')\n\n        dahdi_total_trunks = len(dahdi_results)-3\n\n        dahdi_results[0] = None\n\n        dahdi_online_trunks = 0\n        dahdi_offline_trunks = 0\n\n        for chan in dahdi_results:\n            if chan != None:\n                chan_data = chan.split()\n\n                if len(chan_data) > 1:\n                    if \"Wildcard\" in chan_data[0]:\n                        if len(chan_data) > 2 and chan_data[2] == \"OK\":\n                            dahdi_online_trunks += 1\n                        if len(chan_data) > 2 and chan_data[2] == \"RED\":\n                            dahdi_offline_trunks += 1\n\n                    if \"wanpipe\" in chan_data[0]:\n                        if len(chan_data) > 2 and chan_data[3] == \"OK\":\n                            dahdi_online_trunks += 1\n                        if len(chan_data) > 2 and chan_data[3] == \"RED\":\n                            dahdi_offline_trunks += 1\n\n        self.gauge('asterisk.dahdi.total',dahdi_total_trunks)\n        self.gauge('asterisk.dahdi.online',dahdi_online_trunks)\n        self.gauge('asterisk.dahdi.offline',dahdi_offline_trunks)\n        \n##### G729 Codecs \n        \n        g729_result = mgr.command('g729 show licenses')\n\n        g729_results = g729_result.data.split('\\n')\n\n        g729_total_line = g729_results[0]\n\n        g729_total = re.findall(r'([0-9]+) licensed',g729_total_line)[0]\n        g729_encoders = re.split('/',g729_total_line)[0]\n        g729_decoders = re.findall(r'([0-9]+) encoders/decoders',g729_total_line)[0]\n\n        self.gauge('asterisk.g729.total',g729_total)\n        self.gauge('asterisk.g729.encoders',g729_encoders)\n        self.gauge('asterisk.g729.decoders',g729_decoders)\n        \n\n##### Asterisk Uptime\n\n        uptime_result = mgr.command('core show uptime')\n        \n        uptime_results = uptime_result.data.split('\\n')\n        \n        system_total_line = uptime_results[0]\n        asterisk_total_line = uptime_results[1]\n        \n        system_uptime_days = 0\n        system_uptime_hours = 0\n        system_uptime_minutes = 0\n        system_uptime_seconds = 0\n        \n        system_uptime_days = 0\n        system_uptime_hours = 0\n        system_uptime_minutes = 0\n        system_uptime_seconds = 0\n\n        if \"day\" in system_total_line:\n            system_uptime_days = re.findall(r'([0-9]+) day',system_total_line)[0]\n        if \"hour\" in system_total_line:\n            system_uptime_hours = re.findall(r'([0-9]+) hour',system_total_line)[0]\n        if \"minute\" in system_total_line:\n            system_uptime_minutes = re.findall(r'([0-9]+) minute',system_total_line)[0]\n        if \"second\" in system_total_line:\n            system_uptime_seconds = re.findall(r'([0-9]+) second',system_total_line)[0]\n\n        system_uptime = ( int(system_uptime_days) * 86400) +  ( int(system_uptime_hours) * 3600) + ( int(system_uptime_minutes) * 60) + int(system_uptime_seconds)\n        \n        asterisk_last_reload_days = 0\n        asterisk_last_reload_hours = 0\n        asterisk_last_reload_minutes = 0\n        asterisk_last_reload_seconds = 0\n        \n        if \"day\" in asterisk_total_line:\n            asterisk_last_reload_days = re.findall(r'([0-9]+) day',asterisk_total_line)[0]\n        if \"hour\" in asterisk_total_line:\n            asterisk_last_reload_hours = re.findall(r'([0-9]+) hour',asterisk_total_line)[0]\n        if \"minute\" in asterisk_total_line:\n            asterisk_last_reload_minutes = re.findall(r'([0-9]+) minute',asterisk_total_line)[0]\n        if \"second\" in asterisk_total_line:\n            asterisk_last_reload_seconds = re.findall(r' ([0-9]+) second',asterisk_total_line)[0]\n\n        asterisk_last_reload = ( int(asterisk_last_reload_days) * 86400) + ( int(asterisk_last_reload_hours) * 3600) + ( int(asterisk_last_reload_minutes) * 60) + int(asterisk_last_reload_seconds)\n\n        self.gauge('asterisk.system.uptime',system_uptime)\n        self.gauge('asterisk.last.reload',asterisk_last_reload)\n        \n##### MFCR2 Channels\n\n        mfcr2_result = mgr.command('mfcr2 show channels')\n\n        mfcr2_results = mfcr2_result.data.split('\\n')\n\n        mfcr2_total_channels = len(mfcr2_results)-3\n\n        mfcr2_results[0] = None\n\n        mfcr2_inuse_channels = 0\n        mfcr2_available_channels = 0\n        mfcr2_blocked_channels = 0\n\n        for chan in mfcr2_results:\n            if chan != None:\n                chan_data = chan.split()\n                if len(chan_data) > 2:\n                    if \"IDLE\" in chan_data[6] and \"IDLE\" in chan_data[7] :\n                        mfcr2_available_channels += 1\n                    if \"ANSWER\" in chan_data[6] or \"ANSWER\" in chan_data[7] :\n                        mfcr2_inuse_channels += 1\n                    if \"BLOCK\" in chan_data[6] or \"BLOCK\" in chan_data[7] :\n                        mfcr2_blocked_channels += 1\n                        \n        self.gauge('asterisk.mfcr2.total.channels',mfcr2_total_channels)\n        self.gauge('asterisk.mfcr2.available.channels',mfcr2_available_channels)\n        self.gauge('asterisk.mfcr2.inuse.channels',mfcr2_inuse_channels)\n        self.gauge('asterisk.mfcr2.blocked.channels',mfcr2_blocked_channels)\n\n##### SCCP Devices\n\n        sccp_total_devices = 0\n        sccp_online_devices = 0\n        sccp_offline_devices = 0\n        \n        sccp_result = mgr.command('sccp show devices')\n\n        if \"No such command\" not in sccp_result.data:\n            \n            sccp_devices = re.finditer('^.*.SEP.*', sccp_result.data, re.MULTILINE)\n\n            for sccp_device in sccp_devices:\n                sccp_total_devices +=1\n                if '--' in sccp_device.group():\n                    sccp_offline_devices += 1\n                else:\n                    sccp_online_devices += 1\n\n        self.gauge('asterisk.sccp.devices.total',sccp_total_devices)\n        self.gauge('asterisk.sccp.devices.online',sccp_online_devices)\n        self.gauge('asterisk.sccp.devices.offline',sccp_offline_devices)\n                    \n\n##### Close connection\n\n        mgr.close()"}
{"input": "def _delete_savegame(self, map_files):\n\t\t\"\"\"Deletes the selected savegame if the user confirms\n\t\tself.current has to contain the widget \"savegamelist\"\n\t\t@param map_files: list of files that corresponds to the entries of 'savegamelist'\n\t\t@return: True if something was deleted, else False\n\t\t\"\"\"\n\t\tselected_item = self.current.collectData(\"savegamelist\")\n\t\tif selected_item == -1:\n\t\t\tself.show_popup(_(\"No file selected\"), _(\"You need to select a savegame to delete\"))\n\t\t\treturn False\n\t\tselected_file = map_files[selected_item]\n\t\tif self.show_popup(_(\"Confirm deletion\"),\n\t\t\t\t\t\t\t\t\t\t\t _('Do you really want to delete the savegame \"%s\"?') % \\\n\t\t\t\t\t\t\t\t\t\t\t SavegameManager.get_savegamename_from_filename(selected_file), \\\n\t\t\t\t\t\t\t\t\t\t\t show_cancel_button = True):\n\t\t\ttry:\n\t\t\t\tos.unlink(selected_file)\n\t\t\t\treturn True\n\t\t\texcept:\n\t\t\t\tself.show_popup(_(\"Error!\"), _(\"Failed to delete savefile!\"))\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False", "output": "def _delete_savegame(self, map_files):\n\t\t\"\"\"Deletes the selected savegame if the user confirms\n\t\tself.current has to contain the widget \"savegamelist\"\n\t\t@param map_files: list of files that corresponds to the entries of 'savegamelist'\n\t\t@return: True if something was deleted, else False\n\t\t\"\"\"\n\t\tselected_item = self.current.collectData(\"savegamelist\")\n\t\tif selected_item == -1 or selected_item >= len(map_files):\n\t\t\tself.show_popup(_(\"No file selected\"), _(\"You need to select a savegame to delete\"))\n\t\t\treturn False\n\t\tselected_file = map_files[selected_item]\n\t\tif self.show_popup(_(\"Confirm deletion\"),\n\t\t\t\t\t\t\t\t\t\t\t _('Do you really want to delete the savegame \"%s\"?') % \\\n\t\t\t\t\t\t\t\t\t\t\t SavegameManager.get_savegamename_from_filename(selected_file), \\\n\t\t\t\t\t\t\t\t\t\t\t show_cancel_button = True):\n\t\t\ttry:\n\t\t\t\tos.unlink(selected_file)\n\t\t\t\treturn True\n\t\t\texcept:\n\t\t\t\tself.show_popup(_(\"Error!\"), _(\"Failed to delete savefile!\"))\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False"}
{"input": "def all_keys(module, keyring, short_format):\n    if keyring:\n        cmd = \"apt-key --keyring %s adv --list-public-keys --keyid-format=long\" % keyring\n    else:\n        cmd = \"apt-key adv --list-public-keys --keyid-format=long\"\n    (rc, out, err) = module.run_command(cmd)\n    results = []\n    lines = out.split('\\n')\n    for line in lines:\n        if line.startswith(\"pub\"):\n            tokens = line.split()\n            code = tokens[1]\n            (len_type, real_code) = code.split(\"/\")\n            results.append(real_code)\n    if short_format:\n        results = shorten_key_ids(results)\n    return results", "output": "def all_keys(module, keyring, short_format):\n    if keyring:\n        cmd = \"apt-key --keyring %s adv --list-public-keys --keyid-format=long\" % keyring\n    else:\n        cmd = \"apt-key adv --list-public-keys --keyid-format=long\"\n    (rc, out, err) = module.run_command(cmd)\n    results = []\n    lines = out.split('\\n')\n    for line in lines:\n        if line.startswith(\"pub\") or line.startswith(\"sub\"):\n            tokens = line.split()\n            code = tokens[1]\n            (len_type, real_code) = code.split(\"/\")\n            results.append(real_code)\n    if short_format:\n        results = shorten_key_ids(results)\n    return results"}
{"input": "def get_printout(out, opts=None, **kwargs):\n    '''\n    Return a printer function\n    '''\n    if opts is None:\n        opts = {}\n\n    if 'output' in opts:\n        # new --out option\n        out = opts['output']\n        if out == 'text':\n            out = 'txt'\n\n    if out is None:\n        out = 'nested'\n\n    opts.update(kwargs)\n    if 'color' not in opts:\n        def is_pipe():\n            '''\n            Check if sys.stdout is a pipe or not\n            '''\n            try:\n                fileno = sys.stdout.fileno()\n            except AttributeError:\n                fileno = -1  # sys.stdout is StringIO or fake\n            return not os.isatty(fileno)\n\n        if opts.get('force_color', False):\n            opts['color'] = True\n        elif opts.get('no_color', False) or is_pipe():\n            opts['color'] = False\n        else:\n            opts['color'] = True\n\n    outputters = salt.loader.outputters(opts)\n    if out not in outputters:\n        return outputters['nested']\n    return outputters[out]", "output": "def get_printout(out, opts=None, **kwargs):\n    '''\n    Return a printer function\n    '''\n    if opts is None:\n        opts = {}\n\n    if 'output' in opts:\n        # new --out option\n        out = opts['output']\n        if out == 'text':\n            out = 'txt'\n\n    if out is None:\n        out = 'nested'\n\n    opts.update(kwargs)\n    if 'color' not in opts:\n        def is_pipe():\n            '''\n            Check if sys.stdout is a pipe or not\n            '''\n            try:\n                fileno = sys.stdout.fileno()\n            except AttributeError:\n                fileno = -1  # sys.stdout is StringIO or fake\n            return not os.isatty(fileno)\n\n        if opts.get('force_color', False):\n            opts['color'] = True\n        elif opts.get('no_color', False) or is_pipe() or salt.utils.is_windows():\n            opts['color'] = False\n        else:\n            opts['color'] = True\n\n    outputters = salt.loader.outputters(opts)\n    if out not in outputters:\n        return outputters['nested']\n    return outputters[out]"}
{"input": "def validate_hstore(value, is_serialized=False):\n    \"\"\" HSTORE validation. \"\"\"\n    # if empty\n    if value == '' or value == 'null':\n        value = '{}'\n\n    # ensure valid JSON\n    try:\n        # convert strings to dictionaries\n        if isinstance(value, six.string_types):\n            dictionary = json.loads(value)\n\n            # if serialized field, deserialize values\n            if is_serialized and isinstance(dictionary, dict):\n                dictionary = dict((k, json.loads(v)) for k, v in dictionary.items())  # TODO: modify to use field's deserializer\n        # if not a string we'll check at the next control if it's a dict\n        else:\n            dictionary = value\n    except ValueError as e:\n        raise ValidationError(ugettext(u'Invalid JSON: {0}').format(e))\n\n    # ensure is a dictionary\n    if not isinstance(dictionary, dict):\n        raise ValidationError(ugettext(u'No lists or values allowed, only dictionaries'))\n\n    # convert any non string object into string\n    for key, value in dictionary.items():\n        if isinstance(value, dict) or isinstance(value, list):\n            dictionary[key] = json.dumps(value)\n        if isinstance(value, bool) or isinstance(value, int) or isinstance(value, float):\n            if not is_serialized:  # Only convert if not from serializedfield\n                dictionary[key] = six.text_type(value).lower()\n\n    return dictionary", "output": "def validate_hstore(value, is_serialized=False):\n    \"\"\" HSTORE validation. \"\"\"\n    # if empty\n    if value is None or value == '' or value == 'null':\n        value = '{}'\n\n    # ensure valid JSON\n    try:\n        # convert strings to dictionaries\n        if isinstance(value, six.string_types):\n            dictionary = json.loads(value)\n\n            # if serialized field, deserialize values\n            if is_serialized and isinstance(dictionary, dict):\n                dictionary = dict((k, json.loads(v)) for k, v in dictionary.items())  # TODO: modify to use field's deserializer\n        # if not a string we'll check at the next control if it's a dict\n        else:\n            dictionary = value\n    except ValueError as e:\n        raise ValidationError(ugettext(u'Invalid JSON: {0}').format(e))\n\n    # ensure is a dictionary\n    if not isinstance(dictionary, dict):\n        raise ValidationError(ugettext(u'No lists or values allowed, only dictionaries'))\n\n    # convert any non string object into string\n    for key, value in dictionary.items():\n        if isinstance(value, dict) or isinstance(value, list):\n            dictionary[key] = json.dumps(value)\n        if isinstance(value, bool) or isinstance(value, int) or isinstance(value, float):\n            if not is_serialized:  # Only convert if not from serializedfield\n                dictionary[key] = six.text_type(value).lower()\n\n    return dictionary"}
{"input": "def traiter_lecture(self, lecture: dict, exchange: str):\n        if self.__logger.isEnabledFor(logging.DEBUG):\n            self.__logger.debug(\"Lecture recue : %s\" % json.dumps(lecture, indent=2))\n\n        noeud_id = lecture[SenseursPassifsConstantes.TRANSACTION_NOEUD_ID]\n        uuid_senseur = lecture[SenseursPassifsConstantes.TRANSACTION_ID_SENSEUR]\n        senseurs = lecture['senseurs']\n\n        # Conserver dans staging\n        staging = self.gestionnaire.document_dao.get_collection(SenseursPassifsConstantes.COLLECTION_STAGING_NOM)\n        staging.insert(lecture)\n\n        # Charger le document du senseur\n        collection = self.gestionnaire.document_dao.get_collection(SenseursPassifsConstantes.COLLECTION_DOCUMENTS_NOM)\n\n        filter = {\n            Constantes.DOCUMENT_INFODOC_LIBELLE: SenseursPassifsConstantes.LIBVAL_DOCUMENT_SENSEUR,\n            SenseursPassifsConstantes.TRANSACTION_ID_SENSEUR: uuid_senseur,\n        }\n        doc_senseur = collection.find_one(filter)\n\n        if not doc_senseur or doc_senseur.get('noeud_id') is None:\n            self.ajouter_senseur(lecture, exchange)\n            # Creer un document sommaire qui va etre insere\n            doc_senseur = {\n                SenseursPassifsConstantes.TRANSACTION_ID_SENSEUR: uuid_senseur,\n                'senseurs': dict()\n            }\n\n        # Verifier quels senseurs on met a jour\n        senseurs_actuels = doc_senseur['senseurs']\n        set_ops = dict()\n        for cle, donnees in senseurs.items():\n            donnees_actuelles = senseurs_actuels.get(cle)\n            if donnees_actuelles is None or donnees_actuelles['timestamp'] < donnees['timestamp']:\n                for key, value in donnees.items():\n                    set_ops['senseurs.' + cle + '.' + key] = value\n\n        if len(set_ops.keys()) > 0:\n            set_on_insert = {\n                Constantes.DOCUMENT_INFODOC_DATE_CREATION: datetime.datetime.utcnow()\n            }\n            set_on_insert.update(filter)\n            ops = {\n                '$set': set_ops,\n                '$setOnInsert': set_on_insert,\n            }\n\n            collection.update(filter, ops, upsert=True)\n        else:\n            self.__logger.debug(\"Evenement avec donnees plus vieilles que lectures dans les documents\")", "output": "def traiter_lecture(self, lecture: dict, exchange: str):\n        if self.__logger.isEnabledFor(logging.DEBUG):\n            self.__logger.debug(\"Lecture recue : %s\" % json.dumps(lecture, indent=2))\n\n        noeud_id = lecture[SenseursPassifsConstantes.TRANSACTION_NOEUD_ID]\n        uuid_senseur = lecture[SenseursPassifsConstantes.TRANSACTION_ID_SENSEUR]\n        senseurs = lecture['senseurs']\n\n        # Conserver dans staging\n        staging = self.gestionnaire.document_dao.get_collection(SenseursPassifsConstantes.COLLECTION_STAGING_NOM)\n        staging.insert(lecture)\n\n        # Charger le document du senseur\n        collection = self.gestionnaire.document_dao.get_collection(SenseursPassifsConstantes.COLLECTION_DOCUMENTS_NOM)\n\n        filter = {\n            Constantes.DOCUMENT_INFODOC_LIBELLE: SenseursPassifsConstantes.LIBVAL_DOCUMENT_SENSEUR,\n            SenseursPassifsConstantes.TRANSACTION_ID_SENSEUR: uuid_senseur,\n        }\n        doc_senseur = collection.find_one(filter)\n\n        if not doc_senseur or doc_senseur.get('noeud_id') is None:\n            self.ajouter_senseur(lecture, exchange)\n            # Creer un document sommaire qui va etre insere\n            doc_senseur = {\n                SenseursPassifsConstantes.TRANSACTION_ID_SENSEUR: uuid_senseur,\n                'senseurs': dict()\n            }\n\n        # Verifier quels senseurs on met a jour\n        senseurs_actuels = doc_senseur['senseurs']\n        set_ops = dict()\n        for cle, donnees in senseurs.items():\n            donnees_actuelles = senseurs_actuels.get(cle)\n            if donnees_actuelles is None or donnees_actuelles.get('timestamp') is None or donnees_actuelles['timestamp'] < donnees['timestamp']:\n                for key, value in donnees.items():\n                    set_ops['senseurs.' + cle + '.' + key] = value\n\n        if len(set_ops.keys()) > 0:\n            set_on_insert = {\n                Constantes.DOCUMENT_INFODOC_DATE_CREATION: datetime.datetime.utcnow()\n            }\n            set_on_insert.update(filter)\n            ops = {\n                '$set': set_ops,\n                '$setOnInsert': set_on_insert,\n            }\n\n            collection.update(filter, ops, upsert=True)\n        else:\n            self.__logger.debug(\"Evenement avec donnees plus vieilles que lectures dans les documents\")"}
{"input": "def build_module():\n    csrc = MODULE_PATH[0:-3] + '.c'\n    po = Popen(['cc', '-o', MODULE_PATH, '-shared', csrc])\n    po.communicate()\n    if po.returncode != 0:\n        raise Exception('Failed to compile module')", "output": "def build_module():\n    csrc = MODULE_PATH[0:-3] + '.c'\n    po = Popen(['cc', '-o', MODULE_PATH, '-shared', '-fPIC', csrc])\n    po.communicate()\n    if po.returncode != 0:\n        raise Exception('Failed to compile module')"}
{"input": "def main():\n  map_extra = config_list('depot_tools.map_extra')\n  fmt = '%C(red bold)%h%x09%Creset%C(green)%d%Creset %C(yellow)%ad%Creset ~ %s'\n  log_proc = subprocess2.Popen(\n    [GIT_EXE, 'log', '--graph', '--branches', '--tags', root(),\n     '--color=always', '--date=short', ('--pretty=format:' + fmt)\n    ] + map_extra + sys.argv[1:],\n    stdout=subprocess2.PIPE,\n    shell=False)\n\n  merge_base_map = branch_config_map('base')\n  current = current_branch()\n  all_branches = set(branches())\n  if current in all_branches:\n    all_branches.remove(current)\n  all_tags = set(tags())\n  try:\n    for line in log_proc.stdout.xreadlines():\n      if merge_base_map:\n        commit = line[line.find(BRIGHT_RED)+len(BRIGHT_RED):line.find('\\t')]\n        base_for_branches = set()\n        for branch, sha in merge_base_map.iteritems():\n          if sha.startswith(commit):\n            base_for_branches.add(branch)\n        if base_for_branches:\n          newline = '\\r\\n' if line.endswith('\\r\\n') else '\\n'\n          line = line.rstrip(newline)\n          line += ''.join(\n              (BRIGHT, WHITE, '    <(%s)' % (', '.join(base_for_branches)),\n               newline))\n          for b in base_for_branches:\n            del merge_base_map[b]\n\n      start = line.find(GREEN+' (')\n      end   = line.find(')', start)\n      if start != -1 and end != -1:\n        start += len(GREEN) + 2\n        branch_list = line[start:end].split(', ')\n        branches_str = ''\n        if branch_list:\n          colored_branches = []\n          head_marker = ''\n          for b in branch_list:\n            if b == \"HEAD\":\n              head_marker = BLUEBAK+BRIGHT+'*'\n              continue\n            if b == current:\n              colored_branches.append(CYAN+BRIGHT+b+RESET)\n              current = None\n            elif b in all_branches:\n              colored_branches.append(GREEN+BRIGHT+b+RESET)\n              all_branches.remove(b)\n            elif b in all_tags:\n              colored_branches.append(MAGENTA+BRIGHT+b+RESET)\n            elif b.startswith('tag: '):\n              colored_branches.append(MAGENTA+BRIGHT+b[5:]+RESET)\n            else:\n              colored_branches.append(RED+b)\n            branches_str = '(%s) ' % ((GREEN+\", \").join(colored_branches)+GREEN)\n          line = \"%s%s%s\" % (line[:start-1], branches_str, line[end+5:])\n          if head_marker:\n            line = line.replace('*', head_marker, 1)\n      sys.stdout.write(line)\n  except (IOError, KeyboardInterrupt):\n    pass\n  finally:\n    sys.stderr.close()\n    sys.stdout.close()\n  return 0", "output": "def main():\n  map_extra = config_list('depot_tools.map_extra')\n  fmt = '%C(red bold)%h%x09%Creset%C(green)%d%Creset %C(yellow)%ad%Creset ~ %s'\n  log_proc = subprocess2.Popen(\n    [GIT_EXE, 'log', '--graph', '--branches', '--tags', root(),\n     '--color=always', '--date=short', ('--pretty=format:' + fmt)\n    ] + map_extra + sys.argv[1:],\n    stdout=subprocess2.PIPE,\n    shell=False)\n\n  merge_base_map = branch_config_map('base')\n  current = current_branch()\n  all_branches = set(branches())\n  if current in all_branches:\n    all_branches.remove(current)\n  all_tags = set(tags())\n  try:\n    for line in log_proc.stdout.xreadlines():\n      if merge_base_map:\n        commit = line[line.find(BRIGHT_RED)+len(BRIGHT_RED):line.find('\\t')]\n        base_for_branches = set()\n        for branch, sha in merge_base_map.iteritems():\n          if sha.startswith(commit):\n            base_for_branches.add(branch)\n        if base_for_branches:\n          newline = '\\r\\n' if line.endswith('\\r\\n') else '\\n'\n          line = line.rstrip(newline)\n          line += ''.join(\n              (BRIGHT, WHITE, '    <(%s)' % (', '.join(base_for_branches)),\n               RESET, newline))\n          for b in base_for_branches:\n            del merge_base_map[b]\n\n      start = line.find(GREEN+' (')\n      end   = line.find(')', start)\n      if start != -1 and end != -1:\n        start += len(GREEN) + 2\n        branch_list = line[start:end].split(', ')\n        branches_str = ''\n        if branch_list:\n          colored_branches = []\n          head_marker = ''\n          for b in branch_list:\n            if b == \"HEAD\":\n              head_marker = BLUEBAK+BRIGHT+'*'\n              continue\n            if b == current:\n              colored_branches.append(CYAN+BRIGHT+b+RESET)\n              current = None\n            elif b in all_branches:\n              colored_branches.append(GREEN+BRIGHT+b+RESET)\n              all_branches.remove(b)\n            elif b in all_tags:\n              colored_branches.append(MAGENTA+BRIGHT+b+RESET)\n            elif b.startswith('tag: '):\n              colored_branches.append(MAGENTA+BRIGHT+b[5:]+RESET)\n            else:\n              colored_branches.append(RED+b)\n            branches_str = '(%s) ' % ((GREEN+\", \").join(colored_branches)+GREEN)\n          line = \"%s%s%s\" % (line[:start-1], branches_str, line[end+5:])\n          if head_marker:\n            line = line.replace('*', head_marker, 1)\n      sys.stdout.write(line)\n  except (IOError, KeyboardInterrupt):\n    pass\n  finally:\n    sys.stderr.close()\n    sys.stdout.close()\n  return 0"}
{"input": "def add(request):\n    if request.method == 'POST':\n        form = CredForm(request.user, request.POST)\n        if form.is_valid():\n            form.save()\n            CredAudit(audittype=CredAudit.CREDADD, cred=form.instance, user=request.user).save()\n            return HttpResponseRedirect('/cred/list')\n    else:\n        form = CredForm(request.user)\n\n    return render(request, 'cred_edit.html', {'form': form, 'action':\n        '/cred/add/'})", "output": "def add(request):\n    if request.method == 'POST':\n        form = CredForm(request.user, request.POST)\n        if form.is_valid():\n            form.save()\n            CredAudit(audittype=CredAudit.CREDADD, cred=form.instance, user=request.user).save()\n            return HttpResponseRedirect('/cred/list')\n    else:\n        form = CredForm(request.user)\n\n    return render(request, 'cred_edit.html', {'form': form, 'action':\n      '/cred/add/', 'icons': CredIcon.objects.all()})"}
{"input": "def validate_reference(ref):\n    dict(ref)\n    validate_string([\"firstname\"])\n    validate_string(ref[\"lastname\"])\n    validate_email(ref[\"email\"])\n    validate_string(ref[\"phone\"])", "output": "def validate_reference(ref):\n    dict(ref)\n    validate_string(ref[\"firstname\"])\n    validate_string(ref[\"lastname\"])\n    validate_email(ref[\"email\"])\n    validate_string(ref[\"phone\"])"}
{"input": "def test_unparse_parse(self):\n        for u in ['Python', './Python','x-newscheme://foo.com/stuff']:\n            self.assertEqual(urlparse.urlunsplit(urlparse.urlsplit(u)), u)\n            self.assertEqual(urlparse.urlunparse(urlparse.urlparse(u)), u)", "output": "def test_unparse_parse(self):\n        for u in ['Python', './Python','x-newscheme://foo.com/stuff','x://y','x:/y','x:/','/',]:\n            self.assertEqual(urlparse.urlunsplit(urlparse.urlsplit(u)), u)\n            self.assertEqual(urlparse.urlunparse(urlparse.urlparse(u)), u)"}
{"input": "def checkodesol(ode, sol, func=None, order='auto', solve_for_func=True):\n    r\"\"\"\n    Substitutes ``sol`` into ``ode`` and checks that the result is ``0``.\n\n    This only works when ``func`` is one function, like `f(x)`.  ``sol`` can\n    be a single solution or a list of solutions.  Each solution may be an\n    :py:class:`~sympy.core.relational.Equality` that the solution satisfies,\n    e.g. ``Eq(f(x), C1), Eq(f(x) + C1, 0)``; or simply an\n    :py:class:`~sympy.core.expr.Expr`, e.g. ``f(x) - C1``. In most cases it\n    will not be necessary to explicitly identify the function, but if the\n    function cannot be inferred from the original equation it can be supplied\n    through the ``func`` argument.\n\n    If a sequence of solutions is passed, the same sort of container will be\n    used to return the result for each solution.\n\n    It tries the following methods, in order, until it finds zero equivalence:\n\n    1. Substitute the solution for `f` in the original equation.  This only\n       works if ``ode`` is solved for `f`.  It will attempt to solve it first\n       unless ``solve_for_func == False``.\n    2. Take `n` derivatives of the solution, where `n` is the order of\n       ``ode``, and check to see if that is equal to the solution.  This only\n       works on exact ODEs.\n    3. Take the 1st, 2nd, ..., `n`\\th derivatives of the solution, each time\n       solving for the derivative of `f` of that order (this will always be\n       possible because `f` is a linear operator). Then back substitute each\n       derivative into ``ode`` in reverse order.\n\n    This function returns a tuple.  The first item in the tuple is ``True`` if\n    the substitution results in ``0``, and ``False`` otherwise. The second\n    item in the tuple is what the substitution results in.  It should always\n    be ``0`` if the first item is ``True``. Sometimes this function will\n    return ``False`` even when an expression is identically equal to ``0``.\n    This happens when :py:meth:`~sympy.simplify.simplify.simplify` does not\n    reduce the expression to ``0``.  If an expression returned by this\n    function vanishes identically, then ``sol`` really is a solution to\n    the ``ode``.\n\n    If this function seems to hang, it is probably because of a hard\n    simplification.\n\n    To use this function to test, test the first item of the tuple.\n\n    Examples\n    ========\n\n    >>> from sympy import Eq, Function, checkodesol, symbols\n    >>> x, C1 = symbols('x,C1')\n    >>> f = Function('f')\n    >>> checkodesol(f(x).diff(x), Eq(f(x), C1))\n    (True, 0)\n    >>> assert checkodesol(f(x).diff(x), C1)[0]\n    >>> assert not checkodesol(f(x).diff(x), x)[0]\n    >>> checkodesol(f(x).diff(x, 2), x**2)\n    (False, 2)\n\n    \"\"\"\n    if not isinstance(ode, Equality):\n        ode = Eq(ode, 0)\n    if func is None:\n        try:\n            _, func = _preprocess(ode.lhs)\n        except ValueError:\n            funcs = [s.atoms(AppliedUndef) for s in (\n                sol if is_sequence(sol, set) else [sol])]\n            funcs = set().union(*funcs)\n            if len(funcs) != 1:\n                raise ValueError(\n                    'must pass func arg to checkodesol for this case.')\n            func = funcs.pop()\n    if not isinstance(func, AppliedUndef) or len(func.args) != 1:\n        raise ValueError(\n            \"func must be a function of one variable, not %s\" % func)\n    if is_sequence(sol, set):\n        return type(sol)([checkodesol(ode, i, order=order, solve_for_func=solve_for_func) for i in sol])\n\n    if not isinstance(sol, Equality):\n        sol = Eq(func, sol)\n    elif sol.rhs == func:\n        sol = sol.reversed\n\n    if order == 'auto':\n        order = ode_order(ode, func)\n    solved = sol.lhs == func and not sol.rhs.has(func)\n    if solve_for_func and not solved:\n        rhs = solve(sol, func)\n        if rhs:\n            eqs = [Eq(func, t) for t in rhs]\n            if len(rhs) == 1:\n                eqs = eqs[0]\n            return checkodesol(ode, eqs, order=order,\n                solve_for_func=False)\n\n    x = func.args[0]\n\n    # Handle series solutions here\n    if sol.has(Order):\n        assert sol.lhs == func\n        Oterm = sol.rhs.getO()\n        solrhs = sol.rhs.removeO()\n\n        Oexpr = Oterm.expr\n        assert isinstance(Oexpr, Pow)\n        sorder = Oexpr.exp\n        assert Oterm == Order(x**sorder)\n\n        odesubs = (ode.lhs-ode.rhs).subs(func, solrhs).doit().expand()\n\n        neworder = Order(x**(sorder - order))\n        odesubs = odesubs + neworder\n        assert odesubs.getO() == neworder\n        residual = odesubs.removeO()\n\n        return (residual == 0, residual)\n\n    s = True\n    testnum = 0\n    while s:\n        if testnum == 0:\n            # First pass, try substituting a solved solution directly into the\n            # ODE. This has the highest chance of succeeding.\n            ode_diff = ode.lhs - ode.rhs\n\n            if sol.lhs == func:\n                s = sub_func_doit(ode_diff, func, sol.rhs)\n                s = besselsimp(s)\n            else:\n                testnum += 1\n                continue\n            ss = simplify(s)\n            if ss:\n                # with the new numer_denom in power.py, if we do a simple\n                # expansion then testnum == 0 verifies all solutions.\n                s = ss.expand(force=True)\n            else:\n                s = 0\n            testnum += 1\n        elif testnum == 1:\n            # Second pass. If we cannot substitute f, try seeing if the nth\n            # derivative is equal, this will only work for odes that are exact,\n            # by definition.\n            s = simplify(\n                trigsimp(diff(sol.lhs, x, order) - diff(sol.rhs, x, order)) -\n                trigsimp(ode.lhs) + trigsimp(ode.rhs))\n            # s2 = simplify(\n            #     diff(sol.lhs, x, order) - diff(sol.rhs, x, order) - \\\n            #     ode.lhs + ode.rhs)\n            testnum += 1\n        elif testnum == 2:\n            # Third pass. Try solving for df/dx and substituting that into the\n            # ODE. Thanks to Chris Smith for suggesting this method.  Many of\n            # the comments below are his, too.\n            # The method:\n            # - Take each of 1..n derivatives of the solution.\n            # - Solve each nth derivative for d^(n)f/dx^(n)\n            #   (the differential of that order)\n            # - Back substitute into the ODE in decreasing order\n            #   (i.e., n, n-1, ...)\n            # - Check the result for zero equivalence\n            if sol.lhs == func and not sol.rhs.has(func):\n                diffsols = {0: sol.rhs}\n            elif sol.rhs == func and not sol.lhs.has(func):\n                diffsols = {0: sol.lhs}\n            else:\n                diffsols = {}\n            sol = sol.lhs - sol.rhs\n            for i in range(1, order + 1):\n                # Differentiation is a linear operator, so there should always\n                # be 1 solution. Nonetheless, we test just to make sure.\n                # We only need to solve once.  After that, we automatically\n                # have the solution to the differential in the order we want.\n                if i == 1:\n                    ds = sol.diff(x)\n                    try:\n                        sdf = solve(ds, func.diff(x, i))\n                        if not sdf:\n                            raise NotImplementedError\n                    except NotImplementedError:\n                        testnum += 1\n                        break\n                    else:\n                        diffsols[i] = sdf[0]\n                else:\n                    # This is what the solution says df/dx should be.\n                    diffsols[i] = diffsols[i - 1].diff(x)\n\n            # Make sure the above didn't fail.\n            if testnum > 2:\n                continue\n            else:\n                # Substitute it into ODE to check for self consistency.\n                lhs, rhs = ode.lhs, ode.rhs\n                for i in range(order, -1, -1):\n                    if i == 0 and 0 not in diffsols:\n                        # We can only substitute f(x) if the solution was\n                        # solved for f(x).\n                        break\n                    lhs = sub_func_doit(lhs, func.diff(x, i), diffsols[i])\n                    rhs = sub_func_doit(rhs, func.diff(x, i), diffsols[i])\n                    ode_or_bool = Eq(lhs, rhs)\n                    ode_or_bool = simplify(ode_or_bool)\n\n                    if isinstance(ode_or_bool, (bool, BooleanAtom)):\n                        if ode_or_bool:\n                            lhs = rhs = S.Zero\n                    else:\n                        lhs = ode_or_bool.lhs\n                        rhs = ode_or_bool.rhs\n                # No sense in overworking simplify -- just prove that the\n                # numerator goes to zero\n                num = trigsimp((lhs - rhs).as_numer_denom()[0])\n                # since solutions are obtained using force=True we test\n                # using the same level of assumptions\n                ## replace function with dummy so assumptions will work\n                _func = Dummy('func')\n                num = num.subs(func, _func)\n                ## posify the expression\n                num, reps = posify(num)\n                s = simplify(num).xreplace(reps).xreplace({_func: func})\n                testnum += 1\n        else:\n            break\n\n    if not s:\n        return (True, s)\n    elif s is True:  # The code above never was able to change s\n        raise NotImplementedError(\"Unable to test if \" + str(sol) +\n            \" is a solution to \" + str(ode) + \".\")\n    else:\n        return (False, s)", "output": "def checkodesol(ode, sol, func=None, order='auto', solve_for_func=True):\n    r\"\"\"\n    Substitutes ``sol`` into ``ode`` and checks that the result is ``0``.\n\n    This only works when ``func`` is one function, like `f(x)`.  ``sol`` can\n    be a single solution or a list of solutions.  Each solution may be an\n    :py:class:`~sympy.core.relational.Equality` that the solution satisfies,\n    e.g. ``Eq(f(x), C1), Eq(f(x) + C1, 0)``; or simply an\n    :py:class:`~sympy.core.expr.Expr`, e.g. ``f(x) - C1``. In most cases it\n    will not be necessary to explicitly identify the function, but if the\n    function cannot be inferred from the original equation it can be supplied\n    through the ``func`` argument.\n\n    If a sequence of solutions is passed, the same sort of container will be\n    used to return the result for each solution.\n\n    It tries the following methods, in order, until it finds zero equivalence:\n\n    1. Substitute the solution for `f` in the original equation.  This only\n       works if ``ode`` is solved for `f`.  It will attempt to solve it first\n       unless ``solve_for_func == False``.\n    2. Take `n` derivatives of the solution, where `n` is the order of\n       ``ode``, and check to see if that is equal to the solution.  This only\n       works on exact ODEs.\n    3. Take the 1st, 2nd, ..., `n`\\th derivatives of the solution, each time\n       solving for the derivative of `f` of that order (this will always be\n       possible because `f` is a linear operator). Then back substitute each\n       derivative into ``ode`` in reverse order.\n\n    This function returns a tuple.  The first item in the tuple is ``True`` if\n    the substitution results in ``0``, and ``False`` otherwise. The second\n    item in the tuple is what the substitution results in.  It should always\n    be ``0`` if the first item is ``True``. Sometimes this function will\n    return ``False`` even when an expression is identically equal to ``0``.\n    This happens when :py:meth:`~sympy.simplify.simplify.simplify` does not\n    reduce the expression to ``0``.  If an expression returned by this\n    function vanishes identically, then ``sol`` really is a solution to\n    the ``ode``.\n\n    If this function seems to hang, it is probably because of a hard\n    simplification.\n\n    To use this function to test, test the first item of the tuple.\n\n    Examples\n    ========\n\n    >>> from sympy import Eq, Function, checkodesol, symbols\n    >>> x, C1 = symbols('x,C1')\n    >>> f = Function('f')\n    >>> checkodesol(f(x).diff(x), Eq(f(x), C1))\n    (True, 0)\n    >>> assert checkodesol(f(x).diff(x), C1)[0]\n    >>> assert not checkodesol(f(x).diff(x), x)[0]\n    >>> checkodesol(f(x).diff(x, 2), x**2)\n    (False, 2)\n\n    \"\"\"\n    if not isinstance(ode, Equality):\n        ode = Eq(ode, 0)\n    if func is None:\n        try:\n            _, func = _preprocess(ode.lhs)\n        except ValueError:\n            funcs = [s.atoms(AppliedUndef) for s in (\n                sol if is_sequence(sol, set) else [sol])]\n            funcs = set().union(*funcs)\n            if len(funcs) != 1:\n                raise ValueError(\n                    'must pass func arg to checkodesol for this case.')\n            func = funcs.pop()\n    if not isinstance(func, AppliedUndef) or len(func.args) != 1:\n        raise ValueError(\n            \"func must be a function of one variable, not %s\" % func)\n    if is_sequence(sol, set):\n        return type(sol)([checkodesol(ode, i, order=order, solve_for_func=solve_for_func) for i in sol])\n\n    if not isinstance(sol, Equality):\n        sol = Eq(func, sol)\n    elif sol.rhs == func:\n        sol = sol.reversed\n\n    if order == 'auto':\n        order = ode_order(ode, func)\n    solved = sol.lhs == func and not sol.rhs.has(func)\n    if solve_for_func and not solved:\n        rhs = solve(sol, func)\n        if rhs:\n            eqs = [Eq(func, t) for t in rhs]\n            if len(rhs) == 1:\n                eqs = eqs[0]\n            return checkodesol(ode, eqs, order=order,\n                solve_for_func=False)\n\n    x = func.args[0]\n\n    # Handle series solutions here\n    if sol.has(Order):\n        assert sol.lhs == func\n        Oterm = sol.rhs.getO()\n        solrhs = sol.rhs.removeO()\n\n        Oexpr = Oterm.expr\n        assert isinstance(Oexpr, Pow)\n        sorder = Oexpr.exp\n        assert Oterm == Order(x**sorder)\n\n        odesubs = (ode.lhs-ode.rhs).subs(func, solrhs).doit().expand()\n\n        neworder = Order(x**(sorder - order))\n        odesubs = odesubs + neworder\n        assert odesubs.getO() == neworder\n        residual = odesubs.removeO()\n\n        return (residual == 0, residual)\n\n    s = True\n    testnum = 0\n    while s:\n        if testnum == 0:\n            # First pass, try substituting a solved solution directly into the\n            # ODE. This has the highest chance of succeeding.\n            ode_diff = ode.lhs - ode.rhs\n\n            if sol.lhs == func:\n                s = sub_func_doit(ode_diff, func, sol.rhs)\n                s = besselsimp(s)\n            else:\n                testnum += 1\n                continue\n            ss = simplify(s.rewrite(exp))\n            if ss:\n                # with the new numer_denom in power.py, if we do a simple\n                # expansion then testnum == 0 verifies all solutions.\n                s = ss.expand(force=True)\n            else:\n                s = 0\n            testnum += 1\n        elif testnum == 1:\n            # Second pass. If we cannot substitute f, try seeing if the nth\n            # derivative is equal, this will only work for odes that are exact,\n            # by definition.\n            s = simplify(\n                trigsimp(diff(sol.lhs, x, order) - diff(sol.rhs, x, order)) -\n                trigsimp(ode.lhs) + trigsimp(ode.rhs))\n            # s2 = simplify(\n            #     diff(sol.lhs, x, order) - diff(sol.rhs, x, order) - \\\n            #     ode.lhs + ode.rhs)\n            testnum += 1\n        elif testnum == 2:\n            # Third pass. Try solving for df/dx and substituting that into the\n            # ODE. Thanks to Chris Smith for suggesting this method.  Many of\n            # the comments below are his, too.\n            # The method:\n            # - Take each of 1..n derivatives of the solution.\n            # - Solve each nth derivative for d^(n)f/dx^(n)\n            #   (the differential of that order)\n            # - Back substitute into the ODE in decreasing order\n            #   (i.e., n, n-1, ...)\n            # - Check the result for zero equivalence\n            if sol.lhs == func and not sol.rhs.has(func):\n                diffsols = {0: sol.rhs}\n            elif sol.rhs == func and not sol.lhs.has(func):\n                diffsols = {0: sol.lhs}\n            else:\n                diffsols = {}\n            sol = sol.lhs - sol.rhs\n            for i in range(1, order + 1):\n                # Differentiation is a linear operator, so there should always\n                # be 1 solution. Nonetheless, we test just to make sure.\n                # We only need to solve once.  After that, we automatically\n                # have the solution to the differential in the order we want.\n                if i == 1:\n                    ds = sol.diff(x)\n                    try:\n                        sdf = solve(ds, func.diff(x, i))\n                        if not sdf:\n                            raise NotImplementedError\n                    except NotImplementedError:\n                        testnum += 1\n                        break\n                    else:\n                        diffsols[i] = sdf[0]\n                else:\n                    # This is what the solution says df/dx should be.\n                    diffsols[i] = diffsols[i - 1].diff(x)\n\n            # Make sure the above didn't fail.\n            if testnum > 2:\n                continue\n            else:\n                # Substitute it into ODE to check for self consistency.\n                lhs, rhs = ode.lhs, ode.rhs\n                for i in range(order, -1, -1):\n                    if i == 0 and 0 not in diffsols:\n                        # We can only substitute f(x) if the solution was\n                        # solved for f(x).\n                        break\n                    lhs = sub_func_doit(lhs, func.diff(x, i), diffsols[i])\n                    rhs = sub_func_doit(rhs, func.diff(x, i), diffsols[i])\n                    ode_or_bool = Eq(lhs, rhs)\n                    ode_or_bool = simplify(ode_or_bool)\n\n                    if isinstance(ode_or_bool, (bool, BooleanAtom)):\n                        if ode_or_bool:\n                            lhs = rhs = S.Zero\n                    else:\n                        lhs = ode_or_bool.lhs\n                        rhs = ode_or_bool.rhs\n                # No sense in overworking simplify -- just prove that the\n                # numerator goes to zero\n                num = trigsimp((lhs - rhs).as_numer_denom()[0])\n                # since solutions are obtained using force=True we test\n                # using the same level of assumptions\n                ## replace function with dummy so assumptions will work\n                _func = Dummy('func')\n                num = num.subs(func, _func)\n                ## posify the expression\n                num, reps = posify(num)\n                s = simplify(num).xreplace(reps).xreplace({_func: func})\n                testnum += 1\n        else:\n            break\n\n    if not s:\n        return (True, s)\n    elif s is True:  # The code above never was able to change s\n        raise NotImplementedError(\"Unable to test if \" + str(sol) +\n            \" is a solution to \" + str(ode) + \".\")\n    else:\n        return (False, s)"}
{"input": "def convert_detection_postprocess(self, op):\n        \"\"\"Convert TFLite_Detection_PostProcess\"\"\"\n        flexbuffer = op.CustomOptionsAsNumpy().tobytes()\n        custom_options = FlexBufferDecoder(flexbuffer).decode()\n\n        if \"use_regular_nms\" in custom_options:\n            if custom_options[\"use_regular_nms\"]:\n                raise tvm.error.OpAttributeUnImplemented(\n                    \"use_regular_nms=True is not yet supported for operator {}.\".format(\n                        \"TFLite_Detection_PostProcess\"\n                    )\n                )\n\n        inputs = self.get_input_tensors(op)\n        assert len(inputs) == 3, \"inputs length should be 3\"\n        cls_pred = self.get_expr(inputs[1].tensor_idx)\n        loc_prob = self.get_expr(inputs[0].tensor_idx)\n        batch_size = inputs[1].tensor.Shape(0)\n        anchor_values = self.get_tensor_value(inputs[2])\n        anchor_boxes = len(anchor_values)\n        anchor_type = self.get_tensor_type_str(inputs[2].tensor.Type())\n        anchor_expr = self.exp_tab.new_const(anchor_values, dtype=anchor_type)\n\n        if inputs[0].qnn_params:\n            loc_prob = _qnn.op.dequantize(\n                data=loc_prob,\n                input_scale=inputs[0].qnn_params[\"scale\"],\n                input_zero_point=inputs[0].qnn_params[\"zero_point\"],\n            )\n        if inputs[1].qnn_params:\n            cls_pred = _qnn.op.dequantize(\n                data=cls_pred,\n                input_scale=inputs[1].qnn_params[\"scale\"],\n                input_zero_point=inputs[1].qnn_params[\"zero_point\"],\n            )\n        if inputs[2].qnn_params:\n            anchor_expr = _qnn.op.dequantize(\n                data=anchor_expr,\n                input_scale=inputs[2].qnn_params[\"scale\"],\n                input_zero_point=inputs[2].qnn_params[\"zero_point\"],\n            )\n\n        # reshape the cls_pred and loc_prob tensors so\n        # they can be consumed by multibox_transform_loc\n        cls_pred = _op.transpose(cls_pred, [0, 2, 1])\n        # loc_prob coords are in yxhw format\n        # need to convert to xywh\n        loc_coords = _op.split(loc_prob, 4, axis=2)\n        loc_prob = _op.concatenate(\n            [loc_coords[1], loc_coords[0], loc_coords[3], loc_coords[2]], axis=2\n        )\n        loc_prob = _op.reshape(loc_prob, [batch_size, anchor_boxes * 4])\n\n        # anchor coords are in yxhw format\n        # need to convert to ltrb\n        anchor_coords = _op.split(anchor_expr, 4, axis=1)\n        anchor_y = anchor_coords[0]\n        anchor_x = anchor_coords[1]\n        anchor_h = anchor_coords[2]\n        anchor_w = anchor_coords[3]\n        plus_half = _expr.const(0.5, dtype=\"float32\")\n        minus_half = _expr.const(-0.5, dtype=\"float32\")\n        anchor_l = _op.add(anchor_x, _op.multiply(anchor_w, minus_half))\n        anchor_r = _op.add(anchor_x, _op.multiply(anchor_w, plus_half))\n        anchor_t = _op.add(anchor_y, _op.multiply(anchor_h, minus_half))\n        anchor_b = _op.add(anchor_y, _op.multiply(anchor_h, plus_half))\n        anchor_expr = _op.concatenate([anchor_l, anchor_t, anchor_r, anchor_b], axis=1)\n        anchor_expr = _op.expand_dims(anchor_expr, 0)\n\n        # attributes for multibox_transform_loc\n        multibox_transform_loc_attrs = {}\n        multibox_transform_loc_attrs[\"clip\"] = False\n        multibox_transform_loc_attrs[\"threshold\"] = custom_options[\"nms_score_threshold\"]\n        multibox_transform_loc_attrs[\"variances\"] = (\n            1 / custom_options[\"x_scale\"],\n            1 / custom_options[\"y_scale\"],\n            1 / custom_options[\"w_scale\"],\n            1 / custom_options[\"h_scale\"],\n        )\n\n        # attributes for non_max_suppression\n        non_max_suppression_attrs = {}\n        non_max_suppression_attrs[\"return_indices\"] = False\n        non_max_suppression_attrs[\"iou_threshold\"] = custom_options[\"nms_iou_threshold\"]\n        non_max_suppression_attrs[\"force_suppress\"] = False\n        non_max_suppression_attrs[\"top_k\"] = anchor_boxes\n        non_max_suppression_attrs[\"max_output_size\"] = custom_options[\"max_detections\"]\n        non_max_suppression_attrs[\"invalid_to_bottom\"] = False\n\n        ret = _op.vision.multibox_transform_loc(\n            cls_pred, loc_prob, anchor_expr, **multibox_transform_loc_attrs\n        )\n        ret = _op.vision.non_max_suppression(ret[0], ret[1], ret[1], **non_max_suppression_attrs)\n        ret = _op.vision.get_valid_counts(ret, 0)\n        valid_count = ret[0]\n        # keep only the top 'max_detections' rows\n        ret = _op.strided_slice(\n            ret[1], [0, 0, 0], [batch_size, custom_options[\"max_detections\"], anchor_boxes]\n        )\n        # the output needs some reshaping to match tflite\n        ret = _op.split(ret, 6, axis=2)\n        cls_ids = _op.reshape(ret[0], [batch_size, -1])\n        scores = _op.reshape(ret[1], [batch_size, -1])\n        boxes = _op.concatenate([ret[3], ret[2], ret[5], ret[4]], axis=2)\n        ret = _expr.TupleWrapper(_expr.Tuple([boxes, cls_ids, scores, valid_count]), size=4)\n        return ret", "output": "def convert_detection_postprocess(self, op):\n        \"\"\"Convert TFLite_Detection_PostProcess\"\"\"\n        flexbuffer = op.CustomOptionsAsNumpy().tobytes()\n        custom_options = FlexBufferDecoder(flexbuffer).decode()\n\n        if \"use_regular_nms\" in custom_options:\n            if custom_options[\"use_regular_nms\"]:\n                raise tvm.error.OpAttributeUnImplemented(\n                    \"use_regular_nms=True is not yet supported for operator {}.\".format(\n                        \"TFLite_Detection_PostProcess\"\n                    )\n                )\n\n        inputs = self.get_input_tensors(op)\n        assert len(inputs) == 3, \"inputs length should be 3\"\n        cls_pred = self.get_expr(inputs[1].tensor_idx)\n        loc_prob = self.get_expr(inputs[0].tensor_idx)\n        batch_size = inputs[1].tensor.Shape(0)\n        anchor_values = self.get_tensor_value(inputs[2])\n        anchor_boxes = len(anchor_values)\n        anchor_type = self.get_tensor_type_str(inputs[2].tensor.Type())\n        anchor_expr = self.exp_tab.new_const(anchor_values, dtype=anchor_type)\n\n        if inputs[0].qnn_params:\n            loc_prob = _qnn.op.dequantize(\n                data=loc_prob,\n                input_scale=inputs[0].qnn_params[\"scale\"],\n                input_zero_point=inputs[0].qnn_params[\"zero_point\"],\n            )\n        if inputs[1].qnn_params:\n            cls_pred = _qnn.op.dequantize(\n                data=cls_pred,\n                input_scale=inputs[1].qnn_params[\"scale\"],\n                input_zero_point=inputs[1].qnn_params[\"zero_point\"],\n            )\n        if inputs[2].qnn_params:\n            anchor_expr = _qnn.op.dequantize(\n                data=anchor_expr,\n                input_scale=inputs[2].qnn_params[\"scale\"],\n                input_zero_point=inputs[2].qnn_params[\"zero_point\"],\n            )\n\n        # reshape the cls_pred and loc_prob tensors so\n        # they can be consumed by multibox_transform_loc\n        cls_pred = _op.transpose(cls_pred, [0, 2, 1])\n        # loc_prob coords are in yxhw format\n        # need to convert to xywh\n        loc_coords = _op.split(loc_prob, 4, axis=2)\n        loc_prob = _op.concatenate(\n            [loc_coords[1], loc_coords[0], loc_coords[3], loc_coords[2]], axis=2\n        )\n        loc_prob = _op.reshape(loc_prob, [batch_size, anchor_boxes * 4])\n\n        # anchor coords are in yxhw format\n        # need to convert to ltrb\n        anchor_coords = _op.split(anchor_expr, 4, axis=1)\n        anchor_y = anchor_coords[0]\n        anchor_x = anchor_coords[1]\n        anchor_h = anchor_coords[2]\n        anchor_w = anchor_coords[3]\n        plus_half = _expr.const(0.5, dtype=\"float32\")\n        minus_half = _expr.const(-0.5, dtype=\"float32\")\n        anchor_l = _op.add(anchor_x, _op.multiply(anchor_w, minus_half))\n        anchor_r = _op.add(anchor_x, _op.multiply(anchor_w, plus_half))\n        anchor_t = _op.add(anchor_y, _op.multiply(anchor_h, minus_half))\n        anchor_b = _op.add(anchor_y, _op.multiply(anchor_h, plus_half))\n        anchor_expr = _op.concatenate([anchor_l, anchor_t, anchor_r, anchor_b], axis=1)\n        anchor_expr = _op.expand_dims(anchor_expr, 0)\n\n        # attributes for multibox_transform_loc\n        multibox_transform_loc_attrs = {}\n        multibox_transform_loc_attrs[\"clip\"] = False\n        multibox_transform_loc_attrs[\"threshold\"] = custom_options[\"nms_score_threshold\"]\n        multibox_transform_loc_attrs[\"variances\"] = (\n            1 / custom_options[\"x_scale\"],\n            1 / custom_options[\"y_scale\"],\n            1 / custom_options[\"w_scale\"],\n            1 / custom_options[\"h_scale\"],\n        )\n\n        # attributes for non_max_suppression\n        non_max_suppression_attrs = {}\n        non_max_suppression_attrs[\"return_indices\"] = False\n        non_max_suppression_attrs[\"iou_threshold\"] = custom_options[\"nms_iou_threshold\"]\n        non_max_suppression_attrs[\"force_suppress\"] = False\n        non_max_suppression_attrs[\"top_k\"] = anchor_boxes\n        non_max_suppression_attrs[\"max_output_size\"] = custom_options[\"max_detections\"]\n        non_max_suppression_attrs[\"invalid_to_bottom\"] = False\n\n        ret = _op.vision.multibox_transform_loc(\n            cls_pred, loc_prob, anchor_expr, **multibox_transform_loc_attrs\n        )\n        ret = _op.vision.non_max_suppression(ret[0], ret[1], ret[1], **non_max_suppression_attrs)\n        ret = _op.vision.get_valid_counts(ret, 0)\n        valid_count = ret[0]\n        # keep only the top 'max_detections' rows\n        ret = _op.strided_slice(\n            ret[1], [0, 0, 0], [batch_size, custom_options[\"max_detections\"], 6]\n        )\n        # the output needs some reshaping to match tflite\n        ret = _op.split(ret, 6, axis=2)\n        cls_ids = _op.reshape(ret[0], [batch_size, -1])\n        scores = _op.reshape(ret[1], [batch_size, -1])\n        boxes = _op.concatenate([ret[3], ret[2], ret[5], ret[4]], axis=2)\n        ret = _expr.TupleWrapper(_expr.Tuple([boxes, cls_ids, scores, valid_count]), size=4)\n        return ret"}
{"input": "def run_patches():\n\t# update module\n\tdt_module = {'LC PR Detail':'Stock', 'Landed Cost Detail':'Stock', 'Comment Widget Record': 'Core', 'Tag':'Core', 'Tag Detail': 'Core', 'POS Settings': 'Accounts'}\n\tfor d in dt_module.keys():\n\t\tsql(\"update `tabDocType` set module = '%s' where name = '%s'\" % (dt_module[d], d))\n\tdelete_unwanted_mappers()\n\tdelete_unwanted_doctypes()\n\tsql(\"start transaction\")\n\tdelete_unwanted_pages()\n\n\tdelete_unwanted_search_criteria()\n\tdelete_unwanted_modules()\n\n\trename_merge_modules()\n\tsync_roles()\n\tsync_mapper()\n\t\n\n\t# landed cost wizard link in stock\n\treload_doc('stock', 'Module Def', 'stock')\n\t\n\tsql(\"commit\")", "output": "def run_patches():\n\t# update module\n\tdt_module = {'LC PR Detail':'Stock', 'Landed Cost Detail':'Stock', 'Comment Widget Record': 'Core', 'Tag':'Core', 'Tag Detail': 'Core', 'POS Settings': 'Accounts', 'Salary Structure Details': 'Accounts'}\n\tfor d in dt_module.keys():\n\t\tsql(\"update `tabDocType` set module = '%s' where name = '%s'\" % (dt_module[d], d))\n\tdelete_unwanted_mappers()\n\tdelete_unwanted_doctypes()\n\tsql(\"start transaction\")\n\tdelete_unwanted_pages()\n\n\tdelete_unwanted_search_criteria()\n\tdelete_unwanted_modules()\n\n\trename_merge_modules()\n\tsync_roles()\n\tsync_mapper()\n\t\n\n\t# landed cost wizard link in stock\n\treload_doc('stock', 'Module Def', 'stock')\n\t\n\tsql(\"commit\")"}
{"input": "def main():\n    module = AnsibleModule(\n        argument_spec = dict(\n            src = dict(required=True),\n            dest = dict(required=True),\n            dest_port = dict(default=None, type='int'),\n            delete = dict(default='no', type='bool'),\n            private_key = dict(default=None),\n            rsync_path = dict(default=None),\n            _local_rsync_path = dict(default='rsync', type='path'),\n            _substitute_controller = dict(default='no', type='bool'),\n            archive = dict(default='yes', type='bool'),\n            checksum = dict(default='no', type='bool'),\n            compress = dict(default='yes', type='bool'),\n            existing_only = dict(default='no', type='bool'),\n            dirs  = dict(default='no', type='bool'),\n            recursive = dict(type='bool'),\n            links = dict(type='bool'),\n            copy_links = dict(default='no', type='bool'),\n            perms = dict(type='bool'),\n            times = dict(type='bool'),\n            owner = dict(type='bool'),\n            group = dict(type='bool'),\n            set_remote_user = dict(default='yes', type='bool'),\n            rsync_timeout = dict(type='int', default=0),\n            rsync_opts = dict(type='list'),\n            ssh_args = dict(type='str'),\n            partial = dict(default='no', type='bool'),\n            verify_host = dict(default='no', type='bool'),\n            mode = dict(default='push', choices=['push', 'pull']),\n        ),\n        supports_check_mode = True\n    )\n\n    if module.params['_substitute_controller']:\n        try:\n            source = substitute_controller(module.params['src'])\n            dest = substitute_controller(module.params['dest'])\n        except ValueError:\n            module.fail_json(msg='Could not determine controller hostname for rsync to send to')\n    else:\n        source = module.params['src']\n        dest = module.params['dest']\n    dest_port = module.params['dest_port']\n    delete = module.params['delete']\n    private_key = module.params['private_key']\n    rsync_path = module.params['rsync_path']\n    rsync = module.params.get('_local_rsync_path', 'rsync')\n    rsync_timeout = module.params.get('rsync_timeout', 'rsync_timeout')\n    archive = module.params['archive']\n    checksum = module.params['checksum']\n    compress = module.params['compress']\n    existing_only = module.params['existing_only']\n    dirs = module.params['dirs']\n    partial = module.params['partial']\n    # the default of these params depends on the value of archive\n    recursive = module.params['recursive']\n    links = module.params['links']\n    copy_links = module.params['copy_links']\n    perms = module.params['perms']\n    times = module.params['times']\n    owner = module.params['owner']\n    group = module.params['group']\n    rsync_opts = module.params['rsync_opts']\n    ssh_args = module.params['ssh_args']\n    verify_host = module.params['verify_host']\n\n    if '/' not in rsync:\n        rsync = module.get_bin_path(rsync, required=True)\n\n    cmd = [rsync, '--delay-updates', '-F']\n    if compress:\n        cmd.append('--compress')\n    if rsync_timeout:\n        cmd.append('--timeout=%s' % rsync_timeout)\n    if module.check_mode:\n        cmd.append('--dry-run')\n    if delete:\n        cmd.append('--delete-after')\n    if existing_only:\n        cmd.append('--existing')\n    if checksum:\n        cmd.append('--checksum')\n    if copy_links:\n        cmd.append('--copy-links')\n    if archive:\n        cmd.append('--archive')\n        if recursive is False:\n            cmd.append('--no-recursive')\n        if links is False:\n            cmd.append('--no-links')\n        if perms is False:\n            cmd.append('--no-perms')\n        if times is False:\n            cmd.append('--no-times')\n        if owner is False:\n            cmd.append('--no-owner')\n        if group is False:\n            cmd.append('--no-group')\n    else:\n        if recursive is True:\n            cmd.append('--recursive')\n        if links is True:\n            cmd.append('--links')\n        if perms is True:\n            cmd.append('--perms')\n        if times is True:\n            cmd.append('--times')\n        if owner is True:\n            cmd.append('--owner')\n        if group is True:\n            cmd.append('--group')\n    if dirs:\n        cmd.append('--dirs')\n\n    if source.startswith('rsync://') and dest.startswith('rsync://'):\n        module.fail_json(msg='either src or dest must be a localhost', rc=1)\n\n    if is_rsh_needed(source, dest):\n        ssh_cmd = [module.get_bin_path('ssh', required=True), '-S', 'none']\n        if private_key is not None:\n            ssh_cmd.extend(['-i', private_key])\n        # If the user specified a port value\n        # Note:  The action plugin takes care of setting this to a port from\n        # inventory if the user didn't specify an explicit dest_port\n        if dest_port is not None:\n            ssh_cmd.extend(['-o', 'Port=%s' % dest_port])\n        if not verify_host:\n            ssh_cmd.extend(['-o', 'StrictHostKeyChecking=no'])\n        ssh_cmd_str = ' '.join(shlex_quote(arg) for arg in ssh_cmd)\n        if ssh_args:\n            ssh_cmd_str += ' %s' % ssh_args\n        cmd.append('--rsh=%s' % ssh_cmd_str)\n\n    if rsync_path:\n        cmd.append('--rsync-path=%s' % rsync_path)\n\n    if rsync_opts:\n        cmd.extend(rsync_opts)\n\n    if partial:\n        cmd.append('--partial')\n\n    changed_marker = '<<CHANGED>>'\n    cmd.append('--out-format=' + changed_marker + '%i %n%L')\n\n    # expand the paths\n    if '@' not in source:\n        source = os.path.expanduser(source)\n    if '@' not in dest:\n        dest = os.path.expanduser(dest)\n\n    cmd.append(source)\n    cmd.append(dest)\n    cmdstr = ' '.join(cmd)\n    (rc, out, err) = module.run_command(cmd)\n    if rc:\n        return module.fail_json(msg=err, rc=rc, cmd=cmdstr)\n    else:\n        changed = changed_marker in out\n        out_clean = out.replace(changed_marker, '')\n        out_lines = out_clean.split('\\n')\n        while '' in out_lines:\n            out_lines.remove('')\n        if module._diff:\n            diff = {'prepared': out_clean}\n            return module.exit_json(changed=changed, msg=out_clean,\n                                    rc=rc, cmd=cmdstr, stdout_lines=out_lines,\n                                    diff=diff)\n        else:\n            return module.exit_json(changed=changed, msg=out_clean,\n                                    rc=rc, cmd=cmdstr, stdout_lines=out_lines)", "output": "def main():\n    module = AnsibleModule(\n        argument_spec = dict(\n            src = dict(required=True),\n            dest = dict(required=True),\n            dest_port = dict(default=None, type='int'),\n            delete = dict(default='no', type='bool'),\n            private_key = dict(default=None),\n            rsync_path = dict(default=None),\n            _local_rsync_path = dict(default='rsync', type='path'),\n            _substitute_controller = dict(default='no', type='bool'),\n            archive = dict(default='yes', type='bool'),\n            checksum = dict(default='no', type='bool'),\n            compress = dict(default='yes', type='bool'),\n            existing_only = dict(default='no', type='bool'),\n            dirs  = dict(default='no', type='bool'),\n            recursive = dict(type='bool'),\n            links = dict(type='bool'),\n            copy_links = dict(default='no', type='bool'),\n            perms = dict(type='bool'),\n            times = dict(type='bool'),\n            owner = dict(type='bool'),\n            group = dict(type='bool'),\n            set_remote_user = dict(default='yes', type='bool'),\n            rsync_timeout = dict(type='int', default=0),\n            rsync_opts = dict(type='list'),\n            ssh_args = dict(type='str'),\n            partial = dict(default='no', type='bool'),\n            verify_host = dict(default='no', type='bool'),\n            mode = dict(default='push', choices=['push', 'pull']),\n        ),\n        supports_check_mode = True\n    )\n\n    if module.params['_substitute_controller']:\n        try:\n            source = substitute_controller(module.params['src'])\n            dest = substitute_controller(module.params['dest'])\n        except ValueError:\n            module.fail_json(msg='Could not determine controller hostname for rsync to send to')\n    else:\n        source = module.params['src']\n        dest = module.params['dest']\n    dest_port = module.params['dest_port']\n    delete = module.params['delete']\n    private_key = module.params['private_key']\n    rsync_path = module.params['rsync_path']\n    rsync = module.params.get('_local_rsync_path', 'rsync')\n    rsync_timeout = module.params.get('rsync_timeout', 'rsync_timeout')\n    archive = module.params['archive']\n    checksum = module.params['checksum']\n    compress = module.params['compress']\n    existing_only = module.params['existing_only']\n    dirs = module.params['dirs']\n    partial = module.params['partial']\n    # the default of these params depends on the value of archive\n    recursive = module.params['recursive']\n    links = module.params['links']\n    copy_links = module.params['copy_links']\n    perms = module.params['perms']\n    times = module.params['times']\n    owner = module.params['owner']\n    group = module.params['group']\n    rsync_opts = module.params['rsync_opts']\n    ssh_args = module.params['ssh_args']\n    verify_host = module.params['verify_host']\n\n    if '/' not in rsync:\n        rsync = module.get_bin_path(rsync, required=True)\n\n    cmd = [rsync, '--delay-updates', '-F']\n    if compress:\n        cmd.append('--compress')\n    if rsync_timeout:\n        cmd.append('--timeout=%s' % rsync_timeout)\n    if module.check_mode:\n        cmd.append('--dry-run')\n    if delete:\n        cmd.append('--delete-after')\n    if existing_only:\n        cmd.append('--existing')\n    if checksum:\n        cmd.append('--checksum')\n    if copy_links:\n        cmd.append('--copy-links')\n    if archive:\n        cmd.append('--archive')\n        if recursive is False:\n            cmd.append('--no-recursive')\n        if links is False:\n            cmd.append('--no-links')\n        if perms is False:\n            cmd.append('--no-perms')\n        if times is False:\n            cmd.append('--no-times')\n        if owner is False:\n            cmd.append('--no-owner')\n        if group is False:\n            cmd.append('--no-group')\n    else:\n        if recursive is True:\n            cmd.append('--recursive')\n        if links is True:\n            cmd.append('--links')\n        if perms is True:\n            cmd.append('--perms')\n        if times is True:\n            cmd.append('--times')\n        if owner is True:\n            cmd.append('--owner')\n        if group is True:\n            cmd.append('--group')\n    if dirs:\n        cmd.append('--dirs')\n\n    if source.startswith('rsync://') and dest.startswith('rsync://'):\n        module.fail_json(msg='either src or dest must be a localhost', rc=1)\n\n    if is_rsh_needed(source, dest):\n        ssh_cmd = [module.get_bin_path('ssh', required=True), '-S', 'none']\n        if private_key is not None:\n            ssh_cmd.extend(['-i', private_key])\n        # If the user specified a port value\n        # Note:  The action plugin takes care of setting this to a port from\n        # inventory if the user didn't specify an explicit dest_port\n        if dest_port is not None:\n            ssh_cmd.extend(['-o', 'Port=%s' % dest_port])\n        if not verify_host:\n            ssh_cmd.extend(['-o', 'StrictHostKeyChecking=no', '-o', 'UserKnownHostsFile=/dev/null'])\n        ssh_cmd_str = ' '.join(shlex_quote(arg) for arg in ssh_cmd)\n        if ssh_args:\n            ssh_cmd_str += ' %s' % ssh_args\n        cmd.append('--rsh=%s' % ssh_cmd_str)\n\n    if rsync_path:\n        cmd.append('--rsync-path=%s' % rsync_path)\n\n    if rsync_opts:\n        cmd.extend(rsync_opts)\n\n    if partial:\n        cmd.append('--partial')\n\n    changed_marker = '<<CHANGED>>'\n    cmd.append('--out-format=' + changed_marker + '%i %n%L')\n\n    # expand the paths\n    if '@' not in source:\n        source = os.path.expanduser(source)\n    if '@' not in dest:\n        dest = os.path.expanduser(dest)\n\n    cmd.append(source)\n    cmd.append(dest)\n    cmdstr = ' '.join(cmd)\n    (rc, out, err) = module.run_command(cmd)\n    if rc:\n        return module.fail_json(msg=err, rc=rc, cmd=cmdstr)\n    else:\n        changed = changed_marker in out\n        out_clean = out.replace(changed_marker, '')\n        out_lines = out_clean.split('\\n')\n        while '' in out_lines:\n            out_lines.remove('')\n        if module._diff:\n            diff = {'prepared': out_clean}\n            return module.exit_json(changed=changed, msg=out_clean,\n                                    rc=rc, cmd=cmdstr, stdout_lines=out_lines,\n                                    diff=diff)\n        else:\n            return module.exit_json(changed=changed, msg=out_clean,\n                                    rc=rc, cmd=cmdstr, stdout_lines=out_lines)"}
{"input": "def load(path, mono=True, frame_size=1024):\n    '''\n    Load an audio file into a single, long time series\n\n    Input:\n        path:       path to the input file\n        mono:       convert to mono?        | Default: True\n        frame_size: buffer size             | Default: 1024 samples\n    Output:\n        y:          the time series\n        sr:         the sampling rate\n    '''\n\n    with audioread.audio_open(path) as f:\n        sr  = f.samplerate\n        y   = numpy.concatenate([frame for frame in framegenerator.audioread_timeseries(f, frame_size)], axis=0)\n        pass\n\n    return (y, sr)", "output": "def load(path, mono=True, frame_size=1024):\n    '''\n    Load an audio file into a single, long time series\n\n    Input:\n        path:       path to the input file\n        mono:       convert to mono?        | Default: True\n        frame_size: buffer size             | Default: 1024 samples\n    Output:\n        y:          the time series\n        sr:         the sampling rate\n    '''\n\n    with audioread.audio_open(path) as f:\n        sr  = f.samplerate\n        y   = numpy.concatenate([frame for frame in framegenerator.audioread_timeseries(f, frame_size, mono=mono)], axis=0)\n        pass\n\n    return (y, sr)"}
{"input": "def test_load_freesolv_gaffmol2_vs_sybylmol2_vs_obabelpdb(get_fn, tmpdir):\n    tar_filename = \"freesolve_v0.3.tar.bz2\"\n    tar = tarfile.open(get_fn(tar_filename), mode=\"r:bz2\")\n    os.chdir(str(tmpdir))\n    tar.extractall()\n    tar.close()\n\n    with open(\"./v0.3/database.pickle\", 'rb') as f:\n        database = pickle.load(f)\n\n    for key in database:\n        gaff_filename = \"./v0.3/mol2files_gaff/%s.mol2\" % key\n        pdb_filename = \"./v0.3/mol2files_gaff/%s.pdb\" % key\n        sybyl_filename = \"./v0.3/mol2files_sybyl/%s.mol2\" % key\n\n        cmd = \"obabel -imol2 %s -opdb > %s 2>/dev/null\" % (sybyl_filename, pdb_filename)\n        assert os.system(cmd) == 0\n\n        t_pdb = md.load(pdb_filename)\n        t_gaff = md.load(gaff_filename)\n        t_sybyl = md.load(sybyl_filename)\n\n        eq(t_pdb.n_atoms, t_gaff.n_atoms)\n        eq(t_pdb.n_atoms, t_sybyl.n_atoms)\n\n        eq(t_pdb.n_frames, t_gaff.n_frames)\n        eq(t_pdb.n_frames, t_gaff.n_frames)\n\n        eq(t_pdb.xyz, t_gaff.xyz, decimal=4)\n        eq(t_pdb.xyz, t_sybyl.xyz, decimal=4)\n\n        top_pdb, bonds_pdb = t_pdb.top.to_dataframe()\n        top_gaff, bonds_gaff = t_gaff.top.to_dataframe()\n        top_sybyl, bonds_sybyl = t_sybyl.top.to_dataframe()\n\n        eq(top_sybyl.name.values, top_pdb.name.values)\n\n        # eq(top_gaff.name.values, top_sybyl.name.values)  # THEY CAN HAVE DIFFERENT NAMES, so this isn't TRUE!\n\n        def make_bonds_comparable(bond_array):\n            \"\"\"Create a bond connectivity matrix from a numpy array of atom pairs.  Avoids having to compare the order in which bonds are listed.\"\"\"\n            n_bonds = len(bond_array)\n            data = np.ones(n_bonds)\n            i = bond_array[:, 0]\n            j = bond_array[:, 1]\n            matrix = scipy.sparse.coo_matrix((data, (i, j)), shape=(t_pdb.n_atoms, t_pdb.n_atoms)).toarray()\n            return matrix + matrix.T  # Symmetrize to account for (a ~ b) versus (b ~ a)\n\n        bond_matrix_pdb = make_bonds_comparable(bonds_pdb)\n        bond_matrix_gaff = make_bonds_comparable(bonds_gaff)\n        bond_matrix_sybyl = make_bonds_comparable(bonds_sybyl)\n\n        eq(bond_matrix_pdb, bond_matrix_gaff)\n        eq(bond_matrix_pdb, bond_matrix_sybyl)", "output": "def test_load_freesolv_gaffmol2_vs_sybylmol2_vs_obabelpdb(get_fn, tmpdir):\n    tar_filename = \"freesolve_v0.3.tar.bz2\"\n    tar = tarfile.open(get_fn(tar_filename), mode=\"r:bz2\")\n    os.chdir(str(tmpdir))\n    tar.extractall()\n    tar.close()\n\n    with open(\"./v0.3/database.pickle\", 'rb') as f:\n        database = pickle.load(f, encoding='latin-1')\n\n    for key in database:\n        gaff_filename = \"./v0.3/mol2files_gaff/%s.mol2\" % key\n        pdb_filename = \"./v0.3/mol2files_gaff/%s.pdb\" % key\n        sybyl_filename = \"./v0.3/mol2files_sybyl/%s.mol2\" % key\n\n        cmd = \"obabel -imol2 %s -opdb > %s 2>/dev/null\" % (sybyl_filename, pdb_filename)\n        assert os.system(cmd) == 0\n\n        t_pdb = md.load(pdb_filename)\n        t_gaff = md.load(gaff_filename)\n        t_sybyl = md.load(sybyl_filename)\n\n        eq(t_pdb.n_atoms, t_gaff.n_atoms)\n        eq(t_pdb.n_atoms, t_sybyl.n_atoms)\n\n        eq(t_pdb.n_frames, t_gaff.n_frames)\n        eq(t_pdb.n_frames, t_gaff.n_frames)\n\n        eq(t_pdb.xyz, t_gaff.xyz, decimal=4)\n        eq(t_pdb.xyz, t_sybyl.xyz, decimal=4)\n\n        top_pdb, bonds_pdb = t_pdb.top.to_dataframe()\n        top_gaff, bonds_gaff = t_gaff.top.to_dataframe()\n        top_sybyl, bonds_sybyl = t_sybyl.top.to_dataframe()\n\n        eq(top_sybyl.name.values, top_pdb.name.values)\n\n        # eq(top_gaff.name.values, top_sybyl.name.values)  # THEY CAN HAVE DIFFERENT NAMES, so this isn't TRUE!\n\n        def make_bonds_comparable(bond_array):\n            \"\"\"Create a bond connectivity matrix from a numpy array of atom pairs.  Avoids having to compare the order in which bonds are listed.\"\"\"\n            n_bonds = len(bond_array)\n            data = np.ones(n_bonds)\n            i = bond_array[:, 0]\n            j = bond_array[:, 1]\n            matrix = scipy.sparse.coo_matrix((data, (i, j)), shape=(t_pdb.n_atoms, t_pdb.n_atoms)).toarray()\n            return matrix + matrix.T  # Symmetrize to account for (a ~ b) versus (b ~ a)\n\n        bond_matrix_pdb = make_bonds_comparable(bonds_pdb)\n        bond_matrix_gaff = make_bonds_comparable(bonds_gaff)\n        bond_matrix_sybyl = make_bonds_comparable(bonds_sybyl)\n\n        eq(bond_matrix_pdb, bond_matrix_gaff)\n        eq(bond_matrix_pdb, bond_matrix_sybyl)"}
{"input": "def create_bullet_list(self, lines: typing.Iterable[str]):\n        bullet_list = nodes.bullet_list()\n        parse_msgs = []\n        for line in lines:\n            text_nodes, messages = self.state.inline_text(line, self.lineno + self.content_offset)\n            parse_msgs += messages\n            par_node = nodes.paragraph('', *text_nodes)\n            list_item = nodes.list_item('', par_node)\n            bullet_list += list_item\n\n        return bullet_list, parse_msgs", "output": "def create_bullet_list(self, lines: typing.Iterable[str]):\n        bullet_list = nodes.bullet_list()\n        parse_msgs = []\n        for line in lines:\n            text_nodes, messages = self.state.inline_text(line, self.lineno + self.content_offset)\n            parse_msgs += messages\n            par_node = nodes.paragraph('', '', *text_nodes)\n            list_item = nodes.list_item('', par_node)\n            bullet_list += list_item\n\n        return bullet_list, parse_msgs"}
{"input": "def __init__(self, info):\n        self.info = info.ElementInfo\n        idx = len(self.info.Subname)\n        if idx:\n            sub = info.SelSubname[:-idx]\n        else:\n            sub = info.SelSubname\n        _,mat = info.SelObj.getSubObject(sub,1,FreeCAD.Matrix())\n        pos = utils.getElementPos(self.info.Shape)\n        if not pos:\n            pos = self.info.Shape.BoundBox.Center\n        pos = mat.multiply(pos)\n        self.matrix = mat*self.info.Placement.inverse().toMatrix()\n        base = self.matrix.multiply(self.info.Placement.Base)\n        self.offset = pos - base\n\n        self.matrix.invert()\n        self.view = info.SelObj.ViewObject.Document.ActiveView\n        self.callbackMove = self.view.addEventCallback(\n                \"SoLocation2Event\",self.moveMouse)\n        self.callbackClick = self.view.addEventCallback(\n                \"SoMouseButtonEvent\",self.clickMouse)\n        self.callbackKey = self.view.addEventCallback(\n                \"SoKeyboardEvent\",self.keyboardEvent)\n        FreeCAD.setActiveTransaction('Assembly quick move')\n        self.active = True", "output": "def __init__(self, info):\n        self.info = info.ElementInfo\n        idx = len(self.info.Subname)\n        if idx:\n            sub = info.SelSubname[:-idx]\n        else:\n            sub = info.SelSubname\n        _,mat = info.SelObj.getSubObject(sub,1,FreeCAD.Matrix())\n        pos = utils.getElementPos(self.info.Shape)\n        if not pos:\n            pos = self.info.Shape.BoundBox.Center\n        pos = mat.multiply(pos)\n        self.matrix = mat*self.info.Placement.inverse().toMatrix()\n        base = self.matrix.multiply(self.info.Placement.Base)\n        self.offset = pos - base\n\n        self.matrix.invert()\n        self.view = info.SelObj.ViewObject.Document.ActiveView\n        self.callbackMove = self.view.addEventCallback(\n                \"SoLocation2Event\",self.moveMouse)\n        self.callbackClick = self.view.addEventCallback(\n                \"SoMouseButtonEvent\",self.clickMouse)\n        self.callbackKey = self.view.addEventCallback(\n                \"SoKeyboardEvent\",self.keyboardEvent)\n        FreeCAD.setActiveTransaction('Assembly quick move',True)\n        self.active = True"}
{"input": "def __repr__(self):\n        \"\"\"Represent detailed ZabbixSender view.\"\"\"\n\n        result = json.dumps(self.__dict__)\n        logger.debug('%s: %s', self.__class__.__name__, result)\n\n        return result", "output": "def __repr__(self):\n        \"\"\"Represent detailed ZabbixSender view.\"\"\"\n\n        result = json.dumps(self.__dict__, ensure_ascii=False)\n        logger.debug('%s: %s', self.__class__.__name__, result)\n\n        return result"}
{"input": "def _plot_response_curve(y, t, title, continuous=True):\n    _plt.title(title)\n    _plt.xlabel('t/s')\n    _plt.ylabel('Amplitude')\n    _plt.axvline(x=0, color='black')\n    _plt.axhline(y=0, color='black')\n    if continuous:\n        _plt.plot(t, y)\n    else:\n        _plt.step(t, y)\n    _plt.grid()\n    _plt.show()", "output": "def _plot_response_curve(y, t, title, continuous=True):\n    _plt.title(title)\n    _plt.xlabel('t/s')\n    _plt.ylabel('Amplitude')\n    _plt.axvline(x=0, color='black')\n    _plt.axhline(y=0, color='black')\n    if continuous:\n        _plt.plot(t, y)\n    else:\n        _plt.step(t, y, where='post')\n    _plt.grid()\n    _plt.show()"}
{"input": "def get_current_play(self):\n        user = get_current_user()\n        # now = datetime.now()\n        # selfplay = Play.query.filter(Play.PLcreate == user.USid, Play.PLstatus == PlayStatus.activity.value).first()\n        play = Play.query.join(EnterLog.PLid == Play.PLid).filter(\n            Play.PLstatus == PlayStatus.activity.value,\n            or_(Play.PLcreate == user.USid, EnterLog.USid == user.USid)).first()\n        if not play:\n            raise StatusError('\u5f53\u524d\u65e0\u5f00\u542f\u6d3b\u52a8')\n        self._fill_play(play)\n        return Success(data=play)", "output": "def get_current_play(self):\n        user = get_current_user()\n        # now = datetime.now()\n        # selfplay = Play.query.filter(Play.PLcreate == user.USid, Play.PLstatus == PlayStatus.activity.value).first()\n        play = Play.query.join(EnterLog, EnterLog.PLid == Play.PLid).filter(\n            Play.PLstatus == PlayStatus.activity.value,\n            or_(Play.PLcreate == user.USid, EnterLog.USid == user.USid)).first()\n        if not play:\n            raise StatusError('\u5f53\u524d\u65e0\u5f00\u542f\u6d3b\u52a8')\n        self._fill_play(play)\n        return Success(data=play)"}
{"input": "def write_csv(results, filename):\n    with open(filename, \"w\") as f:\n        writer = csv.writer(f)\n        writer.writerow(['Benchmark', 'Base', 'Changed'])\n        for result in results:\n            name = result.base.get_name()\n            base = average(result.base)\n            changed = average(result.changed)\n            row = [name, format_csv(base), format_csv(changed)]\n            writer.writerow(row)", "output": "def write_csv(results, filename):\n    with open(filename, \"w\", newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Benchmark', 'Base', 'Changed'])\n        for result in results:\n            name = result.base.get_name()\n            base = average(result.base)\n            changed = average(result.changed)\n            row = [name, format_csv(base), format_csv(changed)]\n            writer.writerow(row)"}
{"input": "def configure(self, prompt, default, parent, section_name):\n        if self.parse == _parse_boolean:\n            prompt += ' (y/n)'\n            default = 'y' if default else 'n'\n        return super(ValidatedAttribute, self).configure(prompt, default)", "output": "def configure(self, prompt, default, parent, section_name):\n        if self.parse == _parse_boolean:\n            prompt += ' (y/n)'\n            default = 'y' if default else 'n'\n        return super(ValidatedAttribute, self).configure(prompt, default, parent, section_name)"}
{"input": "def _create_ticket_with_attachment(self, url, data):\n        attachments = data['attachments']\n        del data['attachments']\n        multipart_data = []\n\n        for attachment in attachments:\n            file_name = attachment.split(\"/\")[-1:][0]\n            multipart_data.append(('attachments[]', (file_name, open(attachment), None)))\n\n        ticket = self._api._post(url, data=data, files=multipart_data)\n        return ticket", "output": "def _create_ticket_with_attachment(self, url, data):\n        attachments = data['attachments']\n        del data['attachments']\n        multipart_data = []\n\n        for attachment in attachments:\n            file_name = attachment.split(\"/\")[-1:][0]\n            multipart_data.append(('attachments[]', (file_name, open(attachment, 'rb'), None)))\n\n        ticket = self._api._post(url, data=data, files=multipart_data)\n        return ticket"}
{"input": "def init_repo(repo_dir, repo_url):\n    repo = git.Repo.init(repo_dir)\n    if repo.remotes:\n        origin = repo.remote('origin')\n    else:\n        origin = repo.create_remote('origin', repo_url)\n    try:\n        origin.fetch()\n    except GitCommandError:\n        logger.error(\"Git fetch failed.\", exc_info=True, extra={'data': {\n            'repo_dir': repo_dir\n        }})\n        return\n\n    repo.git.reset('--hard', 'origin/master')\n    logger.info(\"Initialized git repository %s in %s\", repo_dir, repo_url)", "output": "def init_repo(repo_dir, repo_url):\n    repo = git.Repo.init(repo_dir)\n    if repo.remotes:\n        origin = repo.remote('origin')\n    else:\n        origin = repo.create_remote('origin', repo_url)\n    try:\n        origin.fetch()\n    except GitCommandError:\n        logger.error(\"Git fetch failed.\", exc_info=True, extra={'data': {\n            'repo_dir': repo_dir\n        }})\n        return\n\n    repo.git.reset('--hard', 'origin/master')\n    logger.info(\"Initialized git repository %s in %s\", repo_url, repo_dir)"}
{"input": "def scrape_actions(self, bill, url):\n        with self.urlopen(url) as page:\n            page = lxml.html.fromstring(page)\n\n            bill.add_source(url)\n\n            slist = page.xpath(\"//strong[contains(., 'Authors:')]\")[0]\n            slist = slist.tail.split(',')\n            for sponsor in slist:\n                name = sponsor.strip()\n                if name:\n                    bill.add_sponsor(name, 'author')\n\n            act_table = page.xpath(\"//table\")[1]\n            read_yet = False\n\n            for row in act_table.xpath(\"tr\")[1:]:\n                date = row.xpath(\"string(td[1])\").strip()\n                date = datetime.datetime.strptime(date, \"%m/%d/%Y\").date()\n\n                chamber = row.xpath(\"string(td[2])\").strip()\n                if chamber == 'S':\n                    chamber = 'upper'\n                elif chamber == 'H':\n                    chamber = 'lower'\n\n                action = row.xpath(\"string(td[4])\").strip()\n\n                atype = []\n\n                if action.startswith('First reading:'):\n                    if not read_yet:\n                        atype.append('bill:introduced')\n                        read_yet = True\n                    atype.append('bill:reading:1')\n                if 'referred to' in action:\n                    atype.append('committee:referred')\n\n                bill.add_action(chamber, action, date, type=atype)", "output": "def scrape_actions(self, bill, url):\n        with self.urlopen(url) as page:\n            page = lxml.html.fromstring(page)\n\n            bill.add_source(url)\n\n            slist = page.xpath(\"//strong[contains(., 'Authors:')]\")[0]\n            slist = slist.tail.split(',')\n            for sponsor in slist:\n                name = sponsor.strip()\n                if name:\n                    bill.add_sponsor('author', name)\n\n            act_table = page.xpath(\"//table\")[1]\n            read_yet = False\n\n            for row in act_table.xpath(\"tr\")[1:]:\n                date = row.xpath(\"string(td[1])\").strip()\n                date = datetime.datetime.strptime(date, \"%m/%d/%Y\").date()\n\n                chamber = row.xpath(\"string(td[2])\").strip()\n                if chamber == 'S':\n                    chamber = 'upper'\n                elif chamber == 'H':\n                    chamber = 'lower'\n\n                action = row.xpath(\"string(td[4])\").strip()\n\n                atype = []\n\n                if action.startswith('First reading:'):\n                    if not read_yet:\n                        atype.append('bill:introduced')\n                        read_yet = True\n                    atype.append('bill:reading:1')\n                if 'referred to' in action:\n                    atype.append('committee:referred')\n\n                bill.add_action(chamber, action, date, type=atype)"}
{"input": "def run(self, terms, inject=None, **kwargs):\n\n        terms = utils.listify_lookup_plugin_terms(terms, self.basedir, inject) \n\n        if isinstance(basestring, terms):\n            terms = [ terms ]\n\n        ret = []\n        for term in terms:\n            domain = term.split()[0]\n            string = []\n            try:\n                answers = dns.resolver.query(domain, 'TXT')\n                for rdata in answers:\n                    s = rdata.to_text()\n                    string.append(s[1:-1])  # Strip outside quotes on TXT rdata\n\n            except dns.resolver.NXDOMAIN:\n                string = 'NXDOMAIN'\n            except dns.resolver.Timeout:\n                string = ''\n            except dns.exception.DNSException as e:\n                raise errors.AnsibleError(\"dns.resolver unhandled exception\", e)\n\n            ret.append(''.join(string))\n        return ret", "output": "def run(self, terms, inject=None, **kwargs):\n\n        terms = utils.listify_lookup_plugin_terms(terms, self.basedir, inject) \n\n        if isinstance(terms, basestring):\n            terms = [ terms ]\n\n        ret = []\n        for term in terms:\n            domain = term.split()[0]\n            string = []\n            try:\n                answers = dns.resolver.query(domain, 'TXT')\n                for rdata in answers:\n                    s = rdata.to_text()\n                    string.append(s[1:-1])  # Strip outside quotes on TXT rdata\n\n            except dns.resolver.NXDOMAIN:\n                string = 'NXDOMAIN'\n            except dns.resolver.Timeout:\n                string = ''\n            except dns.exception.DNSException as e:\n                raise errors.AnsibleError(\"dns.resolver unhandled exception\", e)\n\n            ret.append(''.join(string))\n        return ret"}
{"input": "def release_tag_model(graph: ProvDocument, packages: ReleaseTagPackage):\n    for package in packages:\n        if package.release_package is not None:\n            r_user, release, release_event, release_evidence, assets = package.release_package\n            graph.agent(*r_user)\n            graph.entity(*release)\n            graph.activity(*release_event)\n            graph.entity(*release_evidence)\n            for asset in assets:\n                graph.entity(*asset)\n                graph.hadMember(asset.id, release.id)\n\n            graph.hadMember(release_evidence.id, release.id)\n            graph.wasGeneratedBy(release.id, release_event.id)\n            graph.wasAttributedTo(release.id, r_user.id)\n            graph.wasAssociatedWith(release_event.id, r_user.id)\n\n        if package.tag_package is not None:\n            t_user, tag, tag_event = package.tag_package\n            graph.agent(*t_user)\n            graph.entity(*tag)\n            graph.activity(*tag_event)\n\n            if package.release_package is not None:\n                graph.hadMember(tag.id, release.id)\n            graph.wasGeneratedBy(tag_event.id, tag.id)\n            graph.wasAttributedTo(tag.id, t_user.id)\n            graph.wasAssociatedWith(tag_event.id, t_user.id)\n\n        if package.commit_package is not None:\n            author, commit_event, _, commit, _ = package.commit_package\n            graph.agent(*author)\n            graph.activity(*commit_event)\n            graph.entity(*commit)\n\n            if package.tag_package is not None:\n                graph.hadMember(commit.id, tag.id)\n            graph.wasGeneratedBy(commit.id, commit_event.id)\n            graph.wasAttributedTo(commit.id, author.id)\n            graph.wasAssociatedWith(commit_event.id, author.id)\n    return graph", "output": "def release_tag_model(graph: ProvDocument, packages: ReleaseTagPackage):\n    for package in packages:\n        if package.release_package is not None:\n            r_user, release, release_event, release_evidence, assets = package.release_package\n            graph.agent(*r_user)\n            graph.entity(*release)\n            graph.activity(*release_event)\n            graph.entity(*release_evidence)\n            for asset in assets:\n                graph.entity(*asset)\n                graph.hadMember(asset.id, release.id)\n\n            graph.hadMember(release_evidence.id, release.id)\n            graph.wasGeneratedBy(release.id, release_event.id)\n            graph.wasAttributedTo(release.id, r_user.id)\n            graph.wasAssociatedWith(release_event.id, r_user.id)\n\n        if package.tag_package is not None:\n            t_user, tag, tag_event = package.tag_package\n            graph.agent(*t_user)\n            graph.entity(*tag)\n            graph.activity(*tag_event)\n\n            if package.release_package is not None:\n                graph.hadMember(tag.id, release.id)\n            graph.wasGeneratedBy(tag.id, tag_event.id)\n            graph.wasAttributedTo(tag.id, t_user.id)\n            graph.wasAssociatedWith(tag_event.id, t_user.id)\n\n        if package.commit_package is not None:\n            author, commit_event, _, commit, _ = package.commit_package\n            graph.agent(*author)\n            graph.activity(*commit_event)\n            graph.entity(*commit)\n\n            if package.tag_package is not None:\n                graph.hadMember(commit.id, tag.id)\n            graph.wasGeneratedBy(commit.id, commit_event.id)\n            graph.wasAttributedTo(commit.id, author.id)\n            graph.wasAssociatedWith(commit_event.id, author.id)\n    return graph"}
{"input": "def checkouts(self, seconds=3600, cookie_ttl=84600):\n        \"\"\"Returns a list of Checkout instances, describing each of\n        the times this client checked out at the target site.\n\n        Keyword Args:\n            seconds    -- the maximum number of seconds that can occur between\n                          a cart addition and it still be removed. Defaults to\n                          one hour.\n            cookie_ttl -- the maximum amount of time represented by this\n                          checkout history, which should correspond to\n                          the expected TTL of an affiliate cookie.\n                          Defaults to 1 day (84600 seconds).\n\n        Return:\n            A list of zero or more Checkout instances.\n        \"\"\"\n        # First, remove redundant checkout records from the collection.\n        # Since we're only tracking add-to-cart instances, there may be\n        # more than one add-to-cart record corresponding to each actual\n        # checkout event.  This pruning step is intended to reduce the number\n        # of false positives this add-to-cart-as-proxy-for-checkout method\n        # would introduce\n        self.prune(seconds=seconds)\n\n        # Now combine all of the types of records we're tracking\n        # (add to cart requests, cookie stuffs, and cookie setting\n        # requests) together in a single collection, and then sort them\n        # in reverse order, from latest occurring to most recently occurring,\n        # so that we can walk through them once to build up the history\n        # of each checkout instance\n        requests = []\n        requests += [(r, g, CART) for r, g in self._cart_requests]\n        requests += [(r, g, STUFF) for r, g in self._cookie_stuffs]\n        requests += [(r, g, SET) for r, g in self._cookie_sets]\n        requests.sort(key=lambda x: x[0].ts, reverse=True)\n\n        checkouts = []\n        for r, g, t in requests:\n            # If the current record is a request to add something to\n            # the cart, then automatically start a new checkout\n            # collection to track what happens to this this request\n            if t == CART:\n                checkouts.append(AffiliateCheckout(r, g, cookie_ttl, self))\n                continue\n\n            # Otherwise, if this is not a request to add something to a cart,\n            # but we haven't seen any requests to add an item to a cart,\n            # then there is no checkout instance to be possibly affected\n            # by the cookie this request is setting, so safely ignore\n            if len(checkouts) == 0:\n                continue\n\n            # Otherwise, we just attempt to add either a cookie set or\n            # cookie stuffing instance to the current checkout history\n            # collection.  Note that these requests will fail (ie just do\n            # nothing) if they occurred too long after the most recent\n            # checkout / cart-add request, and so any cookie they were setting\n            # would be void\n            if t == SET:\n                checkouts[-1].add_cookie_set(r, g)\n            else:\n                checkouts[-1].add_cookie_stuff(r, g)\n        return checkouts", "output": "def checkouts(self, seconds=3600, cookie_ttl=84600):\n        \"\"\"Returns a list of Checkout instances, describing each of\n        the times this client checked out at the target site.\n\n        Keyword Args:\n            seconds    -- the maximum number of seconds that can occur between\n                          a cart addition and it still be removed. Defaults to\n                          one hour.\n            cookie_ttl -- the maximum amount of time represented by this\n                          checkout history, which should correspond to\n                          the expected TTL of an affiliate cookie.\n                          Defaults to 1 day (84600 seconds).\n\n        Return:\n            A list of zero or more Checkout instances.\n        \"\"\"\n        # First, remove redundant checkout records from the collection.\n        # Since we're only tracking add-to-cart instances, there may be\n        # more than one add-to-cart record corresponding to each actual\n        # checkout event.  This pruning step is intended to reduce the number\n        # of false positives this add-to-cart-as-proxy-for-checkout method\n        # would introduce\n        self.prune(seconds=seconds)\n\n        # Now combine all of the types of records we're tracking\n        # (add to cart requests, cookie stuffs, and cookie setting\n        # requests) together in a single collection, and then sort them\n        # in reverse order, from latest occurring to most recently occurring,\n        # so that we can walk through them once to build up the history\n        # of each checkout instance\n        requests = []\n        requests += [(r, g, CART) for r, g in self._cart_requests]\n        requests += [(r, g, STUFF) for r, g in self._cookie_stuffs]\n        requests += [(r, g, SET) for r, g in self._cookie_sets]\n        requests.sort(key=lambda x: x[0].ts, reverse=True)\n\n        checkouts = []\n        for r, g, t in requests:\n            # If the current record is a request to add something to\n            # the cart, then automatically start a new checkout\n            # collection to track what happens to this this request\n            if t == CART:\n                checkouts.append(AffiliateCheckout(r, g, self, cookie_ttl))\n                continue\n\n            # Otherwise, if this is not a request to add something to a cart,\n            # but we haven't seen any requests to add an item to a cart,\n            # then there is no checkout instance to be possibly affected\n            # by the cookie this request is setting, so safely ignore\n            if len(checkouts) == 0:\n                continue\n\n            # Otherwise, we just attempt to add either a cookie set or\n            # cookie stuffing instance to the current checkout history\n            # collection.  Note that these requests will fail (ie just do\n            # nothing) if they occurred too long after the most recent\n            # checkout / cart-add request, and so any cookie they were setting\n            # would be void\n            if t == SET:\n                checkouts[-1].add_cookie_set(r, g)\n            else:\n                checkouts[-1].add_cookie_stuff(r, g)\n        return checkouts"}
{"input": "def test_approp_valid(self):\n        \"\"\"Test valid job.\"\"\"\n        jobId = self.jobIdDict[\"valid\"]\n        self.passed = self.run_test(\n            jobId, 200, \"finished\", 63, 10, \"complete\", 0, False)", "output": "def test_approp_valid(self):\n        \"\"\"Test valid job.\"\"\"\n        jobId = self.jobIdDict[\"valid\"]\n        self.passed = self.run_test(\n            jobId, 200, \"finished\", 63, 0, \"complete\", 10, False)"}
{"input": "def daemon_start(pid_file, log_file):\n\n    def handle_exit(signum, _):\n        if signum == signal.SIGTERM:\n            sys.exit(0)\n        sys.exit(1)\n\n    signal.signal(signal.SIGINT, handle_exit)\n    signal.signal(signal.SIGTERM, handle_exit)\n\n    # fork only once because we are sure parent will exit\n    pid = os.fork()\n    assert pid != -1\n\n    if pid > 0:\n        # parent waits for its child\n        time.sleep(5)\n        sys.exit(0)\n\n    # child signals its parent to exit\n    ppid = os.getppid()\n    pid = os.getpid()\n    if write_pid_file(pid_file, pid) != 0:\n        os.kill(ppid, signal.SIGINT)\n        sys.exit(1)\n\n    os.setsid()\n    signal.signal(signal.SIG_IGN, signal.SIGHUP)\n\n    print('started')\n    os.kill(ppid, signal.SIGTERM)\n\n    sys.stdin.close()\n    try:\n        freopen(log_file, 'a', sys.stdout)\n        freopen(log_file, 'a', sys.stderr)\n    except IOError as e:\n        shell.print_exception(e)\n        sys.exit(1)", "output": "def daemon_start(pid_file, log_file):\n\n    def handle_exit(signum, _):\n        if signum == signal.SIGTERM:\n            sys.exit(0)\n        sys.exit(1)\n\n    signal.signal(signal.SIGINT, handle_exit)\n    signal.signal(signal.SIGTERM, handle_exit)\n\n    # fork only once because we are sure parent will exit\n    pid = os.fork()\n    assert pid != -1\n\n    if pid > 0:\n        # parent waits for its child\n        time.sleep(5)\n        sys.exit(0)\n\n    # child signals its parent to exit\n    ppid = os.getppid()\n    pid = os.getpid()\n    if write_pid_file(pid_file, pid) != 0:\n        os.kill(ppid, signal.SIGINT)\n        sys.exit(1)\n\n    os.setsid()\n    signal.signal(signal.SIGHUP, signal.SIG_IGN)\n\n    print('started')\n    os.kill(ppid, signal.SIGTERM)\n\n    sys.stdin.close()\n    try:\n        freopen(log_file, 'a', sys.stdout)\n        freopen(log_file, 'a', sys.stderr)\n    except IOError as e:\n        shell.print_exception(e)\n        sys.exit(1)"}
{"input": "def test_getListOfGames(self):\n        print(\"testing gettingListOfPlayedGames\")\n        gameStates = [GameState.COMPLETED, GameState.RUNNING]\n        self.instance.getListOfGames(divisionId_Swissdraw,8,gameStates)\n        print(\"#####################################################################\")", "output": "def test_getListOfGames(self):\n        print(\"testing gettingListOfPlayedGames\")\n        gameStates = [GameState.COMPLETED, GameState.RUNNING]\n        self.instance.getListOfGames(divisionId_Swissdraw, gameStates, 8)\n        print(\"#####################################################################\")"}
{"input": "def _load_from_state_dict(self, state_dict, prefix, metadata, strict, missing_keys, unexpected_keys, error_msgs):\n        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n        this module, but not its descendants. This is called on every submodule\n        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n        module in input :attr:`state_dict` is provided as :attr`metadata`.\n        For state dicts without meta data, :attr`metadata` is empty.\n        Subclasses can achieve class-specific backward compatible loading using\n        the version number at `metadata.get(\"version\", None)`.\n\n        .. note::\n            :attr:`state_dict` is not the same object as the input\n            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n            it can be modified.\n\n        Arguments:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            prefix (str): the prefix for parameters and buffers used in this\n                module\n            metadata (dict): a dict containing the metadata for this moodule.\n                See\n            strict (bool): whether to strictly enforce that the keys in\n                :attr:`state_dict` with :attr:`prefix` match the names of\n                parameters and buffers in this module\n            missing_keys (list of str): if ``strict=False``, add missing keys to\n                this list\n            unexpected_keys (list of str): if ``strict=False``, add unexpected\n                keys to this list\n            error_msgs (list of str): error messages should be added to this\n                list, and will be reported together in\n                :meth:`~torch.nn.Module.load_state_dict`\n        \"\"\"\n        local_name_params = itertools.chain(self._parameters.items(), self._buffers.items())\n        local_state = {k: v.data for k, v in local_name_params if v is not None}\n\n        for name, param in local_state.items():\n            key = prefix + name\n            if key in state_dict:\n                input_param = state_dict[key]\n\n                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n                if len(param.shape) == 0 and len(input_param.shape) == 1:\n                    input_param = input_param[0]\n\n                if input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append('size mismatch for {}: copying a param of {} from checkpoint, '\n                                      'where the shape is {} in current model.'\n                                      .format(key, param.shape, input_param.shape))\n                    continue\n\n                if isinstance(input_param, Parameter):\n                    # backwards compatibility for serialized parameters\n                    input_param = input_param.data\n                try:\n                    param.copy_(input_param)\n                except Exception:\n                    error_msgs.append('While copying the parameter named \"{}\", '\n                                      'whose dimensions in the model are {} and '\n                                      'whose dimensions in the checkpoint are {}.'\n                                      .format(key, param.size(), input_param.size()))\n            elif strict:\n                missing_keys.append(key)\n\n        if strict:\n            for key, input_param in state_dict.items():\n                if key.startswith(prefix):\n                    input_name = key[len(prefix):]\n                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n                    if input_name not in self._modules and input_name not in local_state:\n                        unexpected_keys.append(key)", "output": "def _load_from_state_dict(self, state_dict, prefix, metadata, strict, missing_keys, unexpected_keys, error_msgs):\n        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n        this module, but not its descendants. This is called on every submodule\n        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n        module in input :attr:`state_dict` is provided as :attr`metadata`.\n        For state dicts without meta data, :attr`metadata` is empty.\n        Subclasses can achieve class-specific backward compatible loading using\n        the version number at `metadata.get(\"version\", None)`.\n\n        .. note::\n            :attr:`state_dict` is not the same object as the input\n            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n            it can be modified.\n\n        Arguments:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            prefix (str): the prefix for parameters and buffers used in this\n                module\n            metadata (dict): a dict containing the metadata for this moodule.\n                See\n            strict (bool): whether to strictly enforce that the keys in\n                :attr:`state_dict` with :attr:`prefix` match the names of\n                parameters and buffers in this module\n            missing_keys (list of str): if ``strict=False``, add missing keys to\n                this list\n            unexpected_keys (list of str): if ``strict=False``, add unexpected\n                keys to this list\n            error_msgs (list of str): error messages should be added to this\n                list, and will be reported together in\n                :meth:`~torch.nn.Module.load_state_dict`\n        \"\"\"\n        local_name_params = itertools.chain(self._parameters.items(), self._buffers.items())\n        local_state = {k: v.data for k, v in local_name_params if v is not None}\n\n        for name, param in local_state.items():\n            key = prefix + name\n            if key in state_dict:\n                input_param = state_dict[key]\n\n                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n                if len(param.shape) == 0 and len(input_param.shape) == 1:\n                    input_param = input_param[0]\n\n                if input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append('size mismatch for {}: copying a param of {} from checkpoint, '\n                                      'where the shape is {} in current model.'\n                                      .format(key, input_param.shape, param.shape))\n                    continue\n\n                if isinstance(input_param, Parameter):\n                    # backwards compatibility for serialized parameters\n                    input_param = input_param.data\n                try:\n                    param.copy_(input_param)\n                except Exception:\n                    error_msgs.append('While copying the parameter named \"{}\", '\n                                      'whose dimensions in the model are {} and '\n                                      'whose dimensions in the checkpoint are {}.'\n                                      .format(key, param.size(), input_param.size()))\n            elif strict:\n                missing_keys.append(key)\n\n        if strict:\n            for key, input_param in state_dict.items():\n                if key.startswith(prefix):\n                    input_name = key[len(prefix):]\n                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n                    if input_name not in self._modules and input_name not in local_state:\n                        unexpected_keys.append(key)"}
{"input": "def get_history(self, obj):\n        diff_list = []\n        current = None\n\n        for version in reversed(list(reversion.get_for_object(obj))):\n            if current:\n                issues_diff = self.get_issues_diff(version, current)\n                diff_list.append(issues_diff)\n\n            current = version\n\n        return diff_list", "output": "def get_history(self, obj):\n        diff_list = []\n        current = None\n\n        for version in reversed(list(reversion.get_for_object(obj))):\n            if current:\n                issues_diff = self.get_issues_diff(current, version)\n                diff_list.append(issues_diff)\n\n            current = version\n\n        return diff_list"}
{"input": "def open(self, file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):\n        \"\"\"Downloads file from WebDAV server and saves it temprorary, then opens it for further manipulations.\n        Has the same interface as built-in open()\n\n        :param file: the path to remote file for opening.\n        \"\"\"\n        urn = Urn(file)\n        urn_path = urn.path()\n\n        remote_file_exists = self.check(urn_path)\n\n        if not remote_file_exists:\n            if 'r' in mode:\n                raise RemoteResourceNotFound(urn_path)\n        elif self.is_dir(urn_path):\n            raise OptionNotValid(name='file', value=file)\n\n        with tempfile.TemporaryDirectory() as temp_dir:\n            local_path = f'{temp_dir}{os.path.sep}{file}'\n\n            if remote_file_exists:\n                self.download_file(file, local_path)\n            else:\n                if ('w' in mode or 'a' in mode or 'x' in mode) and os.path.sep in local_path:\n                    os.makedirs(local_path.rsplit(os.path.sep, 1)[0], exist_ok=True)\n\n            with open(file=local_path, mode=mode, buffering=buffering, encoding=encoding, errors=errors,\n                      newline=newline, closefd=closefd, opener=opener) as f:\n                yield f\n\n            if 'w' in mode or 'a' in mode or 'x' in mode:\n                self.upload_file(local_path, file)", "output": "def open(self, file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None):\n        \"\"\"Downloads file from WebDAV server and saves it temprorary, then opens it for further manipulations.\n        Has the same interface as built-in open()\n\n        :param file: the path to remote file for opening.\n        \"\"\"\n        urn = Urn(file)\n        urn_path = urn.path()\n\n        remote_file_exists = self.check(urn_path)\n\n        if not remote_file_exists:\n            if 'r' in mode:\n                raise RemoteResourceNotFound(urn_path)\n        elif self.is_dir(urn_path):\n            raise OptionNotValid(name='file', value=file)\n\n        with tempfile.TemporaryDirectory() as temp_dir:\n            local_path = f'{temp_dir}{os.path.sep}{file}'\n\n            if remote_file_exists:\n                self.download_file(file, local_path)\n            else:\n                if ('w' in mode or 'a' in mode or 'x' in mode) and os.path.sep in local_path:\n                    os.makedirs(local_path.rsplit(os.path.sep, 1)[0], exist_ok=True)\n\n            with open(file=local_path, mode=mode, buffering=buffering, encoding=encoding, errors=errors,\n                      newline=newline, closefd=closefd, opener=opener) as f:\n                yield f\n\n            if 'w' in mode or 'a' in mode or 'x' in mode:\n                self.upload_file(file, local_path)"}
{"input": "def __init__(self, definition_file, cfg_set, repo_path, repo_branch, repo_host='github.com'):\n        self.definition_file = definition_file\n        self.repo_path = repo_path\n        self.repo_branch = repo_branch\n        self.repo_host = repo_host\n        self.cfg_set = cfg_set\n        import model\n        self.job_mapping = model.JobMapping(\n            name='dummy',\n            raw_dict={'concourse_target_team': 'dummy'},\n        )", "output": "def __init__(self, definition_file, cfg_set, repo_path, repo_branch, repo_host='github.com'):\n        self.definition_file = definition_file\n        self.repo_path = repo_path\n        self.repo_branch = repo_branch\n        self.repo_host = repo_host\n        self.cfg_set = cfg_set\n        import model\n        self.job_mapping = model.concourse.JobMapping(\n            name='dummy',\n            raw_dict={'concourse_target_team': 'dummy'},\n        )"}
{"input": "def setup_proxies(self):\n        self._setup_widgets()\n        self.add_proxy(self.model, BillDataSlave.payment_widgets)", "output": "def setup_proxies(self):\n        self._setup_widgets()\n        self.add_proxy(self.model.payment, BillDataSlave.payment_widgets)"}
{"input": "def info_printer(solver,header):\n            net = solver.problem.network\n            if header:\n                print('{0:^5}'.format('vmax'), end=' ')\n                print('{0:^5}'.format('vmin'), end=' ')\n                print('{0:^6}'.format('bvvio'), end=' ')\n                print('{0:^6}'.format('gQvio'), end=' ')\n                print('{0:^6}'.format('gPvio'))\n            else:\n                print('{0:^5.2f}'.format(np.average(net.bus_v_max)), end=' ')\n                print('{0:^5.2f}'.format(np.average(net.bus_v_min)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.bus_v_vio)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.gen_Q_vio)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.gen_P_vio)))", "output": "def info_printer(solver,header):\n            net = solver.problem.wrapped_problem.network\n            if header:\n                print('{0:^5}'.format('vmax'), end=' ')\n                print('{0:^5}'.format('vmin'), end=' ')\n                print('{0:^6}'.format('bvvio'), end=' ')\n                print('{0:^6}'.format('gQvio'), end=' ')\n                print('{0:^6}'.format('gPvio'))\n            else:\n                print('{0:^5.2f}'.format(np.average(net.bus_v_max)), end=' ')\n                print('{0:^5.2f}'.format(np.average(net.bus_v_min)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.bus_v_vio)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.gen_Q_vio)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.gen_P_vio)))"}
{"input": "def visit_system_message(self, node):\n        if node['level'] < self.document.reporter['writer'].report_level:\n            raise nodes.SkipNode\n        self.body.append(self.starttag(node, 'div', CLASS='system-message'))\n        self.body.append('<p class=\"system-message-title\">')\n        attr = {}\n        a_end = ''\n        backref_text = ''\n        if node.hasattr('id'):\n            attr['name'] = node['id']\n        if node.hasattr('backrefs'):\n            backrefs = node['backrefs']\n            if len(backrefs) == 1:\n                attr['href'] = '#' + backrefs[0]\n            else:\n                i = 1\n                backlinks = []\n                for backref in backrefs:\n                    backlinks.append('<a href=\"#%s\">%s</a>' % (backref, i))\n                    i += 1\n                backref_text = '%s; ' % ', '.join(backlinks)\n        if attr:\n            self.body.append(self.starttag({}, 'a', attr, ''))\n            a_end = '</a>'\n        self.body.append('%s%s (%slevel %s system message)</p>\\n'\n                         % (node['type'], a_end, backref_text,\n                            node['level']))", "output": "def visit_system_message(self, node):\n        if node['level'] < self.document.reporter['writer'].report_level:\n            raise nodes.SkipNode\n        self.body.append(self.starttag(node, 'div', CLASS='system-message'))\n        self.body.append('<p class=\"system-message-title\">')\n        attr = {}\n        a_end = ''\n        backref_text = ''\n        if node.hasattr('id'):\n            attr['name'] = node['id']\n        if node.hasattr('backrefs'):\n            backrefs = node['backrefs']\n            if len(backrefs) == 1:\n                attr['href'] = '#' + backrefs[0]\n            else:\n                i = 1\n                backlinks = []\n                for backref in backrefs:\n                    backlinks.append('<a href=\"#%s\">%s</a>' % (backref, i))\n                    i += 1\n                backref_text = '%s; ' % ', '.join(backlinks)\n        if attr:\n            self.body.append(self.starttag({}, 'a', '', attr))\n            a_end = '</a>'\n        self.body.append('%s%s (%slevel %s system message)</p>\\n'\n                         % (node['type'], a_end, backref_text,\n                            node['level']))"}
{"input": "def __init__(self, band):\n        self.band = band\n        self.iter = 0\n        self.set_output_filenames()\n\n        self.moon_stripe_mask = HealpixMap(\"/home/users/mberkeley/wisemapper/data/masks/stripe_mask_G.fits\")\n        self.moon_stripe_mask.read_data()\n        self.moon_stripe_inds = np.arange(len(self.moon_stripe_mask.mapdata))[self.moon_stripe_mask.mapdata.astype(bool)]\n\n        self.galaxy_mask = self.mask_galaxy()\n        self.galaxy_mask_inds = np.arange(len(self.galaxy_mask))[self.galaxy_mask]\n\n        self.south_pole_mask = HealpixMap(\"/home/users/mberkeley/wisemapper/data/masks/south_pole_mask.fits\").read_data()\n        self.south_pole_mask_inds = np.arange(len(self.south_pole_mask))[self.south_pole_mask.mapdata.astype(bool)]\n\n        self.numerator = np.zeros_like(self.fsm.mapdata)\n        self.denominator = np.zeros_like(self.fsm.mapdata)\n\n        self.gains = []\n        self.offsets = []\n        self.orbit_sizes = []\n\n        self.all_gains = []\n        self.all_offsets = []", "output": "def __init__(self, band):\n        self.band = band\n        self.iter = 0\n        self.set_output_filenames()\n\n        self.moon_stripe_mask = HealpixMap(\"/home/users/mberkeley/wisemapper/data/masks/stripe_mask_G.fits\")\n        self.moon_stripe_mask.read_data()\n        self.moon_stripe_inds = np.arange(len(self.moon_stripe_mask.mapdata))[self.moon_stripe_mask.mapdata.astype(bool)]\n\n        self.galaxy_mask = self.mask_galaxy()\n        self.galaxy_mask_inds = np.arange(len(self.galaxy_mask))[self.galaxy_mask]\n\n        self.south_pole_mask = HealpixMap(\"/home/users/mberkeley/wisemapper/data/masks/south_pole_mask.fits\").read_data()\n        self.south_pole_mask_inds = np.arange(len(self.south_pole_mask.mapdata))[self.south_pole_mask.mapdata.astype(bool)]\n\n        self.numerator = np.zeros_like(self.fsm.mapdata)\n        self.denominator = np.zeros_like(self.fsm.mapdata)\n\n        self.gains = []\n        self.offsets = []\n        self.orbit_sizes = []\n\n        self.all_gains = []\n        self.all_offsets = []"}
{"input": "def connect(self):\n        \"\"\"Attempt to connect and return ConnectionState\"\"\"\n        if self.state is ConnectionStates.DISCONNECTED:\n            self.close()\n            log.debug('%s: creating new socket', str(self))\n            # if self.afi is set to AF_UNSPEC, then we need to do a name\n            # resolution and try all available address families\n            if self._init_afi == socket.AF_UNSPEC:\n                if self._gai is None:\n                    # XXX: all DNS functions in Python are blocking. If we really\n                    # want to be non-blocking here, we need to use a 3rd-party\n                    # library like python-adns, or move resolution onto its\n                    # own thread. This will be subject to the default libc\n                    # name resolution timeout (5s on most Linux boxes)\n                    try:\n                        self._gai = socket.getaddrinfo(self._init_host,\n                                                       self._init_port,\n                                                       socket.AF_UNSPEC,\n                                                       socket.SOCK_STREAM)\n                    except socket.gaierror as ex:\n                        raise socket.gaierror('getaddrinfo failed for {0}:{1}, '\n                          'exception was {2}. Is your advertised.listeners (called'\n                          'advertised.host.name before Kafka 9) correct and resolvable?'.format(\n                             self._init_host, self._init_port, ex\n                          ))\n                    self._gai_index = 0\n                else:\n                    # if self._gai already exists, then we should try the next\n                    # name\n                    self._gai_index += 1\n                while True:\n                    if self._gai_index >= len(self._gai):\n                        log.error('Unable to connect to any of the names for {0}:{1}'.format(\n                            self._init_host, self._init_port\n                        ))\n                        self.close()\n                        return\n                    afi, _, __, ___, sockaddr = self._gai[self._gai_index]\n                    if afi not in (socket.AF_INET, socket.AF_INET6):\n                        self._gai_index += 1\n                        continue\n                    break\n                self.host, self.port = sockaddr[:2]\n                self._sock = socket.socket(afi, socket.SOCK_STREAM)\n            else:\n                self._sock = socket.socket(self._init_afi, socket.SOCK_STREAM)\n\n            for option in self.config['socket_options']:\n                self._sock.setsockopt(*option)\n\n            self._sock.setblocking(False)\n            if self.config['security_protocol'] in ('SSL', 'SASL_SSL'):\n                self._wrap_ssl()\n            self.state = ConnectionStates.CONNECTING\n            self.last_attempt = time.time()\n            self.config['state_change_callback'](self)\n\n        if self.state is ConnectionStates.CONNECTING:\n            # in non-blocking mode, use repeated calls to socket.connect_ex\n            # to check connection status\n            request_timeout = self.config['request_timeout_ms'] / 1000.0\n            ret = None\n            try:\n                ret = self._sock.connect_ex((self.host, self.port))\n                # if we got here through a host lookup, we've found a host,port,af tuple\n                # that works save it so we don't do a GAI lookup again\n                if self._gai is not None:\n                    self.afi = self._sock.family\n                    self._gai = None\n            except socket.error as err:\n                ret = err\n\n            # Connection succeeded\n            if not ret or ret == errno.EISCONN:\n                log.debug('%s: established TCP connection', str(self))\n                if self.config['security_protocol'] in ('SSL', 'SASL_SSL'):\n                    log.debug('%s: initiating SSL handshake', str(self))\n                    self.state = ConnectionStates.HANDSHAKE\n                elif self.config['security_protocol'] == 'SASL_PLAINTEXT':\n                    self.state = ConnectionStates.AUTHENTICATING\n                else:\n                    self.state = ConnectionStates.CONNECTED\n                self.config['state_change_callback'](self)\n\n            # Connection failed\n            # WSAEINVAL == 10022, but errno.WSAEINVAL is not available on non-win systems\n            elif ret not in (errno.EINPROGRESS, errno.EALREADY, errno.EWOULDBLOCK, 10022):\n                log.error('Connect attempt to %s returned error %s.'\n                          ' Disconnecting.', self, ret)\n                self.close()\n\n            # Connection timed out\n            elif time.time() > request_timeout + self.last_attempt:\n                log.error('Connection attempt to %s timed out', self)\n                self.close() # error=TimeoutError ?\n\n            # Needs retry\n            else:\n                pass\n\n        if self.state is ConnectionStates.HANDSHAKE:\n            if self._try_handshake():\n                log.debug('%s: completed SSL handshake.', str(self))\n                if self.config['security_protocol'] == 'SASL_SSL':\n                    self.state = ConnectionStates.AUTHENTICATING\n                else:\n                    self.state = ConnectionStates.CONNECTED\n                self.config['state_change_callback'](self)\n\n        if self.state is ConnectionStates.AUTHENTICATING:\n            assert self.config['security_protocol'] in ('SASL_PLAINTEXT', 'SASL_SSL')\n            if self._try_authenticate():\n                log.info('%s: Authenticated as %s', str(self), self.config['sasl_plain_username'])\n                self.state = ConnectionStates.CONNECTED\n                self.config['state_change_callback'](self)\n\n        return self.state", "output": "def connect(self):\n        \"\"\"Attempt to connect and return ConnectionState\"\"\"\n        if self.state is ConnectionStates.DISCONNECTED:\n            self.close()\n            log.debug('%s: creating new socket', str(self))\n            # if self.afi is set to AF_UNSPEC, then we need to do a name\n            # resolution and try all available address families\n            if self._init_afi == socket.AF_UNSPEC:\n                if self._gai is None:\n                    # XXX: all DNS functions in Python are blocking. If we really\n                    # want to be non-blocking here, we need to use a 3rd-party\n                    # library like python-adns, or move resolution onto its\n                    # own thread. This will be subject to the default libc\n                    # name resolution timeout (5s on most Linux boxes)\n                    try:\n                        self._gai = socket.getaddrinfo(self._init_host,\n                                                       self._init_port,\n                                                       socket.AF_UNSPEC,\n                                                       socket.SOCK_STREAM)\n                    except socket.gaierror as ex:\n                        raise socket.gaierror('getaddrinfo failed for {0}:{1}, '\n                          'exception was {2}. Is your advertised.listeners (called'\n                          'advertised.host.name before Kafka 9) correct and resolvable?'.format(\n                             self._init_host, self._init_port, ex\n                          ))\n                    self._gai_index = 0\n                else:\n                    # if self._gai already exists, then we should try the next\n                    # name\n                    self._gai_index += 1\n                while True:\n                    if self._gai_index >= len(self._gai):\n                        log.error('Unable to connect to any of the names for {0}:{1}'.format(\n                            self._init_host, self._init_port\n                        ))\n                        self.close()\n                        return\n                    afi, _, __, ___, sockaddr = self._gai[self._gai_index]\n                    if afi not in (socket.AF_INET, socket.AF_INET6):\n                        self._gai_index += 1\n                        continue\n                    break\n                self.host, self.port = sockaddr[:2]\n                self._sock = socket.socket(afi, socket.SOCK_STREAM)\n            else:\n                self._sock = socket.socket(self._init_afi, socket.SOCK_STREAM)\n\n            for option in self.config['socket_options']:\n                self._sock.setsockopt(*option)\n\n            self._sock.setblocking(False)\n            if self.config['security_protocol'] in ('SSL', 'SASL_SSL'):\n                self._wrap_ssl()\n            self.state = ConnectionStates.CONNECTING\n            self.last_attempt = time.time()\n            self.config['state_change_callback'](self)\n\n        if self.state is ConnectionStates.CONNECTING:\n            # in non-blocking mode, use repeated calls to socket.connect_ex\n            # to check connection status\n            request_timeout = self.config['request_timeout_ms'] / 1000.0\n            ret = None\n            try:\n                ret = self._sock.connect_ex((self.host, self.port))\n                # if we got here through a host lookup, we've found a host,port,af tuple\n                # that works save it so we don't do a GAI lookup again\n                if self._gai is not None:\n                    self.afi = self._sock.family\n                    self._gai = None\n            except socket.error as err:\n                ret = err.errno\n\n            # Connection succeeded\n            if not ret or ret == errno.EISCONN:\n                log.debug('%s: established TCP connection', str(self))\n                if self.config['security_protocol'] in ('SSL', 'SASL_SSL'):\n                    log.debug('%s: initiating SSL handshake', str(self))\n                    self.state = ConnectionStates.HANDSHAKE\n                elif self.config['security_protocol'] == 'SASL_PLAINTEXT':\n                    self.state = ConnectionStates.AUTHENTICATING\n                else:\n                    self.state = ConnectionStates.CONNECTED\n                self.config['state_change_callback'](self)\n\n            # Connection failed\n            # WSAEINVAL == 10022, but errno.WSAEINVAL is not available on non-win systems\n            elif ret not in (errno.EINPROGRESS, errno.EALREADY, errno.EWOULDBLOCK, 10022):\n                log.error('Connect attempt to %s returned error %s.'\n                          ' Disconnecting.', self, ret)\n                self.close()\n\n            # Connection timed out\n            elif time.time() > request_timeout + self.last_attempt:\n                log.error('Connection attempt to %s timed out', self)\n                self.close() # error=TimeoutError ?\n\n            # Needs retry\n            else:\n                pass\n\n        if self.state is ConnectionStates.HANDSHAKE:\n            if self._try_handshake():\n                log.debug('%s: completed SSL handshake.', str(self))\n                if self.config['security_protocol'] == 'SASL_SSL':\n                    self.state = ConnectionStates.AUTHENTICATING\n                else:\n                    self.state = ConnectionStates.CONNECTED\n                self.config['state_change_callback'](self)\n\n        if self.state is ConnectionStates.AUTHENTICATING:\n            assert self.config['security_protocol'] in ('SASL_PLAINTEXT', 'SASL_SSL')\n            if self._try_authenticate():\n                log.info('%s: Authenticated as %s', str(self), self.config['sasl_plain_username'])\n                self.state = ConnectionStates.CONNECTED\n                self.config['state_change_callback'](self)\n\n        return self.state"}
{"input": "def get_stddev(self):\n        \"\"\"\n        Returns stddev with 'price' for existing algorithms.\n\n        Could possibly use existing algos.\n        \"\"\"\n        # Sample standard deviation is undefined for a single event or\n        # no events.\n        if len(self) <= 1:\n            return None\n\n        else:\n            average = self.sum['price'] / len(self.ticks)\n            s_squared = (self.sum_sqr['price'] - self.sum['price'] * average) \\\n                / (len(self.ticks) - 1)\n            stddev = sqrt(s_squared)\n        return stddev", "output": "def get_stddev(self):\n        \"\"\"\n        Returns stddev with 'price' for existing algorithms.\n\n        Could possibly use existing algos.\n        \"\"\"\n        # Sample standard deviation is undefined for a single event or\n        # no events.\n        if len(self.ticks) <= 1:\n            return None\n\n        else:\n            average = self.sum['price'] / len(self.ticks)\n            s_squared = (self.sum_sqr['price'] - self.sum['price'] * average) \\\n                / (len(self.ticks) - 1)\n            stddev = sqrt(s_squared)\n        return stddev"}
{"input": "def __init__(self, parent, config, name):\n        BasePlugin.__init__(self, parent, config, name)\n        self.imap_server = self.config.get('email_server', '')\n        self.username = self.config.get('email_username', '')\n        self.password = self.config.get('email_password', '')\n        if self.imap_server and self.username and self.password:\n            self.processor = Processor(self.imap_server, self.username, self.password, self.on_receive)\n            self.processor.start()\n        self.obj = QEmailSignalObject()\n        self.obj.email_new_invoice_signal(self.new_invoice)", "output": "def __init__(self, parent, config, name):\n        BasePlugin.__init__(self, parent, config, name)\n        self.imap_server = self.config.get('email_server', '')\n        self.username = self.config.get('email_username', '')\n        self.password = self.config.get('email_password', '')\n        if self.imap_server and self.username and self.password:\n            self.processor = Processor(self.imap_server, self.username, self.password, self.on_receive)\n            self.processor.start()\n        self.obj = QEmailSignalObject()\n        self.obj.email_new_invoice_signal.connect(self.new_invoice)"}
{"input": "def __call__(self, xi, order=0):\n        \"\"\"Evaluate the path at given position.\"\"\"\n        ret = [poly.derivative(order)(xi) for poly in self._polys]\n        return np.array(ret)", "output": "def __call__(self, xi, order=0):\n        \"\"\"Evaluate the path at given position.\"\"\"\n        ret = [poly.derivative(order)(xi) for poly in self._polys]\n        return np.array(ret).T"}
{"input": "def _cfg_factory_from_dir():\n    # XXX: args does always have a cfg_dir attribute, but pylint does not always understand this\n    if not args or not hasattr(args, 'cfg_dir') and not getattr(args, 'cfg_dir'):\n        return None\n\n    from util import ensure_directory_exists\n    cfg_dir = ensure_directory_exists(getattr(args, 'cfg_dir'))\n\n    from model import ConfigFactory\n    factory = ConfigFactory.from_cfg_dir(cfg_dir=cfg_dir)\n    return factory", "output": "def _cfg_factory_from_dir():\n    # XXX: args does always have a cfg_dir attribute, but pylint does not always understand this\n    if not args or not hasattr(args, 'cfg_dir') or not getattr(args, 'cfg_dir'):\n        return None\n\n    from util import ensure_directory_exists\n    cfg_dir = ensure_directory_exists(getattr(args, 'cfg_dir'))\n\n    from model import ConfigFactory\n    factory = ConfigFactory.from_cfg_dir(cfg_dir=cfg_dir)\n    return factory"}
{"input": "def _form_extended_payload(cls, msg: int, payload: Union[Iterable, bytes, bytearray]) -> bytearray:\n        if payload is None:\n            payload = bytearray()\n        payload_length = len(payload)\n        assert payload_length <= 0x7f_ff\n        if payload_length > 0x80:\n            payload_length = [(payload_length >> 8) | 0x80, payload_length & 0xff]\n        else:\n            payload_length = [payload_length]\n        packet = bytearray([msg, *payload_length, *payload])\n        crc = cls.VEX_CRC16.compute(bytes([*cls._form_simple_packet(0x56), *packet]))\n        packet = bytearray([*packet, crc >> 8, crc & 0xff])\n        assert (cls.VEX_CRC16.compute(bytes([*cls._form_simple_packet(0x56), *packet])) == 0)\n        return packet", "output": "def _form_extended_payload(cls, msg: int, payload: Union[Iterable, bytes, bytearray]) -> bytearray:\n        if payload is None:\n            payload = bytearray()\n        payload_length = len(payload)\n        assert payload_length <= 0x7f_ff\n        if payload_length >= 0x80:\n            payload_length = [(payload_length >> 8) | 0x80, payload_length & 0xff]\n        else:\n            payload_length = [payload_length]\n        packet = bytearray([msg, *payload_length, *payload])\n        crc = cls.VEX_CRC16.compute(bytes([*cls._form_simple_packet(0x56), *packet]))\n        packet = bytearray([*packet, crc >> 8, crc & 0xff])\n        assert (cls.VEX_CRC16.compute(bytes([*cls._form_simple_packet(0x56), *packet])) == 0)\n        return packet"}
{"input": "def rename(self, axis, newname):\n        axis = self.get_axis(axis)\n        axes = [Axis(newname, a.labels) if a is axis else a\n                for a in self.axes]\n        return LArray(self, axes)", "output": "def rename(self, axis, newname):\n        axis = self.get_axis(axis)\n        axes = [Axis(newname, a.labels) if a is axis else a\n                for a in self.axes]\n        return LArray(self.data, axes)"}
{"input": "def create_security_group(self, context, security_group, default_sg=False):\n        \"\"\"Create security group.\n\n        If default_sg is true that means a we are creating a default security\n        group and we don't need to check if one exists.\n        \"\"\"\n        s = security_group.get('security_group')\n\n        tenant_id = self._get_tenant_id_for_create(context, s)\n        if not default_sg:\n            self._ensure_default_security_group(context, tenant_id)\n        # NOTE(salv-orlando): Pre-generating Neutron ID for security group.\n        neutron_id = str(uuid.uuid4())\n        nsx_secgroup = secgrouplib.create_security_profile(\n            self.cluster, neutron_id, tenant_id, s)\n        with context.session.begin(subtransactions=True):\n            s['id'] = neutron_id\n            sec_group = super(NsxPluginV2, self).create_security_group(\n                context, security_group, default_sg)\n            context.session.flush()\n            # Add mapping between neutron and nsx identifiers\n            nsx_db.add_neutron_nsx_security_group_mapping(\n                context.session, neutron_id, nsx_secgroup['uuid'])\n        return sec_group", "output": "def create_security_group(self, context, security_group, default_sg=False):\n        \"\"\"Create security group.\n\n        If default_sg is true that means a we are creating a default security\n        group and we don't need to check if one exists.\n        \"\"\"\n        s = security_group.get('security_group')\n\n        tenant_id = self._get_tenant_id_for_create(context, s)\n        if not default_sg:\n            self._ensure_default_security_group(context, tenant_id)\n        # NOTE(salv-orlando): Pre-generating Neutron ID for security group.\n        neutron_id = str(uuid.uuid4())\n        nsx_secgroup = secgrouplib.create_security_profile(\n            self.cluster, tenant_id, neutron_id, s)\n        with context.session.begin(subtransactions=True):\n            s['id'] = neutron_id\n            sec_group = super(NsxPluginV2, self).create_security_group(\n                context, security_group, default_sg)\n            context.session.flush()\n            # Add mapping between neutron and nsx identifiers\n            nsx_db.add_neutron_nsx_security_group_mapping(\n                context.session, neutron_id, nsx_secgroup['uuid'])\n        return sec_group"}
{"input": "def main():\n    \"\"\"Entry point to the QA Dashboard.\"\"\"\n    log.setLevel(log.INFO)\n\n    log.info(\"Started\")\n\n    with log.indent():\n        log.info(\"Setup\")\n        config = Config()\n        jenkins_url = config.get_jenkins_url()\n        master_build_url = jenkins_url + config.get_master_build_job()\n        log.success(\"Setup done\")\n\n    last_processed_build = read_last_processed()\n\n    log.info(\"Last processed build: {n}\".format(n=last_processed_build))\n\n    last_build, last_build_status, total_builds_cnt, success_builds_cnt = \\\n        read_build_history(master_build_url)\n\n    if last_build >= last_processed_build:\n        log.info(\"New build(s) detected!\")\n        with log.indent():\n            process_new_build(config, last_build, last_build_status, jenkins_url, master_build_url)\n\n        write_last_processed(last_build)\n    else:\n        log.info(\"No new build(s) detected...\")", "output": "def main():\n    \"\"\"Entry point to the QA Dashboard.\"\"\"\n    log.setLevel(log.INFO)\n\n    log.info(\"Started\")\n\n    with log.indent():\n        log.info(\"Setup\")\n        config = Config()\n        jenkins_url = config.get_jenkins_url()\n        master_build_url = jenkins_url + config.get_master_build_job()\n        log.success(\"Setup done\")\n\n    last_processed_build = read_last_processed()\n\n    log.info(\"Last processed build: {n}\".format(n=last_processed_build))\n\n    last_build, last_build_status, total_builds_cnt, success_builds_cnt = \\\n        read_build_history(master_build_url)\n\n    if last_build > last_processed_build:\n        log.info(\"New build(s) detected!\")\n        with log.indent():\n            process_new_build(config, last_build, last_build_status, jenkins_url, master_build_url)\n\n        write_last_processed(last_build)\n    else:\n        log.info(\"No new build(s) detected...\")"}
{"input": "def _sort_map_entries(category_map, sort_alpha):\n    things = []\n    for title, entry in category_map[\"entries\"].items():\n        if entry[\"sort_key\"] == None and sort_alpha:\n            entry[\"sort_key\"] = title\n        things.append((title, entry))\n    for title, category in category_map[\"subcategories\"].items():\n        things.append((title, category))\n        _sort_map_entries(category_map[\"subcategories\"][title], sort_alpha)\n    category_map[\"children\"] = [x[0] for x in sorted(things, key=lambda x: x[1][\"sort_key\"])]", "output": "def _sort_map_entries(category_map, sort_alpha):\n    things = []\n    for title, entry in category_map[\"entries\"].items():\n        if entry[\"sort_key\"] is None and sort_alpha:\n            entry[\"sort_key\"] = title\n        things.append((title, entry))\n    for title, category in category_map[\"subcategories\"].items():\n        things.append((title, category))\n        _sort_map_entries(category_map[\"subcategories\"][title], sort_alpha)\n    category_map[\"children\"] = [x[0] for x in sorted(things, key=lambda x: x[1][\"sort_key\"])]"}
{"input": "def roulette_wheel(fitness: list, extractions: int = 1):\n    cur_fitness = np.array(fitness)\n    fitness_sum = np.sum(cur_fitness)\n    probabilities = cur_fitness / fitness_sum\n    probabilities = probabilities.tolist()\n\n    for _ in range(extractions):\n        candidates = []\n        while(True):\n            idx = randint(0, len(probabilities) - 1)\n            cur_probability = probabilities[idx]\n            if random() >= cur_probability:\n                candidates.append(idx)\n            if len(candidates) == 2:\n                yield candidates\n                break", "output": "def roulette_wheel(fitness: list, extractions: int = 1):\n    cur_fitness = np.array(fitness)\n    fitness_sum = np.sum(cur_fitness)\n    probabilities = cur_fitness / fitness_sum\n    probabilities = probabilities.tolist()\n\n    for _ in range(extractions):\n        candidates = []\n        while(True):\n            idx = randint(0, len(probabilities) - 1)\n            cur_probability = probabilities[idx]\n            if random() <= cur_probability:\n                candidates.append(idx)\n            if len(candidates) == 2:\n                yield candidates\n                break"}
{"input": "def rem_call(self, instance, callback):\n\t\t\"\"\"Removes all callbacks of 'instance' that are 'callback'\n\t\t@param instance: the instance that would execute the call\n\t\t@param callback: the function to remove\n\t\t@return: int, number of removed calls\n\t\t\"\"\"\n\t\tassert callable(callback)\n\t\tremoved_calls = 0\n\t\tfor key in self.schedule:\n\t\t\tcallback_objects = self.schedule[key]\n\t\t\tfor i in xrange(len(callback_objects) - 1, -1, -1):\n\t\t\t\tif callback_objects[i].class_instance is instance and callback_objects[i].callback == callback and \\\n\t\t\t\t   not hasattr(callback, \"invalid\"):\n\t\t\t\t\tdel callback_objects[i]\n\t\t\t\t\tremoved_calls += 1\n\n\t\ttest = 0\n\t\tif removed_calls >= 0: # there also must be calls in the calls_by_instance dict\n\t\t\tfor i in xrange(len(self.calls_by_instance[instance]) - 1, -1, -1):\n\t\t\t\tobj = self.calls_by_instance[instance][i]\n\t\t\t\tif obj.callback == callback:\n\t\t\t\t\tdel self.calls_by_instance[instance][i]\n\t\t\t\t\ttest += 1\n\t\t\tassert test == removed_calls\n\t\t\tif not self.calls_by_instance[instance]:\n\t\t\t\tdel self.calls_by_instance[instance]\n\n\t\tfor i in xrange(len(self.additional_cur_tick_schedule) - 1, -1, -1):\n\t\t\tif self.additional_cur_tick_schedule[i].class_instance is instance and \\\n\t\t\t\tself.additional_cur_tick_schedule[i].callback == callback:\n\t\t\t\t\tdel callback_objects[i]\n\t\t\t\t\tremoved_calls += 1\n\n\t\treturn removed_calls", "output": "def rem_call(self, instance, callback):\n\t\t\"\"\"Removes all callbacks of 'instance' that are 'callback'\n\t\t@param instance: the instance that would execute the call\n\t\t@param callback: the function to remove\n\t\t@return: int, number of removed calls\n\t\t\"\"\"\n\t\tassert callable(callback)\n\t\tremoved_calls = 0\n\t\tfor key in self.schedule:\n\t\t\tcallback_objects = self.schedule[key]\n\t\t\tfor i in xrange(len(callback_objects) - 1, -1, -1):\n\t\t\t\tif callback_objects[i].class_instance is instance and callback_objects[i].callback == callback and \\\n\t\t\t\t   not hasattr(callback, \"invalid\"):\n\t\t\t\t\tdel callback_objects[i]\n\t\t\t\t\tremoved_calls += 1\n\n\t\ttest = 0\n\t\tif removed_calls > 0: # there also must be calls in the calls_by_instance dict\n\t\t\tfor i in xrange(len(self.calls_by_instance[instance]) - 1, -1, -1):\n\t\t\t\tobj = self.calls_by_instance[instance][i]\n\t\t\t\tif obj.callback == callback:\n\t\t\t\t\tdel self.calls_by_instance[instance][i]\n\t\t\t\t\ttest += 1\n\t\t\tassert test == removed_calls\n\t\t\tif not self.calls_by_instance[instance]:\n\t\t\t\tdel self.calls_by_instance[instance]\n\n\t\tfor i in xrange(len(self.additional_cur_tick_schedule) - 1, -1, -1):\n\t\t\tif self.additional_cur_tick_schedule[i].class_instance is instance and \\\n\t\t\t\tself.additional_cur_tick_schedule[i].callback == callback:\n\t\t\t\t\tdel callback_objects[i]\n\t\t\t\t\tremoved_calls += 1\n\n\t\treturn removed_calls"}
{"input": "def main():\n    module = AnsibleModule(\n        argument_spec = dict(\n            state = dict(default='present', choices=['installed', 'latest', 'removed', 'absent', 'present']),\n            update_cache = dict(default=False, aliases=['update-cache'], type='bool'),\n            cache_valid_time = dict(type='int'),\n            purge = dict(default=False, type='bool'),\n            package = dict(default=None, aliases=['pkg', 'name'], type='list'),\n            deb = dict(default=None),\n            default_release = dict(default=None, aliases=['default-release']),\n            install_recommends = dict(default='yes', aliases=['install-recommends'], type='bool'),\n            force = dict(default='no', type='bool'),\n            upgrade = dict(choices=['yes', 'safe', 'full', 'dist']),\n            dpkg_options = dict(default=DPKG_OPTIONS)\n        ),\n        mutually_exclusive = [['package', 'upgrade', 'deb']],\n        required_one_of = [['package', 'upgrade', 'update_cache', 'deb']],\n        supports_check_mode = True\n    )\n\n    if not HAS_PYTHON_APT:\n        try:\n            module.run_command('apt-get update && apt-get install python-apt -y -q', use_unsafe_shell=True, check_rc=True)\n            global apt, apt_pkg\n            import apt\n            import apt_pkg\n        except ImportError:\n            module.fail_json(msg=\"Could not import python modules: apt, apt_pkg. Please install python-apt package.\")\n\n    global APTITUDE_CMD\n    APTITUDE_CMD = module.get_bin_path(\"aptitude\", False)\n    global APT_GET_CMD\n    APT_GET_CMD = module.get_bin_path(\"apt-get\")\n\n    p = module.params\n    if not APTITUDE_CMD and p.get('upgrade', None) in [ 'full', 'safe', 'yes' ]:\n        module.fail_json(msg=\"Could not find aptitude. Please ensure it is installed.\")\n\n    install_recommends = p['install_recommends']\n    dpkg_options = expand_dpkg_options(p['dpkg_options'])\n\n    # Deal with deprecated aliases\n    if p['state'] == 'installed':\n        p['state'] = 'present'\n    if p['state'] == 'removed':\n        p['state'] = 'absent'\n\n    try:\n        cache = apt.Cache()\n        if p['default_release']:\n            try:\n                apt_pkg.config['APT::Default-Release'] = p['default_release']\n            except AttributeError:\n                apt_pkg.Config['APT::Default-Release'] = p['default_release']\n            # reopen cache w/ modified config\n            cache.open(progress=None)\n\n        if p['update_cache']:\n            # Default is: always update the cache\n            cache_valid = False\n            if p['cache_valid_time']:\n                tdelta = datetime.timedelta(seconds=p['cache_valid_time'])\n                try:\n                    mtime = os.stat(APT_UPDATE_SUCCESS_STAMP_PATH).st_mtime\n                except:\n                    mtime = False\n                if mtime is False:\n                    # Looks like the update-success-stamp is not available\n                    # Fallback: Checking the mtime of the lists\n                    try:\n                        mtime = os.stat(APT_LISTS_PATH).st_mtime\n                    except:\n                        mtime = False\n                if mtime is False:\n                    # No mtime could be read - looks like lists are not there\n                    # We update the cache to be safe\n                    cache_valid = False\n                else:\n                    mtimestamp = datetime.datetime.fromtimestamp(mtime)\n                    if mtimestamp + tdelta >= datetime.datetime.now():\n                        # dont update the cache\n                        # the old cache is less than cache_valid_time seconds old - so still valid\n                        cache_valid = True\n\n            if cache_valid is not True:\n                cache.update()\n                cache.open(progress=None)\n            if not p['package'] and not p['upgrade'] and not p['deb']:\n                module.exit_json(changed=False)\n\n        force_yes = p['force']\n\n        if p['upgrade']:\n            upgrade(module, p['upgrade'], force_yes,\n                    p['default_release'], dpkg_options)\n\n        if p['deb']:\n            if p['state'] == 'present':\n                module.fail_json(msg=\"deb only supports state=present\")\n            install_deb(module, p['deb'], cache,\n                        install_recommends=install_recommends,\n                        force=force_yes, dpkg_options=p['dpkg_options'])\n\n        packages = p['package']\n        latest = p['state'] == 'latest'\n        for package in packages:\n            if package.count('=') > 1:\n                module.fail_json(msg=\"invalid package spec: %s\" % package)\n            if latest and '=' in package:\n                module.fail_json(msg='version number inconsistent with state=latest: %s' % package)\n\n        if p['state'] == 'latest':\n            result = install(module, packages, cache, upgrade=True,\n                    default_release=p['default_release'],\n                    install_recommends=install_recommends,\n                    force=force_yes, dpkg_options=dpkg_options)\n            (success, retvals) = result\n            if success:\n                module.exit_json(**retvals)\n            else:\n                module.fail_json(**retvals)\n        elif p['state'] ==  'present':\n            result = install(module, packages, cache, default_release=p['default_release'],\n                      install_recommends=install_recommends,force=force_yes,\n                      dpkg_options=dpkg_options)\n            (success, retvals) = result\n            if success:\n                module.exit_json(**retvals)\n            else:\n                module.fail_json(**retvals)\n        elif p['state'] == 'absent':\n            remove(module, packages, cache, p['purge'], dpkg_options)\n\n    except apt.cache.LockFailedException:\n        module.fail_json(msg=\"Failed to lock apt for exclusive operation\")", "output": "def main():\n    module = AnsibleModule(\n        argument_spec = dict(\n            state = dict(default='present', choices=['installed', 'latest', 'removed', 'absent', 'present']),\n            update_cache = dict(default=False, aliases=['update-cache'], type='bool'),\n            cache_valid_time = dict(type='int'),\n            purge = dict(default=False, type='bool'),\n            package = dict(default=None, aliases=['pkg', 'name'], type='list'),\n            deb = dict(default=None),\n            default_release = dict(default=None, aliases=['default-release']),\n            install_recommends = dict(default='yes', aliases=['install-recommends'], type='bool'),\n            force = dict(default='no', type='bool'),\n            upgrade = dict(choices=['yes', 'safe', 'full', 'dist']),\n            dpkg_options = dict(default=DPKG_OPTIONS)\n        ),\n        mutually_exclusive = [['package', 'upgrade', 'deb']],\n        required_one_of = [['package', 'upgrade', 'update_cache', 'deb']],\n        supports_check_mode = True\n    )\n\n    if not HAS_PYTHON_APT:\n        try:\n            module.run_command('apt-get update && apt-get install python-apt -y -q', use_unsafe_shell=True, check_rc=True)\n            global apt, apt_pkg\n            import apt\n            import apt_pkg\n        except ImportError:\n            module.fail_json(msg=\"Could not import python modules: apt, apt_pkg. Please install python-apt package.\")\n\n    global APTITUDE_CMD\n    APTITUDE_CMD = module.get_bin_path(\"aptitude\", False)\n    global APT_GET_CMD\n    APT_GET_CMD = module.get_bin_path(\"apt-get\")\n\n    p = module.params\n    if not APTITUDE_CMD and p.get('upgrade', None) in [ 'full', 'safe', 'yes' ]:\n        module.fail_json(msg=\"Could not find aptitude. Please ensure it is installed.\")\n\n    install_recommends = p['install_recommends']\n    dpkg_options = expand_dpkg_options(p['dpkg_options'])\n\n    # Deal with deprecated aliases\n    if p['state'] == 'installed':\n        p['state'] = 'present'\n    if p['state'] == 'removed':\n        p['state'] = 'absent'\n\n    try:\n        cache = apt.Cache()\n        if p['default_release']:\n            try:\n                apt_pkg.config['APT::Default-Release'] = p['default_release']\n            except AttributeError:\n                apt_pkg.Config['APT::Default-Release'] = p['default_release']\n            # reopen cache w/ modified config\n            cache.open(progress=None)\n\n        if p['update_cache']:\n            # Default is: always update the cache\n            cache_valid = False\n            if p['cache_valid_time']:\n                tdelta = datetime.timedelta(seconds=p['cache_valid_time'])\n                try:\n                    mtime = os.stat(APT_UPDATE_SUCCESS_STAMP_PATH).st_mtime\n                except:\n                    mtime = False\n                if mtime is False:\n                    # Looks like the update-success-stamp is not available\n                    # Fallback: Checking the mtime of the lists\n                    try:\n                        mtime = os.stat(APT_LISTS_PATH).st_mtime\n                    except:\n                        mtime = False\n                if mtime is False:\n                    # No mtime could be read - looks like lists are not there\n                    # We update the cache to be safe\n                    cache_valid = False\n                else:\n                    mtimestamp = datetime.datetime.fromtimestamp(mtime)\n                    if mtimestamp + tdelta >= datetime.datetime.now():\n                        # dont update the cache\n                        # the old cache is less than cache_valid_time seconds old - so still valid\n                        cache_valid = True\n\n            if cache_valid is not True:\n                cache.update()\n                cache.open(progress=None)\n            if not p['package'] and not p['upgrade'] and not p['deb']:\n                module.exit_json(changed=False)\n\n        force_yes = p['force']\n\n        if p['upgrade']:\n            upgrade(module, p['upgrade'], force_yes,\n                    p['default_release'], dpkg_options)\n\n        if p['deb']:\n            if p['state'] != 'present':\n                module.fail_json(msg=\"deb only supports state=present\")\n            install_deb(module, p['deb'], cache,\n                        install_recommends=install_recommends,\n                        force=force_yes, dpkg_options=p['dpkg_options'])\n\n        packages = p['package']\n        latest = p['state'] == 'latest'\n        for package in packages:\n            if package.count('=') > 1:\n                module.fail_json(msg=\"invalid package spec: %s\" % package)\n            if latest and '=' in package:\n                module.fail_json(msg='version number inconsistent with state=latest: %s' % package)\n\n        if p['state'] == 'latest':\n            result = install(module, packages, cache, upgrade=True,\n                    default_release=p['default_release'],\n                    install_recommends=install_recommends,\n                    force=force_yes, dpkg_options=dpkg_options)\n            (success, retvals) = result\n            if success:\n                module.exit_json(**retvals)\n            else:\n                module.fail_json(**retvals)\n        elif p['state'] ==  'present':\n            result = install(module, packages, cache, default_release=p['default_release'],\n                      install_recommends=install_recommends,force=force_yes,\n                      dpkg_options=dpkg_options)\n            (success, retvals) = result\n            if success:\n                module.exit_json(**retvals)\n            else:\n                module.fail_json(**retvals)\n        elif p['state'] == 'absent':\n            remove(module, packages, cache, p['purge'], dpkg_options)\n\n    except apt.cache.LockFailedException:\n        module.fail_json(msg=\"Failed to lock apt for exclusive operation\")"}
{"input": "def view_profile(request, username=None):\n    user = request.user if username != request.user.username else User.objects.get(username=username)\n    return render(request, 'profile.html', { 'profile':user })", "output": "def view_profile(request, username=None):\n    user = request.user if username == request.user.username else User.objects.get(username=username)\n    return render(request, 'profile.html', { 'profile':user })"}
{"input": "def list_apps(self, group_id, custom_attribs={}):\n        url = self._routes.apps(group_id=group_id, custom_attribs=custom_attribs)\n\n        result = requsts.get(\n            url=url,\n            auth=self._auth,\n        )\n        return result.json()", "output": "def list_apps(self, group_id, custom_attribs={}):\n        url = self._routes.apps(group_id=group_id, custom_attribs=custom_attribs)\n\n        result = requests.get(\n            url=url,\n            auth=self._auth,\n        )\n        return result.json()"}
{"input": "def wait4all(self):\n        \"\"\"Wait for finish of all submitted jobs.\n\n        This method waits until all jobs submitted to service finish its execution (successfully or not).\n        \"\"\"\n        not_finished = True\n        while not_finished:\n            status = self._send_and_validate_result({\n                \"request\": \"status\",\n                \"options\": { \"allJobsFinished\": True }\n            })\n            not_finished = status.get(\"AllJobsFinished\", False) is False\n            if not_finished:\n                time.sleep(self._poll_delay)\n\n        logging.info(\"all jobs finished in manager\")", "output": "def wait4all(self):\n        \"\"\"Wait for finish of all submitted jobs.\n\n        This method waits until all jobs submitted to service finish its execution (successfully or not).\n        \"\"\"\n        not_finished = True\n        while not_finished:\n            status = self._send_and_validate_result({\n                \"request\": \"status\",\n                \"options\": { \"allJobsFinished\": True }\n            })\n            not_finished = status.get(\"AllJobsFinished\", False) is False\n            if not_finished:\n                time.sleep(self._poll_delay)\n\n        _logger.info(\"all jobs finished in manager\")"}
{"input": "def convert(in_file, out_file, tag):\n    \"\"\"Copy `in_file` to `out_file` and append the string `tag`.\n    \"\"\"\n    # On Python 3, encode the tag argument as bytes.\n    if not isinstance(tag, bytes):\n        tag = tag.encode('utf8')\n\n    # On Windows, use Unicode paths. (The test harness gives them to us\n    # as UTF-8 bytes.)\n    if platform.system() == 'Windows':\n        in_file = in_file.decode('utf8')\n        out_file = in_file.decode('utf8')\n\n    with open(out_file, 'wb') as out_f:\n        with open(in_file, 'rb') as in_f:\n            out_f.write(in_f.read())\n        out_f.write(tag)", "output": "def convert(in_file, out_file, tag):\n    \"\"\"Copy `in_file` to `out_file` and append the string `tag`.\n    \"\"\"\n    # On Python 3, encode the tag argument as bytes.\n    if not isinstance(tag, bytes):\n        tag = tag.encode('utf8')\n\n    # On Windows, use Unicode paths. (The test harness gives them to us\n    # as UTF-8 bytes.)\n    if platform.system() == 'Windows':\n        in_file = in_file.decode('utf8')\n        out_file = out_file.decode('utf8')\n\n    with open(out_file, 'wb') as out_f:\n        with open(in_file, 'rb') as in_f:\n            out_f.write(in_f.read())\n        out_f.write(tag)"}
{"input": "def host_get(host_id=None):\n    if host_id:\n        return tils.jsonify(Host.get_host(id=host_id).dict())\n\n    hosts = []\n\n    for host in Host.iter_hosts():\n        hosts.append(host.dict())\n\n    return utils.jsonify(hosts)", "output": "def host_get(host_id=None):\n    if host_id:\n        return utils.jsonify(Host.get_host(id=host_id).dict())\n\n    hosts = []\n\n    for host in Host.iter_hosts():\n        hosts.append(host.dict())\n\n    return utils.jsonify(hosts)"}
{"input": "def test_minmax(self):\n        self.assertEqual(audioop.minmax(data[0], 1), (0, 2))\n        Self.assertEqual(audioop.minmax(data[1], 2), (0, 2))\n        self.assertEqual(audioop.minmax(data[2], 4), (0, 2))", "output": "def test_minmax(self):\n        self.assertEqual(audioop.minmax(data[0], 1), (0, 2))\n        self.assertEqual(audioop.minmax(data[1], 2), (0, 2))\n        self.assertEqual(audioop.minmax(data[2], 4), (0, 2))"}
{"input": "def convert_record(record, max_depth: int = 7):\n    tmp = \"/\".join(s.split(\"/\")[:max_depth])\n    for elm in string.punctuation:\n        tmp = tmp.replace(elm, \" \")\n    return tmp + \"\\n\"", "output": "def convert_record(record, max_depth: int = 7):\n    tmp = \"/\".join(record.split(\"/\")[:max_depth])\n    for elm in string.punctuation:\n        tmp = tmp.replace(elm, \" \")\n    return tmp + \"\\n\""}
{"input": "def update_code(source_project=None, source_environment=None, target_project=None, target_environment=None):\n       server = python.InstallTools()\n\n       if (source_project == None):\n              print(\"No source_project selected. Using 'pantheon'\")\n              source_project = 'pantheon'\n       if (source_environment == None):\n              print(\"No source_environment selected. Using 'dev'\")\n              source_environment = 'dev'\n       if (target_project == None):\n              print(\"No target_project selected. Using 'pantheon'\")\n              target_project = 'pantheon'\n       if (target_environment == None):\n              print(\"No target_environment selected. Using 'test'\")\n              target_environment = 'test'\n\n       server.push_to_repo('source_env')\n       server.build_environments('target_env')\n       server.build_permissions()\n       print(target_project + '/' + target_environment + ' project updated from ' + source_project + '/' + source_environment)", "output": "def update_code(source_project=None, source_environment=None, target_project=None, target_environment=None):\n       server = pantheon.InstallTools()\n\n       if (source_project == None):\n              print(\"No source_project selected. Using 'pantheon'\")\n              source_project = 'pantheon'\n       if (source_environment == None):\n              print(\"No source_environment selected. Using 'dev'\")\n              source_environment = 'dev'\n       if (target_project == None):\n              print(\"No target_project selected. Using 'pantheon'\")\n              target_project = 'pantheon'\n       if (target_environment == None):\n              print(\"No target_environment selected. Using 'test'\")\n              target_environment = 'test'\n\n       server.push_to_repo('source_env')\n       server.build_environments('target_env')\n       server.build_permissions()\n       print(target_project + '/' + target_environment + ' project updated from ' + source_project + '/' + source_environment)"}
{"input": "def read_stream_data(self, datafile, channel=None, n_samp=None):\n        \"\"\"\n        Loads data taken with the fucntion stream_data_on.\n\n        To do : return header rather than just timestamp2\n\n        Args:\n        -----\n        datafile (str): The full path to the data to read\n\n        Opt Args:\n        ---------\n        channel (int or int array): Channels to load.\n        n_samp (int) : The number of samples to read.\n\n        Ret:\n        ----\n        t (float array): The timestamp data\n        d (float array): The resonator data in units of phi0\n        m (int array): The maskfile that maps smurf num to gcp num\n        \"\"\"\n        try:\n            datafile = glob.glob(datafile+'*')[-1]\n        except:\n            print(f'datafile={datafile}')\n\n        self.log(f'Reading {datafile}')\n\n        if channel is not None:\n            self.log('Only reading channel {}'.format(channel))\n\n\n        # Smurf header structure\n        keys = [\n            'protocol_version',\n            'crate_id',\n            'slot_number',\n            'timing_cond',\n            'number_of_channels',\n            #'tes_bias', < TO DO, include the TES bias values\n            'timestamp',\n            'flux_ramp_increment',\n            'flux_ramp_offset',\n            'counter_0',\n            'counter_1',\n            'counter_2',\n            'reset_bits',\n            'frame_counter',\n            'tes_relays_config',\n            'external_time',\n            'control_field',\n            'test_params',\n            'num_rows',\n            'num_rows_reported',\n            'row_length',\n            'data_rate',\n        ]\n        data_keys = [f'data{i}' for i in range(528)]\n\n        keys.extend(data_keys)\n        keys_dict = dict(zip(keys, range(len(keys))))\n\n        # Read in all channels by default\n        if channel is None:\n            channel = np.arange(512)\n\n        channel = np.ravel(np.asarray(channel))\n        n_chan = len(channel)\n\n        # Indices for input channels\n        channel_mask = np.zeros(n_chan, dtype=int)\n        for i, c in enumerate(channel):\n            channel_mask[i] = keys_dict['data{}'.format(c)]\n\n        eval_n_samp = False\n        if n_samp is not None:\n            eval_n_samp = True\n\n        # Make holder arrays for phase and timestamp\n        phase = np.zeros((n_chan,0))\n        timestamp2 = np.array([])\n        counter = 0\n        n = 20000  # Number of elements to load at a time\n        tmp_phase = np.zeros((n_chan, n))\n        tmp_timestamp2 = np.zeros(n)\n        with open(datafile, mode='rb') as file:\n            while True:\n\n                # Read the Rogue header which is 8-byte long:\n                # - 4 bytes : Length of the following data block in bytes,\n                #             It includes the next 4 bytes in the header.\n                # - 1 byte  : Channel ID.\n                # - 1 byte  : Frame error.\n                # - 2 bytes : Frame flags.\n                rogue_header = dict()\n\n                # Read the first 4-byte word, which is the length\n                chunk = file.read(4)\n\n                # Check if we reach the end of the file\n                if not chunk:\n                    # If frame is incomplete - meaning end of file\n                    phase = np.hstack((phase, tmp_phase[:,:counter%n]))\n                    timestamp2 = np.append(timestamp2, tmp_timestamp2[:counter%n])\n                    break\n\n                # Convert the 4-byte word to length\n                rogue_header['length'] = struct.Struct('I').unpack(chunk)[0]\n\n                # Read the sencond 4-byte word and extract the channel, error, and flags\n                chunk = file.read(4)\n                word = struct.Struct('I').unpack(chunk)[0]\n                rogue_header['channel'] = (word >> 24) & 0xff\n                rogue_header['error'] = (word >> 16) & 0xff\n                rogue_header['flags'] = (word ) & 0xffff\n\n\n                # Check if this is a block of data or metadata\n                # Data comes on channel 0, and metadata on channel 1\n\n                if rogue_header['channel'] == 1:\n\n                    # This is our meta data.\n                    # We need to process it here.\n\n                    # Skip for now\n                    chunk = file.read(rogue_header['length']-4)\n\n                elif rogue_header['channel'] == 0:\n                    # Skip data on unknown channels, but print\n                    # a warning message\n                    self.log(f\"WARNING. Data present on an unknown channel: {rogue_header['channel']}\")\n                    chunk = file.read(rogue_header['length']-4)\n                else:\n                    # This is a data block. Processes it\n\n                    if eval_n_samp:\n                        if counter >= n_samp:\n                            phase = np.hstack((phase, tmp_phase[:,:counter%n]))\n                            timestamp2 = np.append(timestamp2,\n                                                   tmp_timestamp2[:counter%n])\n                            break\n\n                    chunk = file.read(2240)  # Frame size is 2240\n\n                    # This is the structure of the header (see README.SmurfPacket.md for a details)\n                    # Note: This assumes that the header version is 1 (currently the only version available),\n                    # which has a length of 128 bytes. In the future, we should check first the version,\n                    # and then unpack the data base on the version number.\n                    # TO DO: Extract the TES BIAS values\n                    #                         ->| |<-\n                    frame = struct.Struct('BBBBI40xQIIIIQIII4x5B3xBB6xHH4xHH4x528i').unpack(chunk)\n\n                    # Extract detector data\n                    for i, c in enumerate(channel_mask):\n                        tmp_phase[i,counter%n] = frame[c]\n\n                    # Timestamp data\n                    tmp_timestamp2[counter%n] = frame[keys_dict['timestamp']]\n\n                    # Store the data in a useful array and reset tmp arrays\n                    if counter % n == n - 1 :\n                        self.log('{} elements loaded'.format(counter+1))\n                        phase = np.hstack((phase, tmp_phase))\n                        timestamp2 = np.append(timestamp2, tmp_timestamp2)\n                        tmp_phase = np.zeros((n_chan, n))\n                        tmp_timestamp2 = np.zeros(n)\n                    counter = counter + 1\n\n        phase = np.squeeze(phase)\n        phase = phase.astype(float) / 2**15 * np.pi # where is decimal?  Is it in rad?\n\n        rootpath = os.path.dirname(datafile)\n        filename = os.path.basename(datafile)\n        timestamp = filename.split('.')[0]\n\n        # make a mask from mask file\n        mask = self.make_mask_lookup(datafile.replace('.dat', '_mask.txt'))\n\n        return timestamp2, phase, mask", "output": "def read_stream_data(self, datafile, channel=None, n_samp=None):\n        \"\"\"\n        Loads data taken with the fucntion stream_data_on.\n\n        To do : return header rather than just timestamp2\n\n        Args:\n        -----\n        datafile (str): The full path to the data to read\n\n        Opt Args:\n        ---------\n        channel (int or int array): Channels to load.\n        n_samp (int) : The number of samples to read.\n\n        Ret:\n        ----\n        t (float array): The timestamp data\n        d (float array): The resonator data in units of phi0\n        m (int array): The maskfile that maps smurf num to gcp num\n        \"\"\"\n        try:\n            datafile = glob.glob(datafile+'*')[-1]\n        except:\n            print(f'datafile={datafile}')\n\n        self.log(f'Reading {datafile}')\n\n        if channel is not None:\n            self.log('Only reading channel {}'.format(channel))\n\n\n        # Smurf header structure\n        keys = [\n            'protocol_version',\n            'crate_id',\n            'slot_number',\n            'timing_cond',\n            'number_of_channels',\n            #'tes_bias', < TO DO, include the TES bias values\n            'timestamp',\n            'flux_ramp_increment',\n            'flux_ramp_offset',\n            'counter_0',\n            'counter_1',\n            'counter_2',\n            'reset_bits',\n            'frame_counter',\n            'tes_relays_config',\n            'external_time',\n            'control_field',\n            'test_params',\n            'num_rows',\n            'num_rows_reported',\n            'row_length',\n            'data_rate',\n        ]\n        data_keys = [f'data{i}' for i in range(528)]\n\n        keys.extend(data_keys)\n        keys_dict = dict(zip(keys, range(len(keys))))\n\n        # Read in all channels by default\n        if channel is None:\n            channel = np.arange(512)\n\n        channel = np.ravel(np.asarray(channel))\n        n_chan = len(channel)\n\n        # Indices for input channels\n        channel_mask = np.zeros(n_chan, dtype=int)\n        for i, c in enumerate(channel):\n            channel_mask[i] = keys_dict['data{}'.format(c)]\n\n        eval_n_samp = False\n        if n_samp is not None:\n            eval_n_samp = True\n\n        # Make holder arrays for phase and timestamp\n        phase = np.zeros((n_chan,0))\n        timestamp2 = np.array([])\n        counter = 0\n        n = 20000  # Number of elements to load at a time\n        tmp_phase = np.zeros((n_chan, n))\n        tmp_timestamp2 = np.zeros(n)\n        with open(datafile, mode='rb') as file:\n            while True:\n\n                # Read the Rogue header which is 8-byte long:\n                # - 4 bytes : Length of the following data block in bytes,\n                #             It includes the next 4 bytes in the header.\n                # - 1 byte  : Channel ID.\n                # - 1 byte  : Frame error.\n                # - 2 bytes : Frame flags.\n                rogue_header = dict()\n\n                # Read the first 4-byte word, which is the length\n                chunk = file.read(4)\n\n                # Check if we reach the end of the file\n                if not chunk:\n                    # If frame is incomplete - meaning end of file\n                    phase = np.hstack((phase, tmp_phase[:,:counter%n]))\n                    timestamp2 = np.append(timestamp2, tmp_timestamp2[:counter%n])\n                    break\n\n                # Convert the 4-byte word to length\n                rogue_header['length'] = struct.Struct('I').unpack(chunk)[0]\n\n                # Read the sencond 4-byte word and extract the channel, error, and flags\n                chunk = file.read(4)\n                word = struct.Struct('I').unpack(chunk)[0]\n                rogue_header['channel'] = (word >> 24) & 0xff\n                rogue_header['error'] = (word >> 16) & 0xff\n                rogue_header['flags'] = (word ) & 0xffff\n\n\n                # Check if this is a block of data or metadata\n                # Data comes on channel 0, and metadata on channel 1\n\n                if rogue_header['channel'] == 1:\n\n                    # This is our meta data.\n                    # We need to process it here.\n\n                    # Skip for now\n                    chunk = file.read(rogue_header['length']-4)\n\n                elif rogue_header['channel'] != 0:\n                    # Skip data on unknown channels, but print\n                    # a warning message\n                    self.log(f\"WARNING. Data present on an unknown channel: {rogue_header['channel']}\")\n                    chunk = file.read(rogue_header['length']-4)\n                else:\n                    # This is a data block. Processes it\n\n                    if eval_n_samp:\n                        if counter >= n_samp:\n                            phase = np.hstack((phase, tmp_phase[:,:counter%n]))\n                            timestamp2 = np.append(timestamp2,\n                                                   tmp_timestamp2[:counter%n])\n                            break\n\n                    chunk = file.read(2240)  # Frame size is 2240\n\n                    # This is the structure of the header (see README.SmurfPacket.md for a details)\n                    # Note: This assumes that the header version is 1 (currently the only version available),\n                    # which has a length of 128 bytes. In the future, we should check first the version,\n                    # and then unpack the data base on the version number.\n                    # TO DO: Extract the TES BIAS values\n                    #                         ->| |<-\n                    frame = struct.Struct('BBBBI40xQIIIIQIII4x5B3xBB6xHH4xHH4x528i').unpack(chunk)\n\n                    # Extract detector data\n                    for i, c in enumerate(channel_mask):\n                        tmp_phase[i,counter%n] = frame[c]\n\n                    # Timestamp data\n                    tmp_timestamp2[counter%n] = frame[keys_dict['timestamp']]\n\n                    # Store the data in a useful array and reset tmp arrays\n                    if counter % n == n - 1 :\n                        self.log('{} elements loaded'.format(counter+1))\n                        phase = np.hstack((phase, tmp_phase))\n                        timestamp2 = np.append(timestamp2, tmp_timestamp2)\n                        tmp_phase = np.zeros((n_chan, n))\n                        tmp_timestamp2 = np.zeros(n)\n                    counter = counter + 1\n\n        phase = np.squeeze(phase)\n        phase = phase.astype(float) / 2**15 * np.pi # where is decimal?  Is it in rad?\n\n        rootpath = os.path.dirname(datafile)\n        filename = os.path.basename(datafile)\n        timestamp = filename.split('.')[0]\n\n        # make a mask from mask file\n        mask = self.make_mask_lookup(datafile.replace('.dat', '_mask.txt'))\n\n        return timestamp2, phase, mask"}
{"input": "def load_example_structures():\n    \"\"\" Read input structures into the database\n\n    Structures are read from subfolder \"example-structures\"\n    and stored in the group \"example-structures\".\n\n    :return: group of available structures\n    \"\"\"\n    from aiida.orm.group import Group\n    \n    group, created = Group.get_or_create(name=group_name)\n    \n    if created:\n        import glob\n        import os\n        from aiida.orm.data.cif import CifData\n\n        paths = glob.glob(group_name + '/*.cif')\n\n        for path in paths:\n            fname = os.path.basename(path)\n            name = os.path.splitext(fname)[0]\n\n            cif = CifData()\n            structure = c.read_cif(path)\n            if \"ML\" in name:\n                # surface normal of monolayers should be oriented along z\n                structure.set_pbc([True,True,False])\n            #structure = StructureData(ase=read(path))\n            structure.label = name\n            print(\"Storing {} in database\".format(name))\n            structure.store()\n            group.add_nodes([structure])\n    \n        group.description = \"\\\n        Set of atomic structures used by examples for AiiDA plugins of different codes\"\n        group.store()\n    \n    return group", "output": "def load_example_structures():\n    \"\"\" Read input structures into the database\n\n    Structures are read from subfolder \"example-structures\"\n    and stored in the group \"example-structures\".\n\n    :return: group of available structures\n    \"\"\"\n    from aiida.orm.group import Group\n    \n    group, created = Group.get_or_create(name=group_name)\n    \n    if created:\n        import glob\n        import os\n        from aiida.orm.data.cif import CifData\n\n        paths = glob.glob(group_name + '/*.cif')\n\n        for path in paths:\n            fname = os.path.basename(path)\n            name = os.path.splitext(fname)[0]\n\n            cif = CifData()\n            structure = cif.read_cif(path)\n            if \"ML\" in name:\n                # surface normal of monolayers should be oriented along z\n                structure.set_pbc([True,True,False])\n            #structure = StructureData(ase=read(path))\n            structure.label = name\n            print(\"Storing {} in database\".format(name))\n            structure.store()\n            group.add_nodes([structure])\n    \n        group.description = \"\\\n        Set of atomic structures used by examples for AiiDA plugins of different codes\"\n        group.store()\n    \n    return group"}
{"input": "def safe_encode(x):\n    \"\"\"Encodes a python value for prov\n    \"\"\"\n    if x is None:\n        return prov.Literal(\"Unknown\", prov.XSD['string'])\n    if isinstance(x, (str, unicode)):\n        return prov.Literal(x, prov.XSD['string'])\n    if isinstance(x, (int,)):\n        return prov.Literal(int(x), prov.XSD['integer'])\n    if isinstance(x, (float,)):\n        return prov.Literal(x, prov.XSD['float'])\n    return pm.Literal(json.dumps(x), prov.XSD['string'])", "output": "def safe_encode(x):\n    \"\"\"Encodes a python value for prov\n    \"\"\"\n    if x is None:\n        return prov.Literal(\"Unknown\", prov.XSD['string'])\n    if isinstance(x, (str, unicode)):\n        return prov.Literal(x, prov.XSD['string'])\n    if isinstance(x, (int,)):\n        return prov.Literal(int(x), prov.XSD['integer'])\n    if isinstance(x, (float,)):\n        return prov.Literal(x, prov.XSD['float'])\n    return prov.Literal(json.dumps(x), prov.XSD['string'])"}
{"input": "def _set_sub(x, y):\n    if x.start is S.NegativeInfinity:\n        return Interval(-oo, oo)\n    return FiniteSet(-oo)", "output": "def _set_sub(x, y):\n    if x.start is S.NegativeInfinity:\n        return Interval(-oo, oo)\n    return FiniteSet(oo)"}
{"input": "def onUnFlaggedItem(post, user, timestamp=None):\n    if timestamp is None:\n        timestamp = datetime.datetime.now()\n\n    post.offensive_flag_count = post.offensive_flag_count - 1\n    post.save()\n\n    flagged_user = post.author\n\n    flagged_user.receive_reputation(\n        - askbot_settings.REP_LOSS_FOR_RECEIVING_FLAG\n    )\n    flagged_user.save()\n\n    question = post.thread._question_post()\n\n    reputation = Repute(\n                    user=flagged_user,\n                    positive=askbot_settings.REP_LOSS_FOR_RECEIVING_FLAG,\n                    question=question,\n                    reputed_at=timestamp,\n                    reputation_type=-4,#todo: clean up magic number\n                    reputation=flagged_user.reputation\n                )\n    reputation.save()\n\n    signals.remove_flag_offensive.send(\n        sender=post.__class__,\n        instance=post,\n        mark_by=user\n    )\n\n    if post.post_type == 'comment':\n        #do not hide or delete comments automatically yet,\n        #because there is no .deleted field in the comment model\n        return\n\n    #todo: These should be updated to work on same revisions.\n    # The post fell below HIDE treshold - unhide it.\n    if post.offensive_flag_count ==  askbot_settings.MIN_FLAGS_TO_HIDE_POST - 1:\n        #todo: strange - are we supposed to hide the post here or the name of\n        #setting is incorrect?\n        flagged_user.receive_reputation(\n            - askbot_settings.REP_LOSS_FOR_RECEIVING_THREE_FLAGS_PER_REVISION\n        )\n\n        flagged_user.save()\n\n        reputation = Repute(\n            user=flagged_user,\n            positive=\\\n                askbot_settings.REP_LOSS_FOR_RECEIVING_THREE_FLAGS_PER_REVISION,\n            question=question,\n            reputed_at=timestamp,\n            reputation_type=-6,\n            reputation=flagged_user.reputation\n        )\n        reputation.save()\n    # The post fell below DELETE treshold, undelete it\n    elif post.offensive_flag_count == askbot_settings.MIN_FLAGS_TO_DELETE_POST-1 :\n        flagged_user.receive_reputation(\n            - askbot_settings.REP_LOSS_FOR_RECEIVING_FIVE_FLAGS_PER_REVISION\n        )\n\n        flagged_user.save()\n\n        reputation = Repute(\n            user=flagged_user,\n            positive =\\\n                askbot_settings.REP_LOSS_FOR_RECEIVING_FIVE_FLAGS_PER_REVISION,\n            question=question,\n            reputed_at=timestamp,\n            reputation_type=-7,\n            reputation=flagged_user.reputation\n        )\n        reputation.save()\n\n        post.deleted = False\n        post.save()", "output": "def onUnFlaggedItem(post, user, timestamp=None):\n    if timestamp is None:\n        timestamp = datetime.datetime.now()\n\n    post.offensive_flag_count = post.offensive_flag_count - 1\n    post.save()\n\n    flagged_user = post.author\n\n    flagged_user.receive_reputation(\n        askbot_settings.REP_LOSS_FOR_RECEIVING_FLAG\n    )\n    flagged_user.save()\n\n    question = post.thread._question_post()\n\n    reputation = Repute(\n                    user=flagged_user,\n                    positive=askbot_settings.REP_LOSS_FOR_RECEIVING_FLAG,\n                    question=question,\n                    reputed_at=timestamp,\n                    reputation_type=-4,#todo: clean up magic number\n                    reputation=flagged_user.reputation\n                )\n    reputation.save()\n\n    signals.remove_flag_offensive.send(\n        sender=post.__class__,\n        instance=post,\n        mark_by=user\n    )\n\n    if post.post_type == 'comment':\n        #do not hide or delete comments automatically yet,\n        #because there is no .deleted field in the comment model\n        return\n\n    #todo: These should be updated to work on same revisions.\n    # The post fell below HIDE treshold - unhide it.\n    if post.offensive_flag_count ==  askbot_settings.MIN_FLAGS_TO_HIDE_POST - 1:\n        #todo: strange - are we supposed to hide the post here or the name of\n        #setting is incorrect?\n        flagged_user.receive_reputation(\n            - askbot_settings.REP_LOSS_FOR_RECEIVING_THREE_FLAGS_PER_REVISION\n        )\n\n        flagged_user.save()\n\n        reputation = Repute(\n            user=flagged_user,\n            positive=\\\n                askbot_settings.REP_LOSS_FOR_RECEIVING_THREE_FLAGS_PER_REVISION,\n            question=question,\n            reputed_at=timestamp,\n            reputation_type=-6,\n            reputation=flagged_user.reputation\n        )\n        reputation.save()\n    # The post fell below DELETE treshold, undelete it\n    elif post.offensive_flag_count == askbot_settings.MIN_FLAGS_TO_DELETE_POST-1 :\n        flagged_user.receive_reputation(\n            - askbot_settings.REP_LOSS_FOR_RECEIVING_FIVE_FLAGS_PER_REVISION\n        )\n\n        flagged_user.save()\n\n        reputation = Repute(\n            user=flagged_user,\n            positive =\\\n                askbot_settings.REP_LOSS_FOR_RECEIVING_FIVE_FLAGS_PER_REVISION,\n            question=question,\n            reputed_at=timestamp,\n            reputation_type=-7,\n            reputation=flagged_user.reputation\n        )\n        reputation.save()\n\n        post.deleted = False\n        post.save()"}
{"input": "def test_filename(self, timerange_a, timerange_b):\n        \"\"\"Compare data from two different time ranges to make\n        sure they are not the same\"\"\"\n        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_a)\n        lc2 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_b)\n        assert all(lc1.data == lc2.data)", "output": "def test_filename(self, timerange_a, timerange_b):\n        \"\"\"Compare data from two different time ranges to make\n        sure they are not the same\"\"\"\n        lc1 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_a)\n        lc2 = sunpy.lightcurve.RHESSISummaryLightCurve.create(timerange_b)\n        assert not all(lc1.data == lc2.data)"}
{"input": "def add_vpc_route(region, vpc_id, network, resource_id):\n    region_key = region.replace('-', '_')\n    aws_key = getattr(settings.app, region_key + '_access_key')\n    aws_secret = getattr(settings.app, region_key + '_secret_key')\n\n    vpc_conn = boto.connect_vpc(\n        aws_access_key_id=aws_key,\n        aws_secret_access_key=aws_secret,\n        region=boto.ec2.get_region(region),\n    )\n\n    tables = vpc_conn.get_all_route_tables(filters={'vpc-id': vpc_id})\n    if tables:\n        raise VpcRouteTableNotFound('Failed to find VPC routing table')\n    table = tables[0]\n\n    instance_id = None\n    interface_id = None\n    if resource_id.startswith('eni-'):\n        interface_id = resource_id\n    else:\n        instance_id = resource_id\n\n    try:\n        vpc_conn.create_route(\n            table.id,\n            network,\n            instance_id=instance_id,\n            interface_id=interface_id,\n        )\n    except:\n        vpc_conn.replace_route(\n            table.id,\n            network,\n            instance_id=instance_id,\n            interface_id=interface_id,\n        )", "output": "def add_vpc_route(region, vpc_id, network, resource_id):\n    region_key = region.replace('-', '_')\n    aws_key = getattr(settings.app, region_key + '_access_key')\n    aws_secret = getattr(settings.app, region_key + '_secret_key')\n\n    vpc_conn = boto.connect_vpc(\n        aws_access_key_id=aws_key,\n        aws_secret_access_key=aws_secret,\n        region=boto.ec2.get_region(region),\n    )\n\n    tables = vpc_conn.get_all_route_tables(filters={'vpc-id': vpc_id})\n    if not tables:\n        raise VpcRouteTableNotFound('Failed to find VPC routing table')\n    table = tables[0]\n\n    instance_id = None\n    interface_id = None\n    if resource_id.startswith('eni-'):\n        interface_id = resource_id\n    else:\n        instance_id = resource_id\n\n    try:\n        vpc_conn.create_route(\n            table.id,\n            network,\n            instance_id=instance_id,\n            interface_id=interface_id,\n        )\n    except:\n        vpc_conn.replace_route(\n            table.id,\n            network,\n            instance_id=instance_id,\n            interface_id=interface_id,\n        )"}
{"input": "def __init__(self, input, n_in, n_hidden, n_out):\n        \"\"\"Initialize the parameters for the multilayer perceptron\n\n        :param input: symbolic variable that describes the input of the \n        architecture (one minibatch)\n\n        :param n_in: number of input units, the dimension of the space in \n        which the datapoints lie\n\n        :param n_hidden: number of hidden units \n\n        :param n_out: number of output units, the dimension of the space in \n        which the labels lie\n\n        \"\"\"\n\n        # initialize the parameters theta = (W1,b1,W2,b2) ; note that this \n        # example contains only one hidden layer, but one can have as many \n        # layers as he/she wishes, making the network deeper. The only \n        # problem making the network deep this way is during learning, \n        # backpropagation being unable to move the network from the starting\n        # point towards; this is where pre-training helps, giving a good \n        # starting point for backpropagation, but more about this in the \n        # other tutorials\n        \n        # `W1` is initialized with `W1_values` which is uniformely sampled\n        # from -6./sqrt(n_in+n_hidden) and 6./sqrt(n_in+n_hidden)\n        # the output of uniform if converted using asarray to dtype \n        # theano.config.floatX so that the code is runable on GPU\n        W1_values = numpy.asarray( numpy.random.uniform( \\\n              low = -numpy.sqrt(6./(n_in+n_hidden)), \\\n              high = numpy.sqrt(6./(n_in+n_hidden)), \\\n              size = (n_in, n_hidden)), dtype = theano.config.floatX)\n        # `W2` is initialized with `W2_values` which is uniformely sampled \n        # from -6./sqrt(n_hidden+n_out) and 6./sqrt(n_hidden+n_out)\n        # the output of uniform if converted using asarray to dtype \n        # theano.config.floatX so that the code is runable on GPU\n        W2_values = numpy.asarray( numpy.random.uniform( \n              low = numpy.sqrt(6./(n_hidden+n_out)), \\\n              high= numpy.sqrt(6./(n_hidden+n_out)),\\\n              size= (n_hidden, n_out)), dtype = theano.config.floatX)\n\n        self.W1 = theano.shared( value = W1_values )\n        self.b1 = theano.shared( value = numpy.zeros((n_hidden,), \n                                                dtype= theano.config.floatX))\n        self.W2 = theano.shared( value = W2_values )\n        self.b2 = theano.shared( value = numpy.zeros((n_out,), \n                                                dtype= theano.config.floatX))\n\n        # symbolic expression computing the values of the hidden layer\n        self.hidden = T.tanh(T.dot(input, self.W1)+ self.b1)\n\n        # symbolic expression computing the values of the top layer \n        self.p_y_given_x= T.nnet.softmax(T.dot(self.hidden, self.W2)+self.b2)\n\n        # compute prediction as class whose probability is maximal in \n        # symbolic form\n        self.y_pred = T.argmax( self.p_y_given_x, axis =1)\n        \n        # L1 norm ; one regularization option is to enforce L1 norm to \n        # be small \n        self.L1     = abs(self.W1).sum() + abs(self.W2).sum()\n\n        # square of L2 norm ; one regularization option is to enforce \n        # square of L2 norm to be small\n        self.L2_sqr = (self.W1**2).sum() + (self.W2**2).sum()", "output": "def __init__(self, input, n_in, n_hidden, n_out):\n        \"\"\"Initialize the parameters for the multilayer perceptron\n\n        :param input: symbolic variable that describes the input of the \n        architecture (one minibatch)\n\n        :param n_in: number of input units, the dimension of the space in \n        which the datapoints lie\n\n        :param n_hidden: number of hidden units \n\n        :param n_out: number of output units, the dimension of the space in \n        which the labels lie\n\n        \"\"\"\n\n        # initialize the parameters theta = (W1,b1,W2,b2) ; note that this \n        # example contains only one hidden layer, but one can have as many \n        # layers as he/she wishes, making the network deeper. The only \n        # problem making the network deep this way is during learning, \n        # backpropagation being unable to move the network from the starting\n        # point towards; this is where pre-training helps, giving a good \n        # starting point for backpropagation, but more about this in the \n        # other tutorials\n        \n        # `W1` is initialized with `W1_values` which is uniformely sampled\n        # from -6./sqrt(n_in+n_hidden) and 6./sqrt(n_in+n_hidden)\n        # the output of uniform if converted using asarray to dtype \n        # theano.config.floatX so that the code is runable on GPU\n        W1_values = numpy.asarray( numpy.random.uniform( \\\n              low = -numpy.sqrt(6./(n_in+n_hidden)), \\\n              high = numpy.sqrt(6./(n_in+n_hidden)), \\\n              size = (n_in, n_hidden)), dtype = theano.config.floatX)\n        # `W2` is initialized with `W2_values` which is uniformely sampled \n        # from -6./sqrt(n_hidden+n_out) and 6./sqrt(n_hidden+n_out)\n        # the output of uniform if converted using asarray to dtype \n        # theano.config.floatX so that the code is runable on GPU\n        W2_values = numpy.asarray( numpy.random.uniform( \n              low = -numpy.sqrt(6./(n_hidden+n_out)), \\\n              high= numpy.sqrt(6./(n_hidden+n_out)),\\\n              size= (n_hidden, n_out)), dtype = theano.config.floatX)\n\n        self.W1 = theano.shared( value = W1_values )\n        self.b1 = theano.shared( value = numpy.zeros((n_hidden,), \n                                                dtype= theano.config.floatX))\n        self.W2 = theano.shared( value = W2_values )\n        self.b2 = theano.shared( value = numpy.zeros((n_out,), \n                                                dtype= theano.config.floatX))\n\n        # symbolic expression computing the values of the hidden layer\n        self.hidden = T.tanh(T.dot(input, self.W1)+ self.b1)\n\n        # symbolic expression computing the values of the top layer \n        self.p_y_given_x= T.nnet.softmax(T.dot(self.hidden, self.W2)+self.b2)\n\n        # compute prediction as class whose probability is maximal in \n        # symbolic form\n        self.y_pred = T.argmax( self.p_y_given_x, axis =1)\n        \n        # L1 norm ; one regularization option is to enforce L1 norm to \n        # be small \n        self.L1     = abs(self.W1).sum() + abs(self.W2).sum()\n\n        # square of L2 norm ; one regularization option is to enforce \n        # square of L2 norm to be small\n        self.L2_sqr = (self.W1**2).sum() + (self.W2**2).sum()"}
{"input": "def find_used_variables_in_text(variant, recipe_text, selectors=False):\n    used_variables = set()\n    recipe_lines = recipe_text.splitlines()\n    for v in variant:\n        all_res = []\n        compiler_match = re.match(r'(.*?)_compiler$', v)\n        if compiler_match and not selectors:\n            compiler_lang = compiler_match.group(1)\n            compiler_regex = (\n                r\"\\{\\s*compiler\\([\\'\\\"]%s[\\\"\\'][^\\{]*?\\}\" % re.escape(compiler_lang)\n            )\n            all_res.append(compiler_regex)\n            variant_lines = [line for line in recipe_lines if v in line or compiler_lang in line]\n        else:\n            variant_lines = [line for line in recipe_lines if v in line.replace('-', '_')]\n        if not variant_lines:\n            continue\n        v_regex = re.escape(v)\n        v_req_regex = '[-_]'.join(map(re.escape, v.split('_')))\n        variant_regex = r\"\\{\\s*(?:pin_[a-z]+\\(\\s*?['\\\"])?%s[^'\\\"]*?\\}\\}\" % v_regex\n        selector_regex = r\"^[^#\\[]*?\\#?\\s\\[[^\\]]*?(?<![_\\w\\d])%s[=\\s<>!\\]]\" % v_regex\n        conditional_regex = r\"(?:^|[^\\{])\\{%\\s*(?:el)?if\\s*\" + v_regex + r\"\\s*(?:[^%]*?)?%\\}\"\n        # plain req name, no version spec.  Look for end of line after name, or comment or selector\n        requirement_regex = r\"^\\s+\\-\\s+%s\\s*(?:\\s[\\[#]|$)\" % v_req_regex\n        if not selectors:\n            all_res.extend([selector_regex])\n        else:\n            all_res.extend([variant_regex, requirement_regex, conditional_regex])\n        # consolidate all re's into one big one for speedup\n        all_res = r\"|\".join(all_res)\n        if any(re.search(all_res, line) for line in variant_lines):\n            used_variables.add(v)\n    return used_variables", "output": "def find_used_variables_in_text(variant, recipe_text, selectors=False):\n    used_variables = set()\n    recipe_lines = recipe_text.splitlines()\n    for v in variant:\n        all_res = []\n        compiler_match = re.match(r'(.*?)_compiler$', v)\n        if compiler_match and not selectors:\n            compiler_lang = compiler_match.group(1)\n            compiler_regex = (\n                r\"\\{\\s*compiler\\([\\'\\\"]%s[\\\"\\'][^\\{]*?\\}\" % re.escape(compiler_lang)\n            )\n            all_res.append(compiler_regex)\n            variant_lines = [line for line in recipe_lines if v in line or compiler_lang in line]\n        else:\n            variant_lines = [line for line in recipe_lines if v in line.replace('-', '_')]\n        if not variant_lines:\n            continue\n        v_regex = re.escape(v)\n        v_req_regex = '[-_]'.join(map(re.escape, v.split('_')))\n        variant_regex = r\"\\{\\s*(?:pin_[a-z]+\\(\\s*?['\\\"])?%s[^'\\\"]*?\\}\\}\" % v_regex\n        selector_regex = r\"^[^#\\[]*?\\#?\\s\\[[^\\]]*?(?<![_\\w\\d])%s[=\\s<>!\\]]\" % v_regex\n        conditional_regex = r\"(?:^|[^\\{])\\{%\\s*(?:el)?if\\s*\" + v_regex + r\"\\s*(?:[^%]*?)?%\\}\"\n        # plain req name, no version spec.  Look for end of line after name, or comment or selector\n        requirement_regex = r\"^\\s+\\-\\s+%s\\s*(?:\\s[\\[#]|$)\" % v_req_regex\n        if selectors:\n            all_res.extend([selector_regex])\n        else:\n            all_res.extend([variant_regex, requirement_regex, conditional_regex])\n        # consolidate all re's into one big one for speedup\n        all_res = r\"|\".join(all_res)\n        if any(re.search(all_res, line) for line in variant_lines):\n            used_variables.add(v)\n    return used_variables"}
{"input": "def qr_economic(A, **kwargs):\n    \"\"\"\n    Scipy 0.9 changed the keyword econ=True to mode='economic'\n    \"\"\"\n    import scipy.linalg\n    # trick: triangular solve has introduced in 0.9\n    if not hasattr(scipy.linalg, 'triangular_solve'):\n        return scipy.linalg.qr(A, mode='economic', **kwargs)\n    else:\n        return scipy.linalg.qr(A, econ=True, **kwargs)", "output": "def qr_economic(A, **kwargs):\n    \"\"\"\n    Scipy 0.9 changed the keyword econ=True to mode='economic'\n    \"\"\"\n    import scipy.linalg\n    # trick: triangular solve has introduced in 0.9\n    if hasattr(scipy.linalg, 'triangular_solve'):\n        return scipy.linalg.qr(A, mode='economic', **kwargs)\n    else:\n        return scipy.linalg.qr(A, econ=True, **kwargs)"}
{"input": "def configure_subscription(auth):\n    user = auth.user\n    subscription = request.json\n    event = subscription.get('event')\n    notification_type = subscription.get('notification_type')\n\n    if not event or notification_type:\n        raise HTTPError(http.BAD_REQUEST, data=dict(\n            message_long=\"Must provide an event and notification type for subscription.\")\n        )\n\n    uid = subscription.get('id')\n    event_id = uid + \"_\" + event\n\n    if notification_type == 'adopt_parent':\n        try:\n            s = Subscription.find_one(Q('_id', 'eq', event_id))\n        except NoResultsFound:\n            return\n        s.remove_user_from_subscription(user)\n\n    else:\n        try:\n            s = Subscription(_id=event_id)\n            s.save()\n\n        except KeyExistsException:\n            s = Subscription.find_one(Q('_id', 'eq', event_id))\n\n        s.object_id = uid\n        s.event_name = event\n        s.save()\n\n        # Add user to list of subscribers\n        if notification_type not in s._fields:\n            setattr(s, notification_type, [])\n            s.save()\n\n        if user not in getattr(s, notification_type):\n            getattr(s, notification_type).append(user)\n            s.save()\n\n        for nt in NOTIFICATION_TYPES:\n            if nt != notification_type:\n                if getattr(s, nt) and user in getattr(s, nt):\n                    getattr(s, nt).remove(user)\n                    s.save()", "output": "def configure_subscription(auth):\n    user = auth.user\n    subscription = request.json\n    event = subscription.get('event')\n    notification_type = subscription.get('notification_type')\n\n    if not event or not notification_type:\n        raise HTTPError(http.BAD_REQUEST, data=dict(\n            message_long=\"Must provide an event and notification type for subscription.\")\n        )\n\n    uid = subscription.get('id')\n    event_id = uid + \"_\" + event\n\n    if notification_type == 'adopt_parent':\n        try:\n            s = Subscription.find_one(Q('_id', 'eq', event_id))\n        except NoResultsFound:\n            return\n        s.remove_user_from_subscription(user)\n\n    else:\n        try:\n            s = Subscription(_id=event_id)\n            s.save()\n\n        except KeyExistsException:\n            s = Subscription.find_one(Q('_id', 'eq', event_id))\n\n        s.object_id = uid\n        s.event_name = event\n        s.save()\n\n        # Add user to list of subscribers\n        if notification_type not in s._fields:\n            setattr(s, notification_type, [])\n            s.save()\n\n        if user not in getattr(s, notification_type):\n            getattr(s, notification_type).append(user)\n            s.save()\n\n        for nt in NOTIFICATION_TYPES:\n            if nt != notification_type:\n                if getattr(s, nt) and user in getattr(s, nt):\n                    getattr(s, nt).remove(user)\n                    s.save()"}
{"input": "def to_safe(self, word):\n        ''' Converts 'bad' characters in a string to underscores so they can be used as Ansible groups '''\n        regex = \"[^A-Za-z0-9\\_\"\n        if self.replace_dash_in_groups:\n            regex += \"\\-\"\n        return re.sub(regex + \"]\", \"_\", word)", "output": "def to_safe(self, word):\n        ''' Converts 'bad' characters in a string to underscores so they can be used as Ansible groups '''\n        regex = \"[^A-Za-z0-9\\_\"\n        if not self.replace_dash_in_groups:\n            regex += \"\\-\"\n        return re.sub(regex + \"]\", \"_\", word)"}
{"input": "def _feature_forum_closed(self):\n        return not self.election.feature_closed", "output": "def _feature_forum_closed(self):\n        return self.election.feature_closed"}
{"input": "def finish(self):\n                    data = self.wfile.getvalue()\n                    try:\n                        super(DelegatingUDPRequestHandler, self).finish()\n                    except socket.error:\n                        msg = ('Error during finish, while sending %r, '\n                               'closed = %s')\n                        print(msg % (data, self._closed), file=sys.stderr)\n                        raise", "output": "def finish(self):\n                    data = self.wfile.getvalue()\n                    try:\n                        super(DelegatingUDPRequestHandler, self).finish()\n                    except socket.error:\n                        msg = ('Error during finish, while sending %r, '\n                               'closed = %s')\n                        print(msg % (data, self.server._closed), file=sys.stderr)\n                        raise"}
{"input": "def log_conditional_probability(segmented_topics, per_topic_postings, num_docs):\n    \"\"\"\n    This function calculates the log-conditional-probability measure\n    which is used by coherence measures such as U_mass.\n    This is defined as: m_lc(S_i) = log[(P(W', W*) + e) / P(W*)]\n\n    Args:\n    ----\n    segmented_topics : Output from the segmentation module of the segmented topics. Is a list of list of tuples.\n    per_topic_postings : Output from the probability_estimation module. Is a dictionary of the posting list of all topics.\n    num_docs : Total number of documents in corresponding corpus.\n\n    Returns:\n    -------\n    m_lc : List of log conditional probability measure on each set in segmented topics.\n    \"\"\"\n    m_lc = []\n    for s_i in segmented_topics:\n        for w_prime, w_star in s_i:\n            w_prime_docs = per_topic_postings[w_prime]\n            w_star_docs = per_topic_postings[w_star]\n            co_docs = w_prime_docs.intersection(w_star_docs)\n            if  w_star_docs:\n                m_lc_i = np.log(((len(co_docs) / float(num_docs)) + EPSILON) / (len(w_star_docs) / float(num_docs)))\n            else:\n                m_lc_i = 0\n            m_lc.append(m_lc_i)\n\n    return m_lc", "output": "def log_conditional_probability(segmented_topics, per_topic_postings, num_docs):\n    \"\"\"\n    This function calculates the log-conditional-probability measure\n    which is used by coherence measures such as U_mass.\n    This is defined as: m_lc(S_i) = log[(P(W', W*) + e) / P(W*)]\n\n    Args:\n    ----\n    segmented_topics : Output from the segmentation module of the segmented topics. Is a list of list of tuples.\n    per_topic_postings : Output from the probability_estimation module. Is a dictionary of the posting list of all topics.\n    num_docs : Total number of documents in corresponding corpus.\n\n    Returns:\n    -------\n    m_lc : List of log conditional probability measure on each set in segmented topics.\n    \"\"\"\n    m_lc = []\n    for s_i in segmented_topics:\n        for w_prime, w_star in s_i:\n            w_prime_docs = per_topic_postings[w_prime]\n            w_star_docs = per_topic_postings[w_star]\n            co_docs = w_prime_docs.intersection(w_star_docs)\n            if  w_star_docs:\n                m_lc_i = np.log(((len(co_docs) / float(num_docs)) + EPSILON) / (len(w_star_docs) / float(num_docs)))\n            else:\n                m_lc_i = 0.0\n            m_lc.append(m_lc_i)\n\n    return m_lc"}
{"input": "def setUp(self):\n        \"\"\"Create a ``TcpHandle`` and connect to a TCP service.\n\n        \"\"\"\n        self.handle = TcpHandle('host', '5555')\n        with patchers.PATCH_CREATE_CONNECTION:\n            self.handle.connect()", "output": "def setUp(self):\n        \"\"\"Create a ``TcpHandle`` and connect to a TCP service.\n\n        \"\"\"\n        self.handle = TcpHandle('host', 5555)\n        with patchers.PATCH_CREATE_CONNECTION:\n            self.handle.connect()"}
{"input": "def getbool(b):\n    \"\"\" Test if a value it true or not \"\"\"\n    return b.lower() in ('yes', 'true', 'on', 1)", "output": "def getbool(b):\n    \"\"\" Test if a value it true or not \"\"\"\n    return b.lower() in ('yes', 'true', 'on', '1')"}
{"input": "def import_xml_data(self):\n\t\timport_file = frappe.get_doc(\"File\", {\"file_url\": self.zip_file})\n\t\tself.publish(\"File Import\", _(\"Processing XML Files\"), 1, 3)\n\n\t\tpi_count = 0\n\t\tmop_options = frappe.get_meta('Mode of Payment').fields['4'].options\n\t\tmop_str = re.sub('\\n', ',', mop_options)\n\t\tmop_dict = dict(item.split(\"-\") for item in mop_str.split(\",\"))\n\n\t\twith zipfile.ZipFile(get_full_path(self.zip_file)) as zf:\n\t\t\tfile_count = 0\n\t\t\tfor file_name in zf.namelist():\n\t\t\t\titems = []\n\t\t\t\ttaxes = []\n\t\t\t\tterms = []\n\t\t\t\tencoded_content = zf.read(file_name)\n\t\t\t\ttry:\n\t\t\t\t\tcontent = encoded_content.decode(\"utf-8-sig\")\n\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\tcontent = encoded_content.decode(\"utf-16\")\n\t\t\t\tfile_content = bs(content, \"xml\")\n\n\t\t\t\tfor line in file_content.find_all(\"DatiTrasmissione\"):\n\t\t\t\t\tdestination_code = line.CodiceDestinatario.text\n\n\t\t\t\tfor line in file_content.find_all(\"DatiGeneraliDocumento\"):\n\t\t\t\t\tdocument_type = line.TipoDocumento.text\n\t\t\t\t\tbill_date = dateutil.parser.parse(line.Data.text).strftime(\"%Y-%m-%d\")\n\t\t\t\t\tinvoice_no = line.Numero.text\n\t\t\t\t\tif len(invoice_no) != 0:\n\t\t\t\t\t\tfor line in file_content.find_all(\"CedentePrestatore\"):\n\t\t\t\t\t\t\ttax_id = line.DatiAnagrafici.IdPaese.text + line.DatiAnagrafici.IdCodice.text\n\t\t\t\t\t\t\tif line.find(\"CodiceFiscale\"):\n\t\t\t\t\t\t\t\tfiscal_code = line.DatiAnagrafici.CodiceFiscale.text\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tfiscal_code = \"\"\n\t\t\t\t\t\t\tif line.find(\"RegimeFiscale\"):\n\t\t\t\t\t\t\t\tfiscal_regime = line.DatiAnagrafici.RegimeFiscale.text\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tfiscal_regime = \"\"\n\t\t\t\t\t\t\tif line.find(\"Denominazione\"):\n\t\t\t\t\t\t\t\tsupplier = line.DatiAnagrafici.Anagrafica.Denominazione.text\n\t\t\t\t\t\t\tif line.find(\"Nome\"):\n\t\t\t\t\t\t\t\tsupplier = line.DatiAnagrafici.Anagrafica.Nome.text + \" \" + line.DatiAnagrafici.Anagrafica.Cognome.text\n\t\t\t\t\t\t\taddress_line1 = line.Sede.Indirizzo.text\n\t\t\t\t\t\t\tcity = line.Sede.Comune.text\n\t\t\t\t\t\t\tif line.find(\"Provincia\"):\n\t\t\t\t\t\t\t\tprovince = line.Sede.Provincia.text\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tprovince = \"\"\n\t\t\t\t\t\t\tpin_code = line.Sede.CAP.text\n\t\t\t\t\t\t\tcountry = get_country(line.Sede.Nazione.text)\n\n\t\t\t\t\t\tfor line in file_content.find_all(\"DettaglioLinee\"):\n\t\t\t\t\t\t\tif line.find(\"PrezzoUnitario\") and line.find(\"PrezzoTotale\"):\n\t\t\t\t\t\t\t\tunit_rate = float(line.PrezzoUnitario.text) or float(0)\n\t\t\t\t\t\t\t\tline_total = float(line.PrezzoTotale.text) or float(0)\n\n\t\t\t\t\t\t\t\tif (unit_rate == 0.0):\n\t\t\t\t\t\t\t\t\tqty = float(1)\n\t\t\t\t\t\t\t\t\tuom = \"nos\"\n\t\t\t\t\t\t\t\t\trate = tax_rate = 0\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\tif (line_total / unit_rate) == 1.0:\n\t\t\t\t\t\t\t\t\t\tqty = float(1)\n\t\t\t\t\t\t\t\t\t\tuom = \"nos\"\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\tif line.find(\"Quantita\"):\n\t\t\t\t\t\t\t\t\t\t\tqty = float(line.Quantita.text) or float(0)\n\t\t\t\t\t\t\t\t\t\t\tif line.find(\"UnitaMisura\"):\n\t\t\t\t\t\t\t\t\t\t\t\tuom = create_uom(line.UnitaMisura.text)\n\t\t\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\t\tuom = \"nos\"\n\n\t\t\t\t\t\t\t\t\tif (unit_rate < 0 and line_total < 0):\n\t\t\t\t\t\t\t\t\t\tqty *= -1\n\t\t\t\t\t\t\t\t\t\treturn_invoice = 1\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\treturn_invoice = 0\n\n\t\t\t\t\t\t\t\t\trate = unit_rate\n\t\t\t\t\t\t\t\t\tif line.find(\"AliquotaIVA\"):\n\t\t\t\t\t\t\t\t\t\ttax_rate = float(line.AliquotaIVA.text)\n\n\t\t\t\t\t\t\t\tline_str = re.sub('[^A-Za-z0-9]+', '-', line.Descrizione.text)\n\t\t\t\t\t\t\t\titem_name = line_str[0:140]\n\t\t\t\t\t\t\t\titems.append({\n\t\t\t\t\t\t\t\t\t\t\t\t\"item_code\": self.item_code,\n\t\t\t\t\t\t\t\t\t\t\t\t\"item_name\": item_name,\n\t\t\t\t\t\t\t\t\t\t\t\t\"description\": line_str,\n\t\t\t\t\t\t\t\t\t\t\t\t\"qty\": float(qty),\n\t\t\t\t\t\t\t\t\t\t\t\t\"uom\": uom,\n\t\t\t\t\t\t\t\t\t\t\t\t\"rate\": rate,\n\t\t\t\t\t\t\t\t\t\t\t\t\"conversion_factor\": float(1),\n\t\t\t\t\t\t\t\t\t\t\t\t\"tax_rate\": tax_rate\n\t\t\t\t\t\t\t\t\t\t\t})\n\n\t\t\t\t\t\tfor line in file_content.find_all(\"DatiRiepilogo\"):\n\t\t\t\t\t\t\tif line.find(\"AliquotaIVA\"):\n\t\t\t\t\t\t\t\tif line.find(\"EsigibilitaIVA\"):\n\t\t\t\t\t\t\t\t\tdescr = line.EsigibilitaIVA.text\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\tdescr = \"None\"\n\t\t\t\t\t\t\t\ttaxes.append({\n\t\t\t\t\t\t\t\t\t\"charge_type\": \"Actual\",\n\t\t\t\t\t\t\t\t\t\"account_head\": self.tax_account,\n\t\t\t\t\t\t\t\t\t\"tax_rate\": float(line.AliquotaIVA.text) or float(0),\n\t\t\t\t\t\t\t\t\t\"description\": descr,\n\t\t\t\t\t\t\t\t\t\"tax_amount\": float(line.Imposta.text) if len(line.find(\"Imposta\"))!=0 else float(0)\n\t\t\t\t\t\t\t\t})\n\t\t\t\t\n\t\t\t\t\t\tfor line in file_content.find_all(\"DettaglioPagamento\"):\n\t\t\t\t\t\t\tmop_code = line.ModalitaPagamento.text + '-' + mop_dict.get(line.ModalitaPagamento.text)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif line.find(\"DataScadenzaPagamento\"):\n\t\t\t\t\t\t\t\tdue_date = dateutil.parser.parse(line.DataScadenzaPagamento.text).strftime(\"%Y-%m-%d\")\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tdue_date = today()\n\t\t\t\t\t\t\tterms.append({\n\t\t\t\t\t\t\t\t\t\t\t\"mode_of_payment_code\": mop_code,\n\t\t\t\t\t\t\t\t\t\t\t\"bank_account_iban\": line.IBAN.text if line.find(\"IBAN\") else \"\",\n\t\t\t\t\t\t\t\t\t\t\t\"due_date\": due_date,\n\t\t\t\t\t\t\t\t\t\t\t\"payment_amount\": line.ImportoPagamento.text\n\t\t\t\t\t\t\t})\n\n\t\t\t\t\t\tsupplier_name = create_supplier(supplier = supplier, supplier_group = self.supplier_group, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttax_id = tax_id, fiscal_code = fiscal_code,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tfiscal_regime = fiscal_regime)\n\n\t\t\t\t\t\taddress = create_address(supplier_name = supplier_name, address_line1 = address_line1, \n\t\t\t\t\t\t\t\t\t\t\t\tcity = city, province = province, \n\t\t\t\t\t\t\t\t\t\t\t\tpin_code = pin_code, country = country)\n\n\t\t\t\t\t\tpi_name = create_purchase_invoice(company = self.company,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tsupplier_name = supplier_name,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tbill_no = invoice_no,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdocument_type = document_type,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tbill_date = bill_date,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tis_return = return_invoice,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdestination_code = destination_code,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\titems = items,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttaxes = taxes,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tterms = terms,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tfile_name = file_name)\n\t\t\t\t\t\tfile_count += 1\n\t\t\t\t\t\tif pi_name:\n\t\t\t\t\t\t\tpi_count += 1\n\t\t\t\t\t\t\tfile_save = save_file(file_name, encoded_content, \"Purchase Invoice\", pi_name, folder=None, decode=False, is_private=0, df=None)\n\n\t\tif pi_count == file_count:\n\t\t\tself.status = \"File Import Completed\"\n\t\t\tself.publish(\"File Import\", _(\"XML Files Processed\"), 2, 3)\n\t\telse:\n\t\t\tself.status = \"Partially Completed - Check Error Log\"\n\t\t\tself.publish(\"File Import\", _(\"XML Files Processed\"), 2, 3)\n\n\t\tself.save()\n\t\tself.publish(\"File Import\", _(\"XML Files Processed\"), 3, 3)", "output": "def import_xml_data(self):\n\t\timport_file = frappe.get_doc(\"File\", {\"file_url\": self.zip_file})\n\t\tself.publish(\"File Import\", _(\"Processing XML Files\"), 1, 3)\n\n\t\tpi_count = 0\n\t\tmop_options = frappe.get_meta('Mode of Payment').fields[4].options\n\t\tmop_str = re.sub('\\n', ',', mop_options)\n\t\tmop_dict = dict(item.split(\"-\") for item in mop_str.split(\",\"))\n\n\t\twith zipfile.ZipFile(get_full_path(self.zip_file)) as zf:\n\t\t\tfile_count = 0\n\t\t\tfor file_name in zf.namelist():\n\t\t\t\titems = []\n\t\t\t\ttaxes = []\n\t\t\t\tterms = []\n\t\t\t\tencoded_content = zf.read(file_name)\n\t\t\t\ttry:\n\t\t\t\t\tcontent = encoded_content.decode(\"utf-8-sig\")\n\t\t\t\texcept UnicodeDecodeError:\n\t\t\t\t\tcontent = encoded_content.decode(\"utf-16\")\n\t\t\t\tfile_content = bs(content, \"xml\")\n\n\t\t\t\tfor line in file_content.find_all(\"DatiTrasmissione\"):\n\t\t\t\t\tdestination_code = line.CodiceDestinatario.text\n\n\t\t\t\tfor line in file_content.find_all(\"DatiGeneraliDocumento\"):\n\t\t\t\t\tdocument_type = line.TipoDocumento.text\n\t\t\t\t\tbill_date = dateutil.parser.parse(line.Data.text).strftime(\"%Y-%m-%d\")\n\t\t\t\t\tinvoice_no = line.Numero.text\n\t\t\t\t\tif len(invoice_no) != 0:\n\t\t\t\t\t\tfor line in file_content.find_all(\"CedentePrestatore\"):\n\t\t\t\t\t\t\ttax_id = line.DatiAnagrafici.IdPaese.text + line.DatiAnagrafici.IdCodice.text\n\t\t\t\t\t\t\tif line.find(\"CodiceFiscale\"):\n\t\t\t\t\t\t\t\tfiscal_code = line.DatiAnagrafici.CodiceFiscale.text\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tfiscal_code = \"\"\n\t\t\t\t\t\t\tif line.find(\"RegimeFiscale\"):\n\t\t\t\t\t\t\t\tfiscal_regime = line.DatiAnagrafici.RegimeFiscale.text\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tfiscal_regime = \"\"\n\t\t\t\t\t\t\tif line.find(\"Denominazione\"):\n\t\t\t\t\t\t\t\tsupplier = line.DatiAnagrafici.Anagrafica.Denominazione.text\n\t\t\t\t\t\t\tif line.find(\"Nome\"):\n\t\t\t\t\t\t\t\tsupplier = line.DatiAnagrafici.Anagrafica.Nome.text + \" \" + line.DatiAnagrafici.Anagrafica.Cognome.text\n\t\t\t\t\t\t\taddress_line1 = line.Sede.Indirizzo.text\n\t\t\t\t\t\t\tcity = line.Sede.Comune.text\n\t\t\t\t\t\t\tif line.find(\"Provincia\"):\n\t\t\t\t\t\t\t\tprovince = line.Sede.Provincia.text\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tprovince = \"\"\n\t\t\t\t\t\t\tpin_code = line.Sede.CAP.text\n\t\t\t\t\t\t\tcountry = get_country(line.Sede.Nazione.text)\n\n\t\t\t\t\t\tfor line in file_content.find_all(\"DettaglioLinee\"):\n\t\t\t\t\t\t\tif line.find(\"PrezzoUnitario\") and line.find(\"PrezzoTotale\"):\n\t\t\t\t\t\t\t\tunit_rate = float(line.PrezzoUnitario.text) or float(0)\n\t\t\t\t\t\t\t\tline_total = float(line.PrezzoTotale.text) or float(0)\n\n\t\t\t\t\t\t\t\tif (unit_rate == 0.0):\n\t\t\t\t\t\t\t\t\tqty = float(1)\n\t\t\t\t\t\t\t\t\tuom = \"nos\"\n\t\t\t\t\t\t\t\t\trate = tax_rate = 0\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\tif (line_total / unit_rate) == 1.0:\n\t\t\t\t\t\t\t\t\t\tqty = float(1)\n\t\t\t\t\t\t\t\t\t\tuom = \"nos\"\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\tif line.find(\"Quantita\"):\n\t\t\t\t\t\t\t\t\t\t\tqty = float(line.Quantita.text) or float(0)\n\t\t\t\t\t\t\t\t\t\t\tif line.find(\"UnitaMisura\"):\n\t\t\t\t\t\t\t\t\t\t\t\tuom = create_uom(line.UnitaMisura.text)\n\t\t\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\t\tuom = \"nos\"\n\n\t\t\t\t\t\t\t\t\tif (unit_rate < 0 and line_total < 0):\n\t\t\t\t\t\t\t\t\t\tqty *= -1\n\t\t\t\t\t\t\t\t\t\treturn_invoice = 1\n\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\treturn_invoice = 0\n\n\t\t\t\t\t\t\t\t\trate = unit_rate\n\t\t\t\t\t\t\t\t\tif line.find(\"AliquotaIVA\"):\n\t\t\t\t\t\t\t\t\t\ttax_rate = float(line.AliquotaIVA.text)\n\n\t\t\t\t\t\t\t\tline_str = re.sub('[^A-Za-z0-9]+', '-', line.Descrizione.text)\n\t\t\t\t\t\t\t\titem_name = line_str[0:140]\n\t\t\t\t\t\t\t\titems.append({\n\t\t\t\t\t\t\t\t\t\t\t\t\"item_code\": self.item_code,\n\t\t\t\t\t\t\t\t\t\t\t\t\"item_name\": item_name,\n\t\t\t\t\t\t\t\t\t\t\t\t\"description\": line_str,\n\t\t\t\t\t\t\t\t\t\t\t\t\"qty\": float(qty),\n\t\t\t\t\t\t\t\t\t\t\t\t\"uom\": uom,\n\t\t\t\t\t\t\t\t\t\t\t\t\"rate\": rate,\n\t\t\t\t\t\t\t\t\t\t\t\t\"conversion_factor\": float(1),\n\t\t\t\t\t\t\t\t\t\t\t\t\"tax_rate\": tax_rate\n\t\t\t\t\t\t\t\t\t\t\t})\n\n\t\t\t\t\t\tfor line in file_content.find_all(\"DatiRiepilogo\"):\n\t\t\t\t\t\t\tif line.find(\"AliquotaIVA\"):\n\t\t\t\t\t\t\t\tif line.find(\"EsigibilitaIVA\"):\n\t\t\t\t\t\t\t\t\tdescr = line.EsigibilitaIVA.text\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\tdescr = \"None\"\n\t\t\t\t\t\t\t\ttaxes.append({\n\t\t\t\t\t\t\t\t\t\"charge_type\": \"Actual\",\n\t\t\t\t\t\t\t\t\t\"account_head\": self.tax_account,\n\t\t\t\t\t\t\t\t\t\"tax_rate\": float(line.AliquotaIVA.text) or float(0),\n\t\t\t\t\t\t\t\t\t\"description\": descr,\n\t\t\t\t\t\t\t\t\t\"tax_amount\": float(line.Imposta.text) if len(line.find(\"Imposta\"))!=0 else float(0)\n\t\t\t\t\t\t\t\t})\n\t\t\t\t\n\t\t\t\t\t\tfor line in file_content.find_all(\"DettaglioPagamento\"):\n\t\t\t\t\t\t\tmop_code = line.ModalitaPagamento.text + '-' + mop_dict.get(line.ModalitaPagamento.text)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif line.find(\"DataScadenzaPagamento\"):\n\t\t\t\t\t\t\t\tdue_date = dateutil.parser.parse(line.DataScadenzaPagamento.text).strftime(\"%Y-%m-%d\")\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tdue_date = today()\n\t\t\t\t\t\t\tterms.append({\n\t\t\t\t\t\t\t\t\t\t\t\"mode_of_payment_code\": mop_code,\n\t\t\t\t\t\t\t\t\t\t\t\"bank_account_iban\": line.IBAN.text if line.find(\"IBAN\") else \"\",\n\t\t\t\t\t\t\t\t\t\t\t\"due_date\": due_date,\n\t\t\t\t\t\t\t\t\t\t\t\"payment_amount\": line.ImportoPagamento.text\n\t\t\t\t\t\t\t})\n\n\t\t\t\t\t\tsupplier_name = create_supplier(supplier = supplier, supplier_group = self.supplier_group, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttax_id = tax_id, fiscal_code = fiscal_code,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tfiscal_regime = fiscal_regime)\n\n\t\t\t\t\t\taddress = create_address(supplier_name = supplier_name, address_line1 = address_line1, \n\t\t\t\t\t\t\t\t\t\t\t\tcity = city, province = province, \n\t\t\t\t\t\t\t\t\t\t\t\tpin_code = pin_code, country = country)\n\n\t\t\t\t\t\tpi_name = create_purchase_invoice(company = self.company,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tsupplier_name = supplier_name,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tbill_no = invoice_no,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdocument_type = document_type,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tbill_date = bill_date,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tis_return = return_invoice,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdestination_code = destination_code,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\titems = items,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttaxes = taxes,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tterms = terms,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tfile_name = file_name)\n\t\t\t\t\t\tfile_count += 1\n\t\t\t\t\t\tif pi_name:\n\t\t\t\t\t\t\tpi_count += 1\n\t\t\t\t\t\t\tfile_save = save_file(file_name, encoded_content, \"Purchase Invoice\", pi_name, folder=None, decode=False, is_private=0, df=None)\n\n\t\tif pi_count == file_count:\n\t\t\tself.status = \"File Import Completed\"\n\t\t\tself.publish(\"File Import\", _(\"XML Files Processed\"), 2, 3)\n\t\telse:\n\t\t\tself.status = \"Partially Completed - Check Error Log\"\n\t\t\tself.publish(\"File Import\", _(\"XML Files Processed\"), 2, 3)\n\n\t\tself.save()\n\t\tself.publish(\"File Import\", _(\"XML Files Processed\"), 3, 3)"}
{"input": "def test_correct_dc_rack_in_nodetool_info(self):\n        \"\"\"\n        @jira_ticket CASSANDRA-10382\n\n        Test that nodetool info returns the correct rack and dc\n        \"\"\"\n\n        cluster = self.cluster\n        cluster.populate([2, 2])\n        cluster.set_configuration_options(values={'endpoint_snitch': 'org.apache.cassandra.locator.GossipingPropertyFileSnitch'})\n\n        for i, node in enumerate(cluster.nodelist()):\n            with open(os.path.join(node.get_conf_dir(), 'cassandra-rackdc.properties'), 'w') as snitch_file:\n                for line in [\"dc={}\".format(node.data_center), \"rack=rack{}\".format(i % 2)]:\n                    snitch_file.write(line + os.linesep)\n\n        cluster.start(wait_for_binary_proto='True')\n\n        for i, node in enumerate(cluster.nodelist()):\n            out, err = node.nodetool('info')\n            self.assertEqual(0, len(err), err)\n            debug(out)\n            for line in out.split(os.linesep):\n                if line.startswith('Data Center'):\n                    self.assertTrue(line.endswith(node.data_center),\n                                    \"Expected dc {} for {} but got {}\".format(node.data_center, node.address(), line.rsplit(None, 1)[-1]))\n                elif line.startswith('Rack'):\n                    rack = \"rack{}\".format(i % 2)\n                    self.assertTrue(line.endswith(rack),\n                                    \"Expected rack {} for {} but got {}\".format(rack, node.address(), line.rsplit(None, 1)[-1]))", "output": "def test_correct_dc_rack_in_nodetool_info(self):\n        \"\"\"\n        @jira_ticket CASSANDRA-10382\n\n        Test that nodetool info returns the correct rack and dc\n        \"\"\"\n\n        cluster = self.cluster\n        cluster.populate([2, 2])\n        cluster.set_configuration_options(values={'endpoint_snitch': 'org.apache.cassandra.locator.GossipingPropertyFileSnitch'})\n\n        for i, node in enumerate(cluster.nodelist()):\n            with open(os.path.join(node.get_conf_dir(), 'cassandra-rackdc.properties'), 'w') as snitch_file:\n                for line in [\"dc={}\".format(node.data_center), \"rack=rack{}\".format(i % 2)]:\n                    snitch_file.write(line + os.linesep)\n\n        cluster.start(wait_for_binary_proto=True)\n\n        for i, node in enumerate(cluster.nodelist()):\n            out, err = node.nodetool('info')\n            self.assertEqual(0, len(err), err)\n            debug(out)\n            for line in out.split(os.linesep):\n                if line.startswith('Data Center'):\n                    self.assertTrue(line.endswith(node.data_center),\n                                    \"Expected dc {} for {} but got {}\".format(node.data_center, node.address(), line.rsplit(None, 1)[-1]))\n                elif line.startswith('Rack'):\n                    rack = \"rack{}\".format(i % 2)\n                    self.assertTrue(line.endswith(rack),\n                                    \"Expected rack {} for {} but got {}\".format(rack, node.address(), line.rsplit(None, 1)[-1]))"}
{"input": "def _get_group_list(request, project):\n    query_kwargs = {\n        'project': project,\n    }\n\n    status = request.GET.get('status', 0)\n    if status:\n        query_kwargs['status'] = int(status)\n\n    if request.user.is_authenticated() and request.GET.get('bookmarks'):\n        query_kwargs['bookmarked_by'] = request.user\n\n    if request.user.is_authenticated() and request.GET.get('assigned'):\n        query_kwargs['assigned_to'] = request.user\n\n    sort_by = request.GET.get('sort') or request.session.get('streamsort')\n    if sort_by is None:\n        sort_by = DEFAULT_SORT_OPTION\n\n    # Save last sort in session\n    if sort_by != request.session.get('streamsort'):\n        request.session['streamsort'] = sort_by\n\n    query_kwargs['sort_by'] = sort_by\n\n    tags = {}\n    for tag_key in TagKey.objects.all_keys(project):\n        if request.GET.get(tag_key):\n            tags[tag_key] = request.GET[tag_key]\n    if tags:\n        query_kwargs['tags'] = tags\n\n    date_from = request.GET.get('df')\n    time_from = request.GET.get('tf')\n    date_to = request.GET.get('dt')\n    time_to = request.GET.get('tt')\n    date_filter = request.GET.get('date_type')\n\n    today = timezone.now()\n    # date format is Y-m-d\n    if any(x is not None for x in [date_from, time_from, date_to, time_to]):\n        date_from, date_to = parse_date(date_from, time_from), parse_date(date_to, time_to)\n    else:\n        date_from = today - datetime.timedelta(days=5)\n        date_to = None\n\n    query_kwargs['date_from'] = date_from\n    query_kwargs['date_to'] = date_to\n    if date_filter:\n        query_kwargs['date_filter'] = date_filter\n\n    cursor = request.GET.get('cursor')\n    if cursor:\n        try:\n            query_kwargs['cursor'] = Cursor.from_string(cursor)\n        except ValueError:\n            # XXX(dcramer): ideally we'd error, but this is an internal API so\n            # we'd rather just throw it away\n            logging.warn('Throwing away invalid cursor: %s', cursor)\n    query_kwargs['limit'] = EVENTS_PER_PAGE\n\n    query = request.GET.get('query', '')\n    if query is not None:\n        query_kwargs.update(parse_query(query, request.user))\n\n    results = app.search.query(**query_kwargs)\n\n    return {\n        'event_list': results[:EVENTS_PER_PAGE],\n        'date_from': date_from,\n        'date_to': date_to,\n        'today': today,\n        'sort': sort_by,\n        'date_type': date_filter,\n        'next_cursor': results.next,\n        'prev_cursor': results.prev,\n    }", "output": "def _get_group_list(request, project):\n    query_kwargs = {\n        'project': project,\n    }\n\n    status = request.GET.get('status', '0')\n    if status:\n        query_kwargs['status'] = int(status)\n\n    if request.user.is_authenticated() and request.GET.get('bookmarks'):\n        query_kwargs['bookmarked_by'] = request.user\n\n    if request.user.is_authenticated() and request.GET.get('assigned'):\n        query_kwargs['assigned_to'] = request.user\n\n    sort_by = request.GET.get('sort') or request.session.get('streamsort')\n    if sort_by is None:\n        sort_by = DEFAULT_SORT_OPTION\n\n    # Save last sort in session\n    if sort_by != request.session.get('streamsort'):\n        request.session['streamsort'] = sort_by\n\n    query_kwargs['sort_by'] = sort_by\n\n    tags = {}\n    for tag_key in TagKey.objects.all_keys(project):\n        if request.GET.get(tag_key):\n            tags[tag_key] = request.GET[tag_key]\n    if tags:\n        query_kwargs['tags'] = tags\n\n    date_from = request.GET.get('df')\n    time_from = request.GET.get('tf')\n    date_to = request.GET.get('dt')\n    time_to = request.GET.get('tt')\n    date_filter = request.GET.get('date_type')\n\n    today = timezone.now()\n    # date format is Y-m-d\n    if any(x is not None for x in [date_from, time_from, date_to, time_to]):\n        date_from, date_to = parse_date(date_from, time_from), parse_date(date_to, time_to)\n    else:\n        date_from = today - datetime.timedelta(days=5)\n        date_to = None\n\n    query_kwargs['date_from'] = date_from\n    query_kwargs['date_to'] = date_to\n    if date_filter:\n        query_kwargs['date_filter'] = date_filter\n\n    cursor = request.GET.get('cursor')\n    if cursor:\n        try:\n            query_kwargs['cursor'] = Cursor.from_string(cursor)\n        except ValueError:\n            # XXX(dcramer): ideally we'd error, but this is an internal API so\n            # we'd rather just throw it away\n            logging.warn('Throwing away invalid cursor: %s', cursor)\n    query_kwargs['limit'] = EVENTS_PER_PAGE\n\n    query = request.GET.get('query', '')\n    if query is not None:\n        query_kwargs.update(parse_query(query, request.user))\n\n    results = app.search.query(**query_kwargs)\n\n    return {\n        'event_list': results[:EVENTS_PER_PAGE],\n        'date_from': date_from,\n        'date_to': date_to,\n        'today': today,\n        'sort': sort_by,\n        'date_type': date_filter,\n        'next_cursor': results.next,\n        'prev_cursor': results.prev,\n    }"}
{"input": "def halftone_cmyk_image(image, pitch, angles=(15, 75, 30, 45), scale=1.0):\n\tc, m, y, k = image.split()\n\tcyan = halftone_image(c, pitch, angles[0], scale)\n\tmagenta = halftone_image(m, pitch, angles[1], scale)\n\tyellow = halftone_image(y, pitch, angles[2], scale)\n\tkey = halftone_image(k, pitch, angles[3], scale)\n\treturn Image.merge(\"CMYK\", [cyan, magenta, yellow, key])", "output": "def halftone_cmyk_image(image, pitch, angles=(15, 75, 30, 45), scale=1):\n\tc, m, y, k = image.split()\n\tcyan = halftone_image(c, pitch, angles[0], scale)\n\tmagenta = halftone_image(m, pitch, angles[1], scale)\n\tyellow = halftone_image(y, pitch, angles[2], scale)\n\tkey = halftone_image(k, pitch, angles[3], scale)\n\treturn Image.merge(\"CMYK\", [cyan, magenta, yellow, key])"}
{"input": "def _make_config(self, checkdomains=None, virusname='UNITTEST-SPEARPHISH', virusaction='REJECT', virusenginename='UNIITEST Spearphishing protection', rejectmessage='threat detected: ${virusname}', check_display_part=True ):\n        config = RawConfigParser()\n        config.add_section('SpearPhishPlugin')\n\n        if checkdomains:\n            tempfilename = tempfile.mktemp(\n                suffix='spearphish', prefix='fuglu-unittest', dir='/tmp')\n            fp = open(tempfilename, 'w')\n            fp.write('\\n'.join(checkdomains))\n            self.tempfiles.append(tempfilename)\n            config.set('SpearPhishPlugin', 'domainsfile', tempfilename)\n        else:\n            config.set('SpearPhishPlugin', 'domainsfile', '')\n        config.set('SpearPhishPlugin', 'virusname', virusname)\n        config.set('SpearPhishPlugin', 'virusaction', virusaction)\n        config.set('SpearPhishPlugin', 'virusenginename', virusenginename)\n        config.set('SpearPhishPlugin', 'rejectmessage', rejectmessage)\n        config.set('SpearPhishPlugin', 'check_display_part', check_display_part)\n        return config", "output": "def _make_config(self, checkdomains=None, virusname='UNITTEST-SPEARPHISH', virusaction='REJECT', virusenginename='UNIITEST Spearphishing protection', rejectmessage='threat detected: ${virusname}', check_display_part='True' ):\n        config = RawConfigParser()\n        config.add_section('SpearPhishPlugin')\n\n        if checkdomains:\n            tempfilename = tempfile.mktemp(\n                suffix='spearphish', prefix='fuglu-unittest', dir='/tmp')\n            fp = open(tempfilename, 'w')\n            fp.write('\\n'.join(checkdomains))\n            self.tempfiles.append(tempfilename)\n            config.set('SpearPhishPlugin', 'domainsfile', tempfilename)\n        else:\n            config.set('SpearPhishPlugin', 'domainsfile', '')\n        config.set('SpearPhishPlugin', 'virusname', virusname)\n        config.set('SpearPhishPlugin', 'virusaction', virusaction)\n        config.set('SpearPhishPlugin', 'virusenginename', virusenginename)\n        config.set('SpearPhishPlugin', 'rejectmessage', rejectmessage)\n        config.set('SpearPhishPlugin', 'check_display_part', check_display_part)\n        return config"}
{"input": "def __init__(self, scrollbars):\n        Gtk.DrawingArea.__init__(self)\n\n        hadj = scrollbars.get_hadjustment()\n        vadj = scrollbars.get_vadjustment()\n\n        self.size_forced = False\n        self.full_size = (1, 1)\n        self.visible_size = (1, 1)\n\n        self.drawers = PriorityQueue()\n\n        self.set_hadjustment(hadj)\n        self.set_vadjustment(vadj)\n\n        self.add_events(Gdk.EventMask.SCROLL_MASK)\n        self.add_events(Gdk.EventMask.BUTTON_PRESS_MASK)\n        self.add_events(Gdk.EventMask.BUTTON_RELEASE_MASK)\n        self.add_events(Gdk.EventMask.POINTER_MOTION_MASK)\n        self.connect(\"size-allocate\", self.__on_size_allocate)\n        self.connect(\"draw\", self.__on_draw)\n        self.connect(\"scroll-event\", self.__on_scroll_event)\n        self.connect(\"button-press-event\", self.__on_button_pressed)\n        self.connect(\"motion-notify-event\", self.__on_motion)\n        self.connect(\"button-release-event\", self.__on_button_released)\n\n        self.set_size_request(-1, -1)\n\n        GLib.timeout_add(1000.0 / 30, self._tick)", "output": "def __init__(self, scrollbars):\n        Gtk.DrawingArea.__init__(self)\n\n        hadj = scrollbars.get_hadjustment()\n        vadj = scrollbars.get_vadjustment()\n\n        self.size_forced = False\n        self.full_size = (1, 1)\n        self.visible_size = (1, 1)\n\n        self.drawers = PriorityQueue()\n\n        self.set_hadjustment(hadj)\n        self.set_vadjustment(vadj)\n\n        self.add_events(Gdk.EventMask.SCROLL_MASK)\n        self.add_events(Gdk.EventMask.BUTTON_PRESS_MASK)\n        self.add_events(Gdk.EventMask.BUTTON_RELEASE_MASK)\n        self.add_events(Gdk.EventMask.POINTER_MOTION_MASK)\n        self.connect(\"size-allocate\", self.__on_size_allocate)\n        self.connect(\"draw\", self.__on_draw)\n        self.connect(\"scroll-event\", self.__on_scroll_event)\n        self.connect(\"button-press-event\", self.__on_button_pressed)\n        self.connect(\"motion-notify-event\", self.__on_motion)\n        self.connect(\"button-release-event\", self.__on_button_released)\n\n        self.set_size_request(-1, -1)\n\n        GLib.timeout_add(1000 / 30, self._tick)"}
{"input": "def __new__(cls, public_key, private_key=\"\"):\n        assert isinstance(public_key, str)\n        assert isinstance(private_key, str)\n        assert ec_check_public_bin(public_key), [len(public_key), public_key.encode(\"HEX\")]\n        assert private_key == \"\" or ec_check_private_bin(private_key), [len(private_key), private_key.encode(\"HEX\")]\n\n        # retrieve Member from cache\n        for index, member in enumerate(cls.__cache):\n            if member._public_key == public_key:\n                if cls.__cache_length / 2 < index:\n                    del cls.__cache[index]\n                    cls.__cache.insert(0, member)\n\n                if __debug__:\n                    cls.__stats_hit += 1\n                    if cls.__cache_length / 2 < index:\n                        cls.__stats_move += 1\n                    else:\n                        cls.__stats_prio_hit += 1\n\n                return member\n\n        # create new Member and store in cache\n        member = object.__new__(cls, public_key, private_key)\n        if len(cls.__cache) >= cls.__cache_length:\n            del cls.__cache[-1]\n        cls.__cache.insert(cls.__cache_length / 3, member)\n\n        if __debug__:\n            # stats prio hit: 3689 hit: 4715 miss: 1325 move: 1026 del: 1069                [insert at length/2]\n            # stats prio hit: 4332 hit: 4529 miss: 1325 move:  197 del: 1069 cache: 256     [insert at length/3]\n            # stats prio hit:119684 hit:125291 miss:33901 move:5607 del:33646 cache:256     [long run]\n            # stats prio hit:142272 hit:147010 miss:40155 move:4738 del:39644 cache:512\n            cls.__stats_miss += 1\n            if len(cls.__cache) == cls.__cache_length:\n                cls.__stats_del += 1\n            dprint(\"stats prio hit:\", cls.__stats_prio_hit, \" hit:\", cls.__stats_hit, \" miss:\", cls.__stats_miss, \" move:\", cls.__stats_move, \" del:\", cls.__stats_del, \" cache:\", len(cls.__cache))\n\n        return member", "output": "def __new__(cls, public_key, private_key=\"\"):\n        assert isinstance(public_key, str)\n        assert isinstance(private_key, str)\n        assert ec_check_public_bin(public_key), [len(public_key), public_key.encode(\"HEX\")]\n        assert private_key == \"\" or ec_check_private_bin(private_key), [len(private_key), private_key.encode(\"HEX\")]\n\n        # retrieve Member from cache\n        for index, member in enumerate(cls.__cache):\n            if member._public_key == public_key:\n                if cls.__cache_length / 2 < index:\n                    del cls.__cache[index]\n                    cls.__cache.insert(0, member)\n\n                if __debug__:\n                    cls.__stats_hit += 1\n                    if cls.__cache_length / 2 < index:\n                        cls.__stats_move += 1\n                    else:\n                        cls.__stats_prio_hit += 1\n\n                return member\n\n        # create new Member and store in cache\n        member = object.__new__(cls)\n        if len(cls.__cache) >= cls.__cache_length:\n            del cls.__cache[-1]\n        cls.__cache.insert(cls.__cache_length / 3, member)\n\n        if __debug__:\n            # stats prio hit: 3689 hit: 4715 miss: 1325 move: 1026 del: 1069                [insert at length/2]\n            # stats prio hit: 4332 hit: 4529 miss: 1325 move:  197 del: 1069 cache: 256     [insert at length/3]\n            # stats prio hit:119684 hit:125291 miss:33901 move:5607 del:33646 cache:256     [long run]\n            # stats prio hit:142272 hit:147010 miss:40155 move:4738 del:39644 cache:512\n            cls.__stats_miss += 1\n            if len(cls.__cache) == cls.__cache_length:\n                cls.__stats_del += 1\n            dprint(\"stats prio hit:\", cls.__stats_prio_hit, \" hit:\", cls.__stats_hit, \" miss:\", cls.__stats_miss, \" move:\", cls.__stats_move, \" del:\", cls.__stats_del, \" cache:\", len(cls.__cache))\n\n        return member"}
{"input": "def get_node(self, path: str, create: bool = False):\n        \"\"\" \n        Get node object from specified path\n        \n        :param path: Path as string\n        :param create: If True, not existing nodes will be created\n        \"\"\"\n        assert isinstance(path, str), \"Node path must be a string type !\"\n\n        node = self.root\n        path = path.lstrip('/')\n        if path:\n            names = path.split('/')\n            for name in names:\n                item = node.get_subnode(name)\n                if item is None:\n                    if create:\n                        item = Node(name)\n                        node.append(item)\n                    else:\n                        raise ValueError(\"Path \\\"{}\\\" doesn't exists\".format(self, path))\n                node = item\n\n        return node", "output": "def get_node(self, path: str, create: bool = False):\n        \"\"\" \n        Get node object from specified path\n        \n        :param path: Path as string\n        :param create: If True, not existing nodes will be created\n        \"\"\"\n        assert isinstance(path, str), \"Node path must be a string type !\"\n\n        node = self.root\n        path = path.lstrip('/')\n        if path:\n            names = path.split('/')\n            for name in names:\n                item = node.get_subnode(name)\n                if item is None:\n                    if create:\n                        item = Node(name)\n                        node.append(item)\n                    else:\n                        raise ValueError(\"Path \\\"{}\\\" doesn't exists\".format(path))\n                node = item\n\n        return node"}
{"input": "def model_config(parser):\n    parser.add_argument('--word_dim', default=300, type=int)\n    parser.add_argument('--d_e', default=300, type=int)\n    parser.add_argument('--num_heads', default=5, type=int)\n    parser.add_argument('--d_ff', default=300 * 4, type=int)\n\n    parser.add_argument('--dist_mask', default=False, action='store_true')\n    parser.add_argument('--alpha', default=1.5, type=float)\n    parser.add_argument('--seg_emb', default=False, action='store_true')\n\n    parser.add_argument('--elmo_num', type=int, default=1)\n    parser.add_argument('--no_elmo_feed_forward', dest='elmo_feed_forward', default=True, action='store_false')\n    parser.add_argument('--elmo_dim', type=int, default=1024)\n\n    return parser", "output": "def model_config(parser):\n    parser.add_argument('--word_dim', default=300, type=int)\n    parser.add_argument('--d_e', default=300, type=int)\n    parser.add_argument('--num_heads', default=5, type=int)\n    parser.add_argument('--d_ff', default=300 * 4, type=int)\n\n    parser.add_argument('--dist_mask', default=False, action='store_true')\n    parser.add_argument('--alpha', default=1.5, type=float)\n    parser.add_argument('--seg_emb', default=False, action='store_true')\n\n    parser.add_argument('--elmo_num', type=int, default=1)\n    parser.add_argument('--no_elmo_feed_forward', dest='elmo_feed_forward', action='store_false')\n    parser.add_argument('--elmo_dim', type=int, default=1024)\n\n    return parser"}
{"input": "def string_references(self, minimum_length=1):\n        \"\"\"\n        ALl of the constant string reference used by this function\n        :param minimum_length: the minimum length of strings to find (default is 1)\n        :return: a list of tuples of (address, string) where is address is the location of the string in memory\n        \"\"\"\n        strings = []\n        memory = self._function_manager._project.ld.memory\n\n        # get known instruction addresses and call targets\n        # these addresses cannot be string references, but show up frequently in the runtime values\n        known_executable_addresses = set()\n        for b in self.basic_blocks:\n            if b in memory:\n                sirsb = self._function_manager._project.block(b)\n                known_executable_addresses.update(sirsb.instruction_addrs())\n        for node in self._function_manager._cfg.nodes():\n            known_executable_addresses.add(node.addr)\n\n        # loop over all local runtime values and check if the value points to a printable string\n        for addr in self.partial_local_runtime_values:\n            if addr in memory:\n                # check that the address isn't an pointing to known executable code\n                # and that it isn't an indirect pointer to known executable code\n                possible_pointer = memory.read_addr_at(addr, self._function_manager._project.ld.main_bin.archinfo)\n                if addr not in known_executable_addresses and possible_pointer not in known_executable_addresses:\n                    # build string\n                    str = \"\"\n                    offset = 0\n                    current_char = memory[addr + offset]\n                    while current_char in string.printable:\n                        str += current_char\n                        offset += 1\n                        current_char = memory[addr + offset]\n\n                    # check that the string was a null terminated string with minimum length\n                    if current_char == \"\\x00\" and len(str) >= minimum_length:\n                        strings.append((addr, str))\n\n        return strings", "output": "def string_references(self, minimum_length=1):\n        \"\"\"\n        ALl of the constant string reference used by this function\n        :param minimum_length: the minimum length of strings to find (default is 1)\n        :return: a list of tuples of (address, string) where is address is the location of the string in memory\n        \"\"\"\n        strings = []\n        memory = self._function_manager._project.ld.memory\n\n        # get known instruction addresses and call targets\n        # these addresses cannot be string references, but show up frequently in the runtime values\n        known_executable_addresses = set()\n        for b in self.basic_blocks:\n            if b in memory:\n                sirsb = self._function_manager._project.block(b)\n                known_executable_addresses.update(sirsb.instruction_addrs())\n        for node in self._function_manager._cfg.nodes():\n            known_executable_addresses.add(node.addr)\n\n        # loop over all local runtime values and check if the value points to a printable string\n        for addr in self.partial_local_runtime_values:\n            if addr in memory:\n                # check that the address isn't an pointing to known executable code\n                # and that it isn't an indirect pointer to known executable code\n                possible_pointer = memory.read_addr_at(addr)\n                if addr not in known_executable_addresses and possible_pointer not in known_executable_addresses:\n                    # build string\n                    str = \"\"\n                    offset = 0\n                    current_char = memory[addr + offset]\n                    while current_char in string.printable:\n                        str += current_char\n                        offset += 1\n                        current_char = memory[addr + offset]\n\n                    # check that the string was a null terminated string with minimum length\n                    if current_char == \"\\x00\" and len(str) >= minimum_length:\n                        strings.append((addr, str))\n\n        return strings"}
{"input": "def get_incoming_rate(self, posting_date, posting_time, item, warehouse, qty = 0, serial_no = ''):\n\t\t\"\"\"Get Incoming Rate based on valuation method\"\"\"\n\t\tin_rate = 0\n\t\tval_method = self.get_valuation_method(item)\n\t\tbin_obj = get_obj('Warehouse',warehouse).get_bin(item)\n\t\tif serial_no:\n\t\t\tin_rate = self.get_serializable_inventory_rate(serial_no)\n\t\telif val_method == 'FIFO':\n\t\t\t# get rate based on the last item value?\n\t\t\tif qty:\n\t\t\t\tprev_sle = bin_obj.get_prev_sle(posting_date, posting_time)\n\t\t\t\tif not prev_sle:\n\t\t\t\t\treturn 0.0\n\t\t\t\tfcfs_stack = eval(prev_sle.get('fcfs_stack', '[]'))\n\t\t\t\tin_rate = fcfs_stack and self.get_fifo_rate(fcfs_stack, qty) or 0\n\t\telif val_method == 'Moving Average':\n\t\t\tprev_sle = bin_obj.get_prev_sle(posting_date, posting_time)\n\t\t\tin_rate = prev_sle and prev_sle.get('valuation_rate', 0) or 0\n\t\treturn in_rate", "output": "def get_incoming_rate(self, posting_date, posting_time, item, warehouse, qty = 0, serial_no = ''):\n\t\t\"\"\"Get Incoming Rate based on valuation method\"\"\"\n\t\tin_rate = 0\n\t\tval_method = self.get_valuation_method(item)\n\t\tbin_obj = get_obj('Warehouse',warehouse).get_bin(item)\n\t\tif serial_no:\n\t\t\tin_rate = self.get_serializable_inventory_rate(serial_no)\n\t\telif val_method == 'FIFO':\n\t\t\t# get rate based on the last item value?\n\t\t\tif qty:\n\t\t\t\tprev_sle = bin_obj.get_prev_sle(posting_date, posting_time)\n\t\t\t\tif not prev_sle:\n\t\t\t\t\treturn 0.0\n\t\t\t\tfcfs_stack = eval(prev_sle.get('fcfs_stack', '[]'))\n\t\t\t\tin_rate = fcfs_stack and self.get_fifo_rate(fcfs_stack) or 0\n\t\telif val_method == 'Moving Average':\n\t\t\tprev_sle = bin_obj.get_prev_sle(posting_date, posting_time)\n\t\t\tin_rate = prev_sle and prev_sle.get('valuation_rate', 0) or 0\n\t\treturn in_rate"}
{"input": "def from_bitstring(cls, bitstring, level=-1, **kwargs):\n        \"\"\"Initialize from bitstring (e.g. '10010011').\n\n        Parameters\n        ----------\n        bitstring : str\n            String of 1s and 0s.\n        level ; int, optional (default -1)\n            Level of fingerprint. 0th level just uses initial atom\n            identifiers, 1st level is after 1st iteration, `n`th level is\n            after `n` iterations.\n\n        Returns\n        -------\n        Fingerprint : fingerprint\n        \"\"\"\n        indices = [i for i, char in enumerate(bitstring) if char != 0]\n        if kwargs.get(\"bits\", None) is None:\n            kwargs[\"bits\"] = len(bitstring)\n        return cls.from_indices(indices, level=level, **kwargs)", "output": "def from_bitstring(cls, bitstring, level=-1, **kwargs):\n        \"\"\"Initialize from bitstring (e.g. '10010011').\n\n        Parameters\n        ----------\n        bitstring : str\n            String of 1s and 0s.\n        level ; int, optional (default -1)\n            Level of fingerprint. 0th level just uses initial atom\n            identifiers, 1st level is after 1st iteration, `n`th level is\n            after `n` iterations.\n\n        Returns\n        -------\n        Fingerprint : fingerprint\n        \"\"\"\n        indices = [i for i, char in enumerate(bitstring) if char != '0']\n        if kwargs.get(\"bits\", None) is None:\n            kwargs[\"bits\"] = len(bitstring)\n        return cls.from_indices(indices, level=level, **kwargs)"}
{"input": "def auth_put():\n    administrator = flask.g.administrator\n\n    if 'username' in flask.request.json and flask.request.json['username']:\n        administrator.username = utils.filter_str(\n            flask.request.json['username'])\n    if 'password' in flask.request.json and flask.request.json['password']:\n        administrator.password = flask.request.json['password']\n    if 'token' in flask.request.json and flask.request.json['token']:\n        administrator.generate_token()\n    if 'secret' in flask.request.json and flask.request.json['secret']:\n        administrator.generate_secret()\n\n    settings_commit = False\n    if 'email_from' in flask.request.json:\n        settings_commit = True\n        email_from = flask.request.json['email_from']\n        settings.app.email_from_addr = email_from or None\n    if 'email_api_key' in flask.request.json:\n        settings_commit = True\n        email_api_key = flask.request.json['email_api_key']\n        settings.app.email_api_key = email_api_key or None\n    if settings_commit:\n        settings.commit(settings.changed)\n\n    administrator.commit(administrator.changed)\n\n    response = flask.g.administrator.dict()\n    response.update({\n        'email_from': settings.app.email_from_addr,\n        'email_api_key': settings.app.email_api_key,\n    })\n    return utils.jsonify(response)", "output": "def auth_put():\n    administrator = flask.g.administrator\n\n    if 'username' in flask.request.json and flask.request.json['username']:\n        administrator.username = utils.filter_str(\n            flask.request.json['username'])\n    if 'password' in flask.request.json and flask.request.json['password']:\n        administrator.password = flask.request.json['password']\n    if 'token' in flask.request.json and flask.request.json['token']:\n        administrator.generate_token()\n    if 'secret' in flask.request.json and flask.request.json['secret']:\n        administrator.generate_secret()\n\n    settings_commit = False\n    if 'email_from' in flask.request.json:\n        settings_commit = True\n        email_from = flask.request.json['email_from']\n        settings.app.email_from_addr = email_from or None\n    if 'email_api_key' in flask.request.json:\n        settings_commit = True\n        email_api_key = flask.request.json['email_api_key']\n        settings.app.email_api_key = email_api_key or None\n    if settings_commit:\n        settings.commit()\n\n    administrator.commit(administrator.changed)\n\n    response = flask.g.administrator.dict()\n    response.update({\n        'email_from': settings.app.email_from_addr,\n        'email_api_key': settings.app.email_api_key,\n    })\n    return utils.jsonify(response)"}
{"input": "def addPackageChild(self, name, links, password, root, paused):\n        \"\"\"Adds a package, with links to desired package.\n\n        :param root: parents package id\n        :return: package id of the new package\n        \"\"\"\n        if self.core.config['general']['folder_per_package']:\n            folder = name\n        else:\n            folder = \"\"\n\n        pid = self.createPackage(name, folder, root, password, paused=paused)\n        self.addLinks(pid, links, paused)\n\n        return pid", "output": "def addPackageChild(self, name, links, password, root, paused):\n        \"\"\"Adds a package, with links to desired package.\n\n        :param root: parents package id\n        :return: package id of the new package\n        \"\"\"\n        if self.core.config['general']['folder_per_package']:\n            folder = name\n        else:\n            folder = \"\"\n\n        pid = self.createPackage(name, folder, root, password, paused=paused)\n        self.addLinks(pid, links)\n\n        return pid"}
{"input": "def test_empty_environment(self):\n        \"\"\"\n        When a container with no custom environment variables is launched via\n        ``DockerClient.add`` the environment in the resulting ``Unit`` returned\n        from ``DockerClient.list`` will ignore the default HOME and PATH\n        environment variables, leaving the ``Unit`` with an Environment of\n        None.\n        \"\"\"\n        name = random_name()\n        d = self.start_container(name, image_name=u\"busybox\")\n\n        def started(client):\n            deferred_units = client.list()\n\n            def check_units(units):\n                unit = [unit for unit in units if unit.name == name][0]\n                self.assertIsNone(unit.environment)\n\n            deferred_units.addCallback(check_units)\n        d.addCallback(started)\n        return d", "output": "def test_empty_environment(self):\n        \"\"\"\n        When a container with no custom environment variables is launched via\n        ``DockerClient.add`` the environment in the resulting ``Unit`` returned\n        from ``DockerClient.list`` will ignore the default HOME and PATH\n        environment variables, leaving the ``Unit`` with an Environment of\n        None.\n        \"\"\"\n        name = random_name()\n        d = self.start_container(name)\n\n        def started(client):\n            deferred_units = client.list()\n\n            def check_units(units):\n                unit = [unit for unit in units if unit.name == name][0]\n                self.assertIsNone(unit.environment)\n\n            deferred_units.addCallback(check_units)\n        d.addCallback(started)\n        return d"}
{"input": "def test_detector_angles():\n    #set a test date\n    date = parse_time('2012-02-15')\n    file = fermi.download_weekly_pointing_file(date)\n    det=fermi.get_detector_sun_angles_for_date(date,file,plot=False)\n    assert len(det) == 12\n    #assert type(det) == collections.OrderedDict\n    assert_almost_equal(det['n0'][0], 20.30309,decimal=1)\n    assert_almost_equal(det['n1'][0], 30.30430, decimal=1)\n    assert_almost_equal(det['n2'][0], 74.86032, decimal=1)\n    assert_almost_equal(det['n3'][0], 31.24400, decimal=1)\n    assert_almost_equal(det['n4'][0], 75.10403, decimal=1)\n    assert_almost_equal(det['n5'][0], 60.40967, decimal=1)\n    assert_almost_equal(det['n6'][0], 46.14087, decimal=1)\n    assert_almost_equal(det['n7'][0], 69.71780, decimal=1)\n    assert_almost_equal(det['n8'][0], 106.08064, decimal=1)\n    assert_almost_equal(det['n9'][0], 68.543067, decimal=1)\n    assert_almost_equal(det['n10'][0], 105.76825, decimal=1)\n    assert_almost_equal(det['n11'][0], 119.69057, decimal=1)\n\n    det2=fermi.get_detector_sun_angles_for_time(parse_time('2012-02-15 02:00'),file)\n    assert len(det2) == 12\n    assert type(det2) == dict\n    assert_almost_equal(det2['n0'], 87.24744,decimal=1)\n    assert_almost_equal(det2['n1'], 69.90883,decimal=1)\n    assert_almost_equal(det2['n10'], 123.56429,decimal=1)\n    assert_almost_equal(det2['n11'], 167.26615,decimal=1)\n    assert_almost_equal(det2['n2'], 59.82642,decimal=1)\n    assert_almost_equal(det2['n3'], 69.18959,decimal=1)\n    assert_almost_equal(det2['n4'], 56.83158,decimal=1)\n    assert_almost_equal(det2['n5'], 12.49959,decimal=1)\n    assert_almost_equal(det2['n6'], 115.31259,decimal=1)\n    assert_almost_equal(det2['n7'], 129.49283,decimal=1)\n    assert_almost_equal(det2['n8'], 121.91083,decimal=1)\n    assert_almost_equal(det2['n9'], 130.04144,decimal=1)", "output": "def test_detector_angles():\n    #set a test date\n    date = parse_time('2012-02-15')\n    file = fermi.download_weekly_pointing_file(date)\n    det=fermi.get_detector_sun_angles_for_date(date,file)\n    assert len(det) == 12\n    #assert type(det) == collections.OrderedDict\n    assert_almost_equal(det['n0'][0], 20.30309,decimal=1)\n    assert_almost_equal(det['n1'][0], 30.30430, decimal=1)\n    assert_almost_equal(det['n2'][0], 74.86032, decimal=1)\n    assert_almost_equal(det['n3'][0], 31.24400, decimal=1)\n    assert_almost_equal(det['n4'][0], 75.10403, decimal=1)\n    assert_almost_equal(det['n5'][0], 60.40967, decimal=1)\n    assert_almost_equal(det['n6'][0], 46.14087, decimal=1)\n    assert_almost_equal(det['n7'][0], 69.71780, decimal=1)\n    assert_almost_equal(det['n8'][0], 106.08064, decimal=1)\n    assert_almost_equal(det['n9'][0], 68.543067, decimal=1)\n    assert_almost_equal(det['n10'][0], 105.76825, decimal=1)\n    assert_almost_equal(det['n11'][0], 119.69057, decimal=1)\n\n    det2=fermi.get_detector_sun_angles_for_time(parse_time('2012-02-15 02:00'),file)\n    assert len(det2) == 12\n    assert type(det2) == dict\n    assert_almost_equal(det2['n0'], 87.24744,decimal=1)\n    assert_almost_equal(det2['n1'], 69.90883,decimal=1)\n    assert_almost_equal(det2['n10'], 123.56429,decimal=1)\n    assert_almost_equal(det2['n11'], 167.26615,decimal=1)\n    assert_almost_equal(det2['n2'], 59.82642,decimal=1)\n    assert_almost_equal(det2['n3'], 69.18959,decimal=1)\n    assert_almost_equal(det2['n4'], 56.83158,decimal=1)\n    assert_almost_equal(det2['n5'], 12.49959,decimal=1)\n    assert_almost_equal(det2['n6'], 115.31259,decimal=1)\n    assert_almost_equal(det2['n7'], 129.49283,decimal=1)\n    assert_almost_equal(det2['n8'], 121.91083,decimal=1)\n    assert_almost_equal(det2['n9'], 130.04144,decimal=1)"}
{"input": "def handle_irsb(self):\n\t\tif o.BREAK_SIRSB_START in self.options:\n\t\t\timport ipdb\n\t\t\tipdb.set_trace()\n\n\t\t# finish the initial setup\n\t\tself.prepare_temps(self.state)\n\n\t\t# handle the statements\n\t\ttry:\n\t\t\tself.handle_statements()\n\t\texcept s_exception.SimError:\n\t\t\tl.warning(\"A SimError was hit when analyzing statements. This may signify an unavoidable exit (ok) or an actual error (not ok)\", exc_info=True)\n\n\t\t# some finalization\n\t\tself.state.inplace_after()\n\t\tself.num_stmts = len(self.irsb.statements())\n\n\t\t# If there was an error, and not all the statements were processed,\n\t\t# then this block does not have a default exit. This can happen if\n\t\t# the block has an unavoidable \"conditional\" exit or if there's a legitimate\n\t\t# error in the simulation\n\t\tself.default_exit = None\n\t\tself.postcall_exit = None\n\t\tif len(self.statements) == self.num_stmts:\n\t\t\tself.next_expr = SimIRExpr(self.irsb.next, self.last_imark, self.num_stmts, self.state, self.options)\n\t\t\tself.state.add_constraints(*self.next_expr.constraints)\n\t\t\tself.state.inplace_after()\n\n\t\t\t# TODO: in static mode, we probably only want to count one\n\t\t\t# \tcode ref even when multiple exits are going to the same\n\t\t\t#\tplace.\n\t\t\tself.add_refs(SimCodeRef(self.last_imark.addr, self.num_stmts, self.next_expr.sim_value, self.next_expr.reg_deps(), self.next_expr.tmp_deps()))\n\n\t\t\t# the default exit\n\t\t\tself.default_exit = s_exit.SimExit(sirsb_exit = self)\n\t\t\tl.debug(\"Adding default exit.\")\n\t\t\tself.add_exits(self.default_exit)\n\n\t\t\t# ret emulation\n\t\t\tif o.DO_RET_EMULATION in self.options and self.irsb.jumpkind == \"Ijk_Call\":\n\t\t\t\tself.postcall_exit = s_exit.SimExit(sirsb_postcall = self, static = (o.SYMBOLIC not in self.options))\n\t\t\t\tl.debug(\"Adding postcall exit.\")\n\t\t\t\tself.add_exits(self.postcall_exit)\n\t\telse:\n\t\t\tl.debug(\"SimIRSB %s has no default exit\", self.id)\n\n\t\tif o.BREAK_SIRSB_END in self.options:\n\t\t\timport ipdb\n\t\t\tipdb.set_trace()", "output": "def handle_irsb(self):\n\t\tif o.BREAK_SIRSB_START in self.options:\n\t\t\timport ipdb\n\t\t\tipdb.set_trace()\n\n\t\t# finish the initial setup\n\t\tself.prepare_temps(self.state)\n\n\t\t# handle the statements\n\t\ttry:\n\t\t\tself.handle_statements()\n\t\texcept s_exception.SimError:\n\t\t\tl.warning(\"A SimError was hit when analyzing statements. This may signify an unavoidable exit (ok) or an actual error (not ok)\", exc_info=True)\n\n\t\t# some finalization\n\t\tself.state.inplace_after()\n\t\tself.num_stmts = len(self.irsb.statements())\n\n\t\t# If there was an error, and not all the statements were processed,\n\t\t# then this block does not have a default exit. This can happen if\n\t\t# the block has an unavoidable \"conditional\" exit or if there's a legitimate\n\t\t# error in the simulation\n\t\tself.default_exit = None\n\t\tself.postcall_exit = None\n\t\tif len(self.statements) == self.num_stmts:\n\t\t\tself.next_expr = SimIRExpr(self.irsb.next, self.last_imark, self.num_stmts, self.state, self.options)\n\t\t\tself.state.add_constraints(*self.next_expr.constraints)\n\t\t\tself.state.inplace_after()\n\n\t\t\t# TODO: in static mode, we probably only want to count one\n\t\t\t# \tcode ref even when multiple exits are going to the same\n\t\t\t#\tplace.\n\t\t\tself.add_refs(SimCodeRef(self.last_imark.addr, self.num_stmts, self.next_expr.sim_value, self.next_expr.reg_deps(), self.next_expr.tmp_deps()))\n\n\t\t\t# the default exit\n\t\t\tself.default_exit = s_exit.SimExit(sirsb_exit = self)\n\t\t\tl.debug(\"Adding default exit.\")\n\t\t\tself.add_exits(self.default_exit)\n\n\t\t\t# ret emulation\n\t\t\tif o.DO_RET_EMULATION in self.options and self.irsb.jumpkind == \"Ijk_Call\":\n\t\t\t\tself.postcall_exit = s_exit.SimExit(sirsb_postcall = self, simple_postcall = (o.SYMBOLIC not in self.options))\n\t\t\t\tl.debug(\"Adding postcall exit.\")\n\t\t\t\tself.add_exits(self.postcall_exit)\n\t\telse:\n\t\t\tl.debug(\"SimIRSB %s has no default exit\", self.id)\n\n\t\tif o.BREAK_SIRSB_END in self.options:\n\t\t\timport ipdb\n\t\t\tipdb.set_trace()"}
{"input": "def add_identity(self, category='', itype='', name='', node=''):\n        self.add_node(node)\n        self.nodes[node].addIdentity(category=category,\n                         id_type=itype,\n                         name=name)", "output": "def add_identity(self, category='', itype='', name='', node=''):\n        self.add_node(node)\n        self.nodes[node].addIdentity(category=category,\n                         itype=itype,\n                         name=name)"}
{"input": "def get_admin(seed_user_id = None):\n    \"\"\"returns user objects with id == seed_user_id\n    if the user with that id is not an administrator,\n    the function will try to find another admin or moderator\n    who has the smallest user id\n\n    if the user is not found, or there are no moderators/admins\n    User.DoesNotExist will be raised\n\n    The reason this function is here and not on a manager of\n    the user object is because we still patch the django-auth User table\n    and it's probably better not to patch the manager\n    \"\"\"\n\n    if seed_user_id:\n        user = models.User.objects.get(id = seed_user_id)#let it raise error here\n        if user.is_administrator() or user.is_moderator():\n            return user\n    try:\n        return models.User.objects.filter(\n                        Q(is_superuser=True) | Q(status='m')\n                    ).order_by('id')[0]\n    except IndexError:\n        raise models.User.DoesNotExist(\n                \"\"\"Please add a moderator or an administrator to the forum first\n                there don't seem to be any\"\"\"\n            )", "output": "def get_admin(seed_user_id = None):\n    \"\"\"returns user objects with id == seed_user_id\n    if the user with that id is not an administrator,\n    the function will try to find another admin or moderator\n    who has the smallest user id\n\n    if the user is not found, or there are no moderators/admins\n    User.DoesNotExist will be raised\n\n    The reason this function is here and not on a manager of\n    the user object is because we still patch the django-auth User table\n    and it's probably better not to patch the manager\n    \"\"\"\n\n    if seed_user_id:\n        user = models.User.objects.get(id = seed_user_id)#let it raise error here\n        if user.is_administrator() or user.is_moderator():\n            return user\n    try:\n        return models.User.objects.filter(\n                        Q(is_superuser=True) | Q(askbot_profile__status='m')\n                    ).order_by('id')[0]\n    except IndexError:\n        raise models.User.DoesNotExist(\n                \"\"\"Please add a moderator or an administrator to the forum first\n                there don't seem to be any\"\"\"\n            )"}
{"input": "def search(request, template=None):\n    \"\"\"Performs search or displays the search form.\"\"\"\n\n    # JSON-specific variables\n    is_json = (request.GET.get('format') == 'json')\n    callback = request.GET.get('callback', '').strip()\n    mimetype = 'application/x-javascript' if callback else 'application/json'\n\n    # Search \"Expires\" header format\n    expires_fmt = '%A, %d %B %Y %H:%M:%S GMT'\n\n    # Check callback is valid\n    if is_json and callback and not jsonp_is_valid(callback):\n        return HttpResponse(\n            json.dumps({'error': _('Invalid callback function.')}),\n            mimetype=mimetype, status=400)\n\n    language = locale_or_default(request.GET.get('language', request.locale))\n    r = request.GET.copy()\n    a = request.GET.get('a', '0')\n\n    # Search default values\n    try:\n        category = map(int, r.getlist('category')) or \\\n                   settings.SEARCH_DEFAULT_CATEGORIES\n    except ValueError:\n        category = settings.SEARCH_DEFAULT_CATEGORIES\n    r.setlist('category', category)\n\n    # Basic form\n    if a == '0':\n        r['w'] = r.get('w', constants.WHERE_BASIC)\n    # Advanced form\n    if a == '2':\n        r['language'] = language\n        r['a'] = '1'\n\n    # TODO: Rewrite so SearchForm is unbound initially and we can use `initial`\n    # on the form fields.\n    if 'include_archived' not in r:\n        r['include_archived'] = False\n\n    search_form = SearchForm(r)\n\n    if not search_form.is_valid() or a == '2':\n        if is_json:\n            return HttpResponse(\n                json.dumps({'error': _('Invalid search data.')}),\n                mimetype=mimetype,\n                status=400)\n\n        t = template if request.MOBILE else 'search/form.html'\n        search_ = jingo.render(request, t,\n                               {'advanced': a, 'request': request,\n                                'search_form': search_form})\n        search_['Cache-Control'] = 'max-age=%s' % \\\n                                   (settings.SEARCH_CACHE_PERIOD * 60)\n        search_['Expires'] = (datetime.utcnow() +\n                              timedelta(\n                                minutes=settings.SEARCH_CACHE_PERIOD)) \\\n                              .strftime(expires_fmt)\n        return search_\n\n    cleaned = search_form.cleaned_data\n\n    page = max(smart_int(request.GET.get('page')), 1)\n    offset = (page - 1) * settings.SEARCH_RESULTS_PER_PAGE\n\n    # get language name for display in template\n    lang = language.lower()\n    if settings.LANGUAGES.get(lang):\n        lang_name = settings.LANGUAGES[lang]\n    else:\n        lang_name = ''\n\n    wiki_s = wiki_search\n    question_s = question_search\n    discussion_s = discussion_search\n\n    documents = []\n\n    # wiki filters\n    # Category filter\n    if cleaned['category']:\n        wiki_s = wiki_s.filter(category__in=cleaned['category'])\n\n    # Locale filter\n    wiki_s = wiki_s.filter(locale=language)\n\n    # Product filter\n    products = cleaned['product']\n    for p in products:\n        wiki_s = wiki_s.filter(tag=p)\n\n    # Tags filter\n    tags = [t.strip() for t in cleaned['tags'].split()]\n    for t in tags:\n        wiki_s = wiki_s.filter(tag=t)\n\n    # Archived bit\n    if a == '0' and not cleaned['include_archived']:\n        # Default to NO for basic search:\n        cleaned['include_archived'] = False\n    if not cleaned['include_archived']:\n        wiki_s = wiki_s.filter(is_archived=False)\n    # End of wiki filters\n\n    # Support questions specific filters\n    if cleaned['w'] & constants.WHERE_SUPPORT:\n\n        # Solved is set by default if using basic search\n        if a == '0' and not cleaned['has_helpful']:\n            cleaned['has_helpful'] = constants.TERNARY_YES\n\n        # These filters are ternary, they can be either YES, NO, or OFF\n        ternary_filters = ('is_locked', 'is_solved', 'has_answers',\n                           'has_helpful')\n        d = dict((filter_name, _ternary_filter(cleaned[filter_name]))\n                 for filter_name in ternary_filters\n                 if cleaned[filter_name])\n        if d:\n            question_s = question_s.filter(**d)\n\n        if cleaned['asked_by']:\n            question_s = question_s.filter(\n                question_creator=cleaned['asked_by'])\n\n        if cleaned['answered_by']:\n            question_s = question_s.filter(\n                answer_creator=cleaned['answered_by'])\n\n        q_tags = [t.strip() for t in cleaned['q_tags'].split()]\n        for t in q_tags:\n            question_s = question_s.filter(tag=t)\n\n    # Discussion forum specific filters\n    if cleaned['w'] & constants.WHERE_DISCUSSION:\n        if cleaned['author']:\n            discussion_s = discussion_s.filter(author_ord=cleaned['author'])\n\n        if cleaned['thread_type']:\n            if constants.DISCUSSION_STICKY in cleaned['thread_type']:\n                discussion_s = discussion_s.filter(is_sticky=1)\n\n            if constants.DISCUSSION_LOCKED in cleaned['thread_type']:\n                discussion_s = discussion_s.filter(is_locked=1)\n\n        if cleaned['forum']:\n            discussion_s = discussion_s.filter(forum_id=cleaned['forum'])\n\n    # Filters common to support and discussion forums\n    # Created filter\n    unix_now = int(time.time())\n    interval_filters = (\n        ('created', cleaned['created'], cleaned['created_date']),\n        ('updated', cleaned['updated'], cleaned['updated_date']),\n        ('question_votes', cleaned['num_voted'], cleaned['num_votes']))\n    for filter_name, filter_option, filter_date in interval_filters:\n        if filter_option == constants.INTERVAL_BEFORE:\n            before = {filter_name + '__gte': 0,\n                      filter_name + '__lte': max(filter_date, 0)}\n\n            if filter_name != 'question_votes':\n                discussion_s = discussion_s.filter(**before)\n            question_s = question_s.filter(**before)\n        elif filter_option == constants.INTERVAL_AFTER:\n            after = {filter_name + '__gte': min(filter_date, unix_now),\n                     filter_name + '__lte': unix_now}\n\n            if filter_name != 'question_votes':\n                discussion_s = discussion_s.filter(**after)\n            question_s = question_s.filter(**after)\n\n    sortby = smart_int(request.GET.get('sortby'))\n    try:\n        max_results = settings.SEARCH_MAX_RESULTS\n        cleaned_q = cleaned['q']\n\n        if cleaned['w'] & constants.WHERE_WIKI:\n            wiki_s = wiki_s.query(cleaned_q)[:max_results]\n            # Execute the query and append to documents\n            documents += [('wiki', (pair[0], pair[1]))\n                          for pair in enumerate(wiki_s.object_ids())]\n\n        if cleaned['w'] & constants.WHERE_SUPPORT:\n            # Sort results by\n            try:\n                question_s = question_s.order_by(\n                    *constants.SORT_QUESTIONS[sortby])\n            except IndexError:\n                pass\n\n            question_s = question_s.highlight(\n                'content',\n                before_match='<b>',\n                after_match='</b>',\n                limit=settings.SEARCH_SUMMARY_LENGTH)\n\n            question_s = question_s.query(cleaned_q)[:max_results]\n            documents += [('question', (pair[0], pair[1]))\n                          for pair in enumerate(question_s.object_ids())]\n\n        if cleaned['w'] & constants.WHERE_DISCUSSION:\n            # Sort results by\n            try:\n                # Note that the first attribute needs to be the same\n                # here and in forums/models.py discussion_search.\n                discussion_s = discussion_s.group_by(\n                    'thread_id', constants.GROUPSORT[sortby])\n            except IndexError:\n                pass\n\n            discussion_s = discussion_s.highlight(\n                'content',\n                before_match='<b>',\n                after_match='</b>',\n                limit=settings.SEARCH_SUMMARY_LENGTH)\n\n            discussion_s = discussion_s.query(cleaned_q)[:max_results]\n            documents += [('discussion', (pair[0], pair[1]))\n                          for pair in enumerate(discussion_s.object_ids())]\n\n    except SearchError:\n        if is_json:\n            return HttpResponse(json.dumps({'error':\n                                             _('Search Unavailable')}),\n                                mimetype=mimetype, status=503)\n\n        t = 'search/mobile/down.html' if request.MOBILE else 'search/down.html'\n        return jingo.render(request, t, {'q': cleaned['q']}, status=503)\n\n    pages = paginate(request, documents, settings.SEARCH_RESULTS_PER_PAGE)\n\n    # Build a dict of { type_ -> list of indexes } for the specific\n    # docs that we're going to display on this page.  This makes it\n    # easy for us to slice the appropriate search Ss so we're limiting\n    # our db hits to just the items we're showing.\n    documents_dict = {}\n    for doc in documents[offset:offset + settings.SEARCH_RESULTS_PER_PAGE]:\n        documents_dict.setdefault(doc[0], []).append(doc[1][0])\n\n    docs_for_page = []\n    for type_, search_s in [('wiki', wiki_s),\n                            ('question', question_s),\n                            ('discussion', discussion_s)]:\n        if type_ not in documents_dict:\n            continue\n\n        # documents_dict[type_] is a list of indexes--one for each\n        # object id search result for that type_.  We use the values\n        # at the beginning and end of the list for slice boundaries.\n        begin = documents_dict[type_][0]\n        end = documents_dict[type_][-1] + 1\n        docs_for_page += [(type_, doc) for doc in search_s[begin:end]]\n\n    results = []\n    for i, docinfo in enumerate(docs_for_page):\n        rank = i + offset\n        type_, doc = docinfo\n        try:\n            if type_ == 'wiki':\n                summary = doc.current_revision.summary\n\n                result = {\n                    'search_summary': summary,\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'document',\n                    'rank': rank,\n                    'object': doc,\n                }\n                results.append(result)\n\n            elif type_ == 'question':\n                try:\n                    excerpt = question_s.excerpt(doc)[0]\n                except ExcerptTimeoutError:\n                    statsd.incr('search.excerpt.timeout')\n                    excerpt = u''\n                except ExcerptSocketErrorError:\n                    statsd.incr('search.excerpt.socketerror')\n                    excerpt = u''\n\n                summary = jinja2.Markup(clean_excerpt(excerpt))\n\n                result = {\n                    'search_summary': summary,\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'question',\n                    'rank': rank,\n                    'object': doc,\n                }\n                results.append(result)\n\n            else:\n                # discussion_s is based on Post--not Thread, so we have\n                # to get this manually.\n                thread = Thread.objects.get(pk=doc.thread_id)\n\n                try:\n                    excerpt = discussion_s.excerpt(doc)[0]\n                except ExcerptTimeoutError:\n                    statsd.incr('search.excerpt.timeout')\n                    excerpt = u''\n                except ExcerptSocketErrorError:\n                    statsd.incr('search.excerpt.socketerror')\n                    excerpt = u''\n\n                summary = jinja2.Markup(clean_excerpt(excerpt))\n\n                result = {\n                    'search_summary': summary,\n                    'url': thread.get_absolute_url(),\n                    'title': thread.title,\n                    'type': 'thread',\n                    'rank': rank,\n                    'object': thread,\n                }\n                results.append(result)\n        except IndexError:\n            break\n        except ObjectDoesNotExist:\n            continue\n\n    items = [(k, v) for k in search_form.fields for\n             v in r.getlist(k) if v and k != 'a']\n    items.append(('a', '2'))\n\n    if is_json:\n        # Models are not json serializable.\n        for r in results:\n            del r['object']\n        data = {}\n        data['results'] = results\n        data['total'] = len(results)\n        data['query'] = cleaned['q']\n        if not results:\n            data['message'] = _('No pages matched the search criteria')\n        json_data = json.dumps(data)\n        if callback:\n            json_data = callback + '(' + json_data + ');'\n\n        return HttpResponse(json_data, mimetype=mimetype)\n\n    results_ = jingo.render(request, template,\n        {'num_results': len(documents), 'results': results, 'q': cleaned['q'],\n         'pages': pages, 'w': cleaned['w'],\n         'search_form': search_form, 'lang_name': lang_name, })\n    results_['Cache-Control'] = 'max-age=%s' % \\\n                                (settings.SEARCH_CACHE_PERIOD * 60)\n    results_['Expires'] = (datetime.utcnow() +\n                           timedelta(minutes=settings.SEARCH_CACHE_PERIOD)) \\\n                           .strftime(expires_fmt)\n    results_.set_cookie(settings.LAST_SEARCH_COOKIE, urlquote(cleaned['q']),\n                        max_age=3600, secure=False, httponly=False)\n    return results_", "output": "def search(request, template=None):\n    \"\"\"Performs search or displays the search form.\"\"\"\n\n    # JSON-specific variables\n    is_json = (request.GET.get('format') == 'json')\n    callback = request.GET.get('callback', '').strip()\n    mimetype = 'application/x-javascript' if callback else 'application/json'\n\n    # Search \"Expires\" header format\n    expires_fmt = '%A, %d %B %Y %H:%M:%S GMT'\n\n    # Check callback is valid\n    if is_json and callback and not jsonp_is_valid(callback):\n        return HttpResponse(\n            json.dumps({'error': _('Invalid callback function.')}),\n            mimetype=mimetype, status=400)\n\n    language = locale_or_default(request.GET.get('language', request.locale))\n    r = request.GET.copy()\n    a = request.GET.get('a', '0')\n\n    # Search default values\n    try:\n        category = map(int, r.getlist('category')) or \\\n                   settings.SEARCH_DEFAULT_CATEGORIES\n    except ValueError:\n        category = settings.SEARCH_DEFAULT_CATEGORIES\n    r.setlist('category', category)\n\n    # Basic form\n    if a == '0':\n        r['w'] = r.get('w', constants.WHERE_BASIC)\n    # Advanced form\n    if a == '2':\n        r['language'] = language\n        r['a'] = '1'\n\n    # TODO: Rewrite so SearchForm is unbound initially and we can use `initial`\n    # on the form fields.\n    if 'include_archived' not in r:\n        r['include_archived'] = False\n\n    search_form = SearchForm(r)\n\n    if not search_form.is_valid() or a == '2':\n        if is_json:\n            return HttpResponse(\n                json.dumps({'error': _('Invalid search data.')}),\n                mimetype=mimetype,\n                status=400)\n\n        t = template if request.MOBILE else 'search/form.html'\n        search_ = jingo.render(request, t,\n                               {'advanced': a, 'request': request,\n                                'search_form': search_form})\n        search_['Cache-Control'] = 'max-age=%s' % \\\n                                   (settings.SEARCH_CACHE_PERIOD * 60)\n        search_['Expires'] = (datetime.utcnow() +\n                              timedelta(\n                                minutes=settings.SEARCH_CACHE_PERIOD)) \\\n                              .strftime(expires_fmt)\n        return search_\n\n    cleaned = search_form.cleaned_data\n\n    page = max(smart_int(request.GET.get('page')), 1)\n    offset = (page - 1) * settings.SEARCH_RESULTS_PER_PAGE\n\n    # get language name for display in template\n    lang = language.lower()\n    if settings.LANGUAGES.get(lang):\n        lang_name = settings.LANGUAGES[lang]\n    else:\n        lang_name = ''\n\n    wiki_s = wiki_search\n    question_s = question_search\n    discussion_s = discussion_search\n\n    documents = []\n\n    # wiki filters\n    # Category filter\n    if cleaned['category']:\n        wiki_s = wiki_s.filter(category__in=cleaned['category'])\n\n    # Locale filter\n    wiki_s = wiki_s.filter(locale=language)\n\n    # Product filter\n    products = cleaned['product']\n    for p in products:\n        wiki_s = wiki_s.filter(tag=p)\n\n    # Tags filter\n    tags = [t.strip() for t in cleaned['tags'].split()]\n    for t in tags:\n        wiki_s = wiki_s.filter(tag=t)\n\n    # Archived bit\n    if a == '0' and not cleaned['include_archived']:\n        # Default to NO for basic search:\n        cleaned['include_archived'] = False\n    if not cleaned['include_archived']:\n        wiki_s = wiki_s.filter(is_archived=False)\n    # End of wiki filters\n\n    # Support questions specific filters\n    if cleaned['w'] & constants.WHERE_SUPPORT:\n\n        # Solved is set by default if using basic search\n        if a == '0' and not cleaned['has_helpful']:\n            cleaned['has_helpful'] = constants.TERNARY_YES\n\n        # These filters are ternary, they can be either YES, NO, or OFF\n        ternary_filters = ('is_locked', 'is_solved', 'has_answers',\n                           'has_helpful')\n        d = dict((filter_name, _ternary_filter(cleaned[filter_name]))\n                 for filter_name in ternary_filters\n                 if cleaned[filter_name])\n        if d:\n            question_s = question_s.filter(**d)\n\n        if cleaned['asked_by']:\n            question_s = question_s.filter(\n                question_creator=cleaned['asked_by'])\n\n        if cleaned['answered_by']:\n            question_s = question_s.filter(\n                answer_creator=cleaned['answered_by'])\n\n        q_tags = [t.strip() for t in cleaned['q_tags'].split()]\n        for t in q_tags:\n            question_s = question_s.filter(tag=t)\n\n    # Discussion forum specific filters\n    if cleaned['w'] & constants.WHERE_DISCUSSION:\n        if cleaned['author']:\n            discussion_s = discussion_s.filter(author_ord=cleaned['author'])\n\n        if cleaned['thread_type']:\n            if constants.DISCUSSION_STICKY in cleaned['thread_type']:\n                discussion_s = discussion_s.filter(is_sticky=1)\n\n            if constants.DISCUSSION_LOCKED in cleaned['thread_type']:\n                discussion_s = discussion_s.filter(is_locked=1)\n\n        if cleaned['forum']:\n            discussion_s = discussion_s.filter(forum_id__in=cleaned['forum'])\n\n    # Filters common to support and discussion forums\n    # Created filter\n    unix_now = int(time.time())\n    interval_filters = (\n        ('created', cleaned['created'], cleaned['created_date']),\n        ('updated', cleaned['updated'], cleaned['updated_date']),\n        ('question_votes', cleaned['num_voted'], cleaned['num_votes']))\n    for filter_name, filter_option, filter_date in interval_filters:\n        if filter_option == constants.INTERVAL_BEFORE:\n            before = {filter_name + '__gte': 0,\n                      filter_name + '__lte': max(filter_date, 0)}\n\n            if filter_name != 'question_votes':\n                discussion_s = discussion_s.filter(**before)\n            question_s = question_s.filter(**before)\n        elif filter_option == constants.INTERVAL_AFTER:\n            after = {filter_name + '__gte': min(filter_date, unix_now),\n                     filter_name + '__lte': unix_now}\n\n            if filter_name != 'question_votes':\n                discussion_s = discussion_s.filter(**after)\n            question_s = question_s.filter(**after)\n\n    sortby = smart_int(request.GET.get('sortby'))\n    try:\n        max_results = settings.SEARCH_MAX_RESULTS\n        cleaned_q = cleaned['q']\n\n        if cleaned['w'] & constants.WHERE_WIKI:\n            wiki_s = wiki_s.query(cleaned_q)[:max_results]\n            # Execute the query and append to documents\n            documents += [('wiki', (pair[0], pair[1]))\n                          for pair in enumerate(wiki_s.object_ids())]\n\n        if cleaned['w'] & constants.WHERE_SUPPORT:\n            # Sort results by\n            try:\n                question_s = question_s.order_by(\n                    *constants.SORT_QUESTIONS[sortby])\n            except IndexError:\n                pass\n\n            question_s = question_s.highlight(\n                'content',\n                before_match='<b>',\n                after_match='</b>',\n                limit=settings.SEARCH_SUMMARY_LENGTH)\n\n            question_s = question_s.query(cleaned_q)[:max_results]\n            documents += [('question', (pair[0], pair[1]))\n                          for pair in enumerate(question_s.object_ids())]\n\n        if cleaned['w'] & constants.WHERE_DISCUSSION:\n            # Sort results by\n            try:\n                # Note that the first attribute needs to be the same\n                # here and in forums/models.py discussion_search.\n                discussion_s = discussion_s.group_by(\n                    'thread_id', constants.GROUPSORT[sortby])\n            except IndexError:\n                pass\n\n            discussion_s = discussion_s.highlight(\n                'content',\n                before_match='<b>',\n                after_match='</b>',\n                limit=settings.SEARCH_SUMMARY_LENGTH)\n\n            discussion_s = discussion_s.query(cleaned_q)[:max_results]\n            documents += [('discussion', (pair[0], pair[1]))\n                          for pair in enumerate(discussion_s.object_ids())]\n\n    except SearchError:\n        if is_json:\n            return HttpResponse(json.dumps({'error':\n                                             _('Search Unavailable')}),\n                                mimetype=mimetype, status=503)\n\n        t = 'search/mobile/down.html' if request.MOBILE else 'search/down.html'\n        return jingo.render(request, t, {'q': cleaned['q']}, status=503)\n\n    pages = paginate(request, documents, settings.SEARCH_RESULTS_PER_PAGE)\n\n    # Build a dict of { type_ -> list of indexes } for the specific\n    # docs that we're going to display on this page.  This makes it\n    # easy for us to slice the appropriate search Ss so we're limiting\n    # our db hits to just the items we're showing.\n    documents_dict = {}\n    for doc in documents[offset:offset + settings.SEARCH_RESULTS_PER_PAGE]:\n        documents_dict.setdefault(doc[0], []).append(doc[1][0])\n\n    docs_for_page = []\n    for type_, search_s in [('wiki', wiki_s),\n                            ('question', question_s),\n                            ('discussion', discussion_s)]:\n        if type_ not in documents_dict:\n            continue\n\n        # documents_dict[type_] is a list of indexes--one for each\n        # object id search result for that type_.  We use the values\n        # at the beginning and end of the list for slice boundaries.\n        begin = documents_dict[type_][0]\n        end = documents_dict[type_][-1] + 1\n        docs_for_page += [(type_, doc) for doc in search_s[begin:end]]\n\n    results = []\n    for i, docinfo in enumerate(docs_for_page):\n        rank = i + offset\n        type_, doc = docinfo\n        try:\n            if type_ == 'wiki':\n                summary = doc.current_revision.summary\n\n                result = {\n                    'search_summary': summary,\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'document',\n                    'rank': rank,\n                    'object': doc,\n                }\n                results.append(result)\n\n            elif type_ == 'question':\n                try:\n                    excerpt = question_s.excerpt(doc)[0]\n                except ExcerptTimeoutError:\n                    statsd.incr('search.excerpt.timeout')\n                    excerpt = u''\n                except ExcerptSocketErrorError:\n                    statsd.incr('search.excerpt.socketerror')\n                    excerpt = u''\n\n                summary = jinja2.Markup(clean_excerpt(excerpt))\n\n                result = {\n                    'search_summary': summary,\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'question',\n                    'rank': rank,\n                    'object': doc,\n                }\n                results.append(result)\n\n            else:\n                # discussion_s is based on Post--not Thread, so we have\n                # to get this manually.\n                thread = Thread.objects.get(pk=doc.thread_id)\n\n                try:\n                    excerpt = discussion_s.excerpt(doc)[0]\n                except ExcerptTimeoutError:\n                    statsd.incr('search.excerpt.timeout')\n                    excerpt = u''\n                except ExcerptSocketErrorError:\n                    statsd.incr('search.excerpt.socketerror')\n                    excerpt = u''\n\n                summary = jinja2.Markup(clean_excerpt(excerpt))\n\n                result = {\n                    'search_summary': summary,\n                    'url': thread.get_absolute_url(),\n                    'title': thread.title,\n                    'type': 'thread',\n                    'rank': rank,\n                    'object': thread,\n                }\n                results.append(result)\n        except IndexError:\n            break\n        except ObjectDoesNotExist:\n            continue\n\n    items = [(k, v) for k in search_form.fields for\n             v in r.getlist(k) if v and k != 'a']\n    items.append(('a', '2'))\n\n    if is_json:\n        # Models are not json serializable.\n        for r in results:\n            del r['object']\n        data = {}\n        data['results'] = results\n        data['total'] = len(results)\n        data['query'] = cleaned['q']\n        if not results:\n            data['message'] = _('No pages matched the search criteria')\n        json_data = json.dumps(data)\n        if callback:\n            json_data = callback + '(' + json_data + ');'\n\n        return HttpResponse(json_data, mimetype=mimetype)\n\n    results_ = jingo.render(request, template,\n        {'num_results': len(documents), 'results': results, 'q': cleaned['q'],\n         'pages': pages, 'w': cleaned['w'],\n         'search_form': search_form, 'lang_name': lang_name, })\n    results_['Cache-Control'] = 'max-age=%s' % \\\n                                (settings.SEARCH_CACHE_PERIOD * 60)\n    results_['Expires'] = (datetime.utcnow() +\n                           timedelta(minutes=settings.SEARCH_CACHE_PERIOD)) \\\n                           .strftime(expires_fmt)\n    results_.set_cookie(settings.LAST_SEARCH_COOKIE, urlquote(cleaned['q']),\n                        max_age=3600, secure=False, httponly=False)\n    return results_"}
{"input": "def set_cxx_flags_clib_hook(build_clib, build_info):\n    cc = build_clib.compiler\n    new_args = []\n    new_link_args = []\n\n    std_flag = get_cxx_std_flag(cc)\n    if std_flag is not None:\n        new_args.append(std_flag)\n\n    if sys.platform == 'darwin':\n        # Set min macOS version\n        min_macos_flag = '-mmacosx-version-min=10.9'\n        if has_flag(cc, min_macos_flag):\n            new_args.append(min_macos_flag)\n            new_link_args.append(min_macos_flag)\n\n    dict_append(build_info, extra_compile_args=new_args,\n                extra_link_args=new_link_args)", "output": "def set_cxx_flags_clib_hook(build_clib, build_info):\n    cc = build_clib.compiler\n    new_args = []\n    new_link_args = []\n\n    std_flag = get_cxx_std_flag(cc)\n    if std_flag is not None:\n        new_args.append(std_flag)\n\n    if sys.platform == 'darwin':\n        # Set min macOS version\n        min_macos_flag = '-mmacosx-version-min=10.9'\n        if has_flag(cc, min_macos_flag):\n            new_args.append(min_macos_flag)\n            new_link_args.append(min_macos_flag)\n\n    dict_append(build_info, extra_compiler_args=new_args,\n                extra_link_args=new_link_args)"}
{"input": "def __init__(self, session):\n\t\tScreen.__init__(self, session)\n\t\tself[\"video\"] = VideoWindow()\n\t\tself.currentService = None\n\t\tconfig.av.pip = ConfigPosition(default=[-1, -1, -1, -1], limits = (719, 567, 720, 568))\n\t\tself.onLayoutFinish.append(self.LayoutFinished)", "output": "def __init__(self, session):\n\t\tScreen.__init__(self, session)\n\t\tself[\"video\"] = VideoWindow()\n\t\tself.currentService = None\n\t\tconfig.av.pip = ConfigPosition(default=[-1, -1, -1, -1], args = (719, 567, 720, 568))\n\t\tself.onLayoutFinish.append(self.LayoutFinished)"}
{"input": "def register(cls, name, password, email):\n        pw_hash = make_pw_hash(name, password)\n        return User(\n                parent=users_key(),\n                name=name,\n                password=pw_hash,\n                email=email\n        )", "output": "def register(cls, name, password, email):\n        pw_hash = make_pw_hash(name, password)\n        return User(\n                parent=users_key(),\n                username=name,\n                password=pw_hash,\n                email=email\n        )"}
{"input": "def semich_bite():\n    # Hamiltonian Parameters\n    A = 2*CoFa.eV_to_au\n\n    # Gaps used in the dirac system\n    mx = 0.05*CoFa.eV_to_au\n    muz = 0.033\n\n    semich_bite_system = cued.hamiltonian.Semiconductor(A=A, mz=muz, mx=mx,\n                                                        a=8.28834, align=True)\n    return semich_bite_system", "output": "def semich_bite():\n    # Hamiltonian Parameters\n    A = 2*CoFa.eV_to_au\n\n    # Gaps used in the dirac system\n    mx = 0.05*CoFa.eV_to_au\n    muz = 0.033\n\n    semich_bite_system = cued.hamiltonian.Semiconductor(A=A, mz=muz, mx=mx,\n                                                        a=8.28834, nature=True)\n    return semich_bite_system"}
{"input": "def get_product(self):\n        return self.content_type.get_object_for_this_type(id=self.object_id)", "output": "def get_product(self):\n        return self.content_type.get_object_for_this_type(pk=self.object_id)"}
{"input": "def post_list(request):\n\tposts = Post.objects.filter(published_date_lte=timezone.now()).order_by('published_date')\n\treturn render(request, 'blog/post_list.html', {'posts':posts})", "output": "def post_list(request):\n\tposts = Post.objects.filter(published_date__lte=timezone.now()).order_by('published_date')\n\treturn render(request, 'blog/post_list.html', {'posts':posts})"}
{"input": "def get_spendable_coins(self, domain = None):\n        return self.get_utxos(self, domain, exclude_frozen=True, mature=True)", "output": "def get_spendable_coins(self, domain = None):\n        return self.get_utxos(domain, exclude_frozen=True, mature=True)"}
