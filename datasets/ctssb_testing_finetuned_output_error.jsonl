{"input": "def _build_werkzeug_request_data(request):\n    request_data = {\n        'url': request.url,\n        'GET': dict(request.args),\n        'POST': dict(request.form),\n        'user_ip': _extract_user_ip(request),\n        'headers': dict(request.headers),\n        'method': request.method,\n        'files_keys': request.files.keys(),\n    }\n\n    if request.get_json():\n        request_data['body'] = _scrub_request_params(request.json)\n\n    return request_data", "output": "def _build_werkzeug_request_data(request):\n    request_data = {\n        'url': request.url,\n        'GET': dict(request.args),\n        'POST': dict(request.form),\n        'user_ip': _extract_user_ip(request),\n        'headers': dict(request.headers),\n        'method': request.method,\n        'files_keys': request.files.keys(),\n    }\n\n    if request.get_json():\n        request_data['body'] = json.dumps(_scrub_request_params(request.json))\n\n    return request_data"}
{"input": "def mouseMoveEvent(self, ev):\n        if self.lastMousePos is None:\n            self.lastMousePos = Point(ev.pos())\n        delta = Point(ev.pos() - self.lastMousePos)\n        self.lastMousePos = Point(ev.pos())\n\n        QtGui.QGraphicsView.mouseMoveEvent(self, ev)\n        if not self.mouseEnabled:\n            return\n        self.sigSceneMouseMoved.emit(self.mapToScene(ev.pos()))\n            \n        if self.clickAccepted:  ## Ignore event if an item in the scene has already claimed it.\n            return\n        \n        if ev.buttons() == QtCore.Qt.RightButton:\n            delta = Point(np.clip(delta[0], -50, 50), np.clip(-delta[1], -50, 50))\n            scale = 1.01 ** delta\n            self.scale(scale[0], scale[1], center=self.mapToScene(self.mousePressPos))\n            self.sigDeviceRangeChanged.emit(self, self.range)\n\n        elif ev.buttons() in [QtCore.Qt.MidButton, QtCore.Qt.LeftButton]:  ## Allow panning by left or mid button.\n            px = self.pixelSize()\n            tr = -delta * px\n            \n            self.translate(tr[0], tr[1])\n            self.sigDeviceRangeChanged.emit(self, self.range)", "output": "def mouseMoveEvent(self, ev):\n        if self.lastMousePos is None:\n            self.lastMousePos = Point(ev.pos())\n        delta = Point(Point(ev.pos()) - self.lastMousePos)\n        self.lastMousePos = Point(ev.pos())\n\n        QtGui.QGraphicsView.mouseMoveEvent(self, ev)\n        if not self.mouseEnabled:\n            return\n        self.sigSceneMouseMoved.emit(self.mapToScene(ev.pos()))\n            \n        if self.clickAccepted:  ## Ignore event if an item in the scene has already claimed it.\n            return\n        \n        if ev.buttons() == QtCore.Qt.RightButton:\n            delta = Point(np.clip(delta[0], -50, 50), np.clip(-delta[1], -50, 50))\n            scale = 1.01 ** delta\n            self.scale(scale[0], scale[1], center=self.mapToScene(self.mousePressPos))\n            self.sigDeviceRangeChanged.emit(self, self.range)\n\n        elif ev.buttons() in [QtCore.Qt.MidButton, QtCore.Qt.LeftButton]:  ## Allow panning by left or mid button.\n            px = self.pixelSize()\n            tr = -delta * px\n            \n            self.translate(tr[0], tr[1])\n            self.sigDeviceRangeChanged.emit(self, self.range)"}
{"input": "def onChanged(mcs,obj,prop):\n        if prop == mcs._disabled:\n            obj.ViewObject.signalChangeIcon()\n        if super(Constraint,mcs).onChanged(obj,prop):\n            try:\n                if obj.Name==obj.Label or \\\n                   mcs.getType(utils.getLabel(obj)):\n                    obj.Label = mcs.getTypeName(obj)\n            except Exception as e:\n                logger.debug('auto constraint label failed: {}'.format(e))", "output": "def onChanged(mcs,obj,prop):\n        if prop == mcs._disabled:\n            obj.ViewObject.signalChangeIcon()\n        if super(Constraint,mcs).onChanged(obj,prop):\n            try:\n                if obj.Name==obj.Label or \\\n                   mcs.getType(str(utils.getLabel(obj))):\n                    obj.Label = mcs.getTypeName(obj)\n            except Exception as e:\n                logger.debug('auto constraint label failed: {}'.format(e))"}
{"input": "def add_layers_to_map(self):\n        '''\n        Required function to actually add the layers to the html packet\n        '''\n        layers_temp = self.env.get_template('add_layers.js')\n\n        data_string = ''\n        for i, layer in enumerate(self.added_layers):\n            name = layer.keys()[0]\n            data_string+='\\\"'\n            data_string+=name\n            data_string+='\\\"'\n            data_string+=': '\n            data_string+=name\n            if i < len(self.added_layers)-1:\n                data_string+=\",\\n\"\n            else:\n                data_string+=\"\\n\"\n\n        data_layers = layers_temp.render({'layers': data_string})\n        self.template_vars.setdefault('data_layers', []).append((data_string))", "output": "def add_layers_to_map(self):\n        '''\n        Required function to actually add the layers to the html packet\n        '''\n        layers_temp = self.env.get_template('add_layers.js')\n\n        data_string = ''\n        for i, layer in enumerate(self.added_layers):\n            name = list(layer.keys())[0]\n            data_string+='\\\"'\n            data_string+=name\n            data_string+='\\\"'\n            data_string+=': '\n            data_string+=name\n            if i < len(self.added_layers)-1:\n                data_string+=\",\\n\"\n            else:\n                data_string+=\"\\n\"\n\n        data_layers = layers_temp.render({'layers': data_string})\n        self.template_vars.setdefault('data_layers', []).append((data_string))"}
{"input": "def _register_relative_to_build_file(build_file_dir, rel_source_root_dir, *allowed_target_types):\n    source_root_dir = os.path.join(build_file_dir, rel_source_root_dir)\n    SourceRoot._register(source_root_dir, *allowed_target_types)", "output": "def _register_relative_to_build_file(build_file_dir, rel_source_root_dir, *allowed_target_types):\n    source_root_dir = os.path.relpath(os.path.join(build_file_dir, rel_source_root_dir), get_buildroot())\n    SourceRoot._register(source_root_dir, *allowed_target_types)"}
{"input": "def test_uid_list_multiple_headers(db, RE):\n    RE.subscribe(db.insert)\n    uids = RE(pchain(count([det]), count([det])))\n    headers = db[uids]\n    assert uids == [h['start']['uid'] for h in headers]", "output": "def test_uid_list_multiple_headers(db, RE):\n    RE.subscribe(db.insert)\n    uids = RE(pchain(count([det]), count([det])))\n    headers = db[uids]\n    assert uids == tuple([h['start']['uid'] for h in headers])"}
{"input": "def __init__(self):\n        Source.__init__(self)\n        if not self.url:\n            raise FatalError(_(\"'url' attribute is missing in the recipe\"))\n        self.url = self.url % {'version': self.version, 'name': self.name}\n        self.filename = os.path.basename(self.url)\n        self.download_path = os.path.join(self.repo_dir, self.filename)", "output": "def __init__(self):\n        Source.__init__(self)\n        if not self.url:\n            raise InvalidRecipeError(_(\"'url' attribute is missing in the recipe\"))\n        self.url = self.url % {'version': self.version, 'name': self.name}\n        self.filename = os.path.basename(self.url)\n        self.download_path = os.path.join(self.repo_dir, self.filename)"}
{"input": "def read(self):\n        while not self.is_ready():\n            pass\n\n        dataBits = b''\n\n        for j in range(0, 3):\n            octet = 0\n            for i in range(0, 8):\n                self.pSCK.value(True)\n                octet <<= 1\n                bitLu = self.pOUt()\n                if bitLu: octet += 1\n                self.pSCK.value(False)\n\n            dataBits += bytes([octet])\n\n\n        # set channel and gain factor for next reading\n        for i in range(self.GAIN):\n            self.pSCK.value(True)\n            self.pSCK.value(False)\n\n        # check for all 1\n        if self.DEBUG:\n            print('{}'.format(dataBits))\n        if all(item == 1 for item in dataBits[0]):\n            if self.DEBUG:\n                print('all true')\n            self.allTrue = True\n            return self.lastVal\n        self.allTrue = False\n        readbits = \"\"\n\n\n        return struct.unpack('<i', measure + ('\\0' if measure[2] < 128 else '\\xff'))", "output": "def read(self):\n        while not self.is_ready():\n            pass\n\n        dataBits = b''\n\n        for j in range(0, 3):\n            octet = 0\n            for i in range(0, 8):\n                self.pSCK.value(True)\n                octet <<= 1\n                bitLu = self.pOut()\n                if bitLu: octet += 1\n                self.pSCK.value(False)\n\n            dataBits += bytes([octet])\n\n\n        # set channel and gain factor for next reading\n        for i in range(self.GAIN):\n            self.pSCK.value(True)\n            self.pSCK.value(False)\n\n        # check for all 1\n        if self.DEBUG:\n            print('{}'.format(dataBits))\n        if all(item == 1 for item in dataBits[0]):\n            if self.DEBUG:\n                print('all true')\n            self.allTrue = True\n            return self.lastVal\n        self.allTrue = False\n        readbits = \"\"\n\n\n        return struct.unpack('<i', measure + ('\\0' if measure[2] < 128 else '\\xff'))"}
{"input": "def set_logger(script):\n    log_format = logging.Formatter(\"%(message)s\")\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n\n    file_handler = logging.fileHandler(script + \".logs\")\n    file_handler.setFormatter(log_format)\n    root_logger.addHandler(file_handler)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    root_logger.addHandler(console_handler)\n\n    return root_logger", "output": "def set_logger(script):\n    log_format = logging.Formatter(\"%(message)s\")\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n\n    file_handler = logging.FileHandler(script + \".logs\")\n    file_handler.setFormatter(log_format)\n    root_logger.addHandler(file_handler)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    root_logger.addHandler(console_handler)\n\n    return root_logger"}
{"input": "def calculate(lines, nu, beta_rad):\n        beta_rad = pd.Series(beta_rad)\n        nu = pd.Series(nu)\n        h = const.h.cgs.value\n        c = const.c.cgs.value\n        df = pd.DataFrame(1, index=nu.index, columns=beta_rad.index)\n        df = df.mul(nu, axis='index') * beta_rad\n        exponential = (np.exp(h * df) - 1)**(-1)\n        remainder = (2 * (h * nu.values ** 3) /\n            (c ** 2))\n        j_blues = exponential.multiply(remainder, axis=0)\n        return pd.DataFrame(j_blues, index=lines.index, columns=beta_rad.index)", "output": "def calculate(lines, nu, beta_rad):\n        beta_rad = pd.Series(beta_rad)\n        nu = pd.Series(nu)\n        h = const.h.cgs.value\n        c = const.c.cgs.value\n        df = pd.DataFrame(1, index=nu.index, columns=beta_rad.index)\n        df = df.mul(nu, axis='index') * beta_rad\n        exponential = (np.exp(h * df) - 1)**(-1)\n        remainder = (2 * (h * nu.values ** 3) /\n            (c ** 2))\n        j_blues = exponential.mul(remainder, axis=0)\n        return pd.DataFrame(j_blues, index=lines.index, columns=beta_rad.index)"}
{"input": "def from_session(cls, session):\n        if session.start_time is None and session.end_time is not None:\n            # Session may have anded before it fully started\n            session.start_time = session.end_time\n        call_time = session.start_time or datetime.now()\n        if session.start_time and session.end_time:\n            duration = session.end_time - session.start_time\n        else:\n            duration = None\n        remote_identity = session.remote_identity\n        remote_uri = '%s@%s' % (remote_identity.uri.user, remote_identity.uri.host)\n        try:\n            contact = next(contact for contact in AddressbookManager().get_contacts() if remote_uri in (addr.uri for addr in contact.uris))\n        except StopIteration:\n            display_name = remote_identity.display_name\n        else:\n            display_name = contact.name\n        match = self.phone_number_re.match(remote_uri)\n        if match:\n            remote_uri = match.group('number')\n        if display_name and display_name != remote_uri:\n            remote_identity = '%s <%s>' % (display_name, remote_uri)\n        else:\n            remote_identity = remote_uri\n        return cls(remote_identity, remote_uri, unicode(session.account.id), call_time, duration)", "output": "def from_session(cls, session):\n        if session.start_time is None and session.end_time is not None:\n            # Session may have anded before it fully started\n            session.start_time = session.end_time\n        call_time = session.start_time or datetime.now()\n        if session.start_time and session.end_time:\n            duration = session.end_time - session.start_time\n        else:\n            duration = None\n        remote_identity = session.remote_identity\n        remote_uri = '%s@%s' % (remote_identity.uri.user, remote_identity.uri.host)\n        try:\n            contact = next(contact for contact in AddressbookManager().get_contacts() if remote_uri in (addr.uri for addr in contact.uris))\n        except StopIteration:\n            display_name = remote_identity.display_name\n        else:\n            display_name = contact.name\n        match = cls.phone_number_re.match(remote_uri)\n        if match:\n            remote_uri = match.group('number')\n        if display_name and display_name != remote_uri:\n            remote_identity = '%s <%s>' % (display_name, remote_uri)\n        else:\n            remote_identity = remote_uri\n        return cls(remote_identity, remote_uri, unicode(session.account.id), call_time, duration)"}
{"input": "def connect(self, address=tuple(), reattempt=True, use_tls=True):\n        \"\"\"\n        Connect to the XMPP server.\n\n        When no address is given, a SRV lookup for the server will\n        be attempted. If that fails, the server user in the JID\n        will be used.\n\n        Arguments:\n            address   -- A tuple containing the server's host and port.\n            reattempt -- If True, reattempt the connection if an\n                         error occurs. Defaults to True.\n            use_tls   -- Indicates if TLS should be used for the\n                         connection. Defaults to True.\n        \"\"\"\n        self.session_started_event.clear()\n        if not address or len(address) < 2:\n            if not self.srv_support:\n                log.debug(\"Did not supply (address, port) to connect\" + \\\n                              \" to and no SRV support is installed\" + \\\n                              \" (http://www.dnspython.org).\" + \\\n                              \" Continuing to attempt connection, using\" + \\\n                              \" server hostname from JID.\")\n            else:\n                log.debug(\"Since no address is supplied,\" + \\\n                              \"attempting SRV lookup.\")\n                try:\n                    xmpp_srv = \"_xmpp-client._tcp.%s\" % self.boundjid.host\n                    answers = dns.resolver.query(xmpp_srv, dns.rdatatype.SRV)\n                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                    log.debug(\"No appropriate SRV record found.\" + \\\n                                  \" Using JID server name.\")\n                except (dns.exception.Timeout,):\n                    log.debug(\"DNS resolution timed out.\")\n                else:\n                    # Pick a random server, weighted by priority.\n\n                    addresses = {}\n                    intmax = 0\n                    topprio = 65535\n                    for answer in answers:\n                        topprio = min(topprio, answer.priority)\n                    for answer in answers:\n                        if answer.priority == topprio:\n                            intmax += answer.weight\n                            addresses[intmax] = (answer.target.to_text()[:-1],\n                                             answer.port)\n\n                    #python3 returns a generator for dictionary keys\n                    items = [x for x in addresses.keys()]\n                    items.sort()\n\n                    picked = random.randint(0, intmax)\n                    for item in items:\n                        if picked <= priority:\n                            address = addresses[item]\n                            break\n\n        if not address:\n            # If all else fails, use the server from the JID.\n            address = (self.boundjid.host, 5222)\n\n        return XMLStream.connect(self, address[0], address[1],\n                                 use_tls=use_tls, reattempt=reattempt)", "output": "def connect(self, address=tuple(), reattempt=True, use_tls=True):\n        \"\"\"\n        Connect to the XMPP server.\n\n        When no address is given, a SRV lookup for the server will\n        be attempted. If that fails, the server user in the JID\n        will be used.\n\n        Arguments:\n            address   -- A tuple containing the server's host and port.\n            reattempt -- If True, reattempt the connection if an\n                         error occurs. Defaults to True.\n            use_tls   -- Indicates if TLS should be used for the\n                         connection. Defaults to True.\n        \"\"\"\n        self.session_started_event.clear()\n        if not address or len(address) < 2:\n            if not self.srv_support:\n                log.debug(\"Did not supply (address, port) to connect\" + \\\n                              \" to and no SRV support is installed\" + \\\n                              \" (http://www.dnspython.org).\" + \\\n                              \" Continuing to attempt connection, using\" + \\\n                              \" server hostname from JID.\")\n            else:\n                log.debug(\"Since no address is supplied,\" + \\\n                              \"attempting SRV lookup.\")\n                try:\n                    xmpp_srv = \"_xmpp-client._tcp.%s\" % self.boundjid.host\n                    answers = dns.resolver.query(xmpp_srv, dns.rdatatype.SRV)\n                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):\n                    log.debug(\"No appropriate SRV record found.\" + \\\n                                  \" Using JID server name.\")\n                except (dns.exception.Timeout,):\n                    log.debug(\"DNS resolution timed out.\")\n                else:\n                    # Pick a random server, weighted by priority.\n\n                    addresses = {}\n                    intmax = 0\n                    topprio = 65535\n                    for answer in answers:\n                        topprio = min(topprio, answer.priority)\n                    for answer in answers:\n                        if answer.priority == topprio:\n                            intmax += answer.weight\n                            addresses[intmax] = (answer.target.to_text()[:-1],\n                                             answer.port)\n\n                    #python3 returns a generator for dictionary keys\n                    items = [x for x in addresses.keys()]\n                    items.sort()\n\n                    picked = random.randint(0, intmax)\n                    for item in items:\n                        if picked <= item:\n                            address = addresses[item]\n                            break\n\n        if not address:\n            # If all else fails, use the server from the JID.\n            address = (self.boundjid.host, 5222)\n\n        return XMLStream.connect(self, address[0], address[1],\n                                 use_tls=use_tls, reattempt=reattempt)"}
{"input": "def make_iq(self, id=0, ifrom=None, ito=None, type=None, query=None):\n        \"\"\"\n        Create a new Iq stanza with a given Id and from JID.\n\n        Arguments:\n            id    -- An ideally unique ID value for this stanza thread.\n                     Defaults to 0.\n            ifrom -- The from JID to use for this stanza.\n            ito   -- The destination JID for this stanza.\n            type  -- The Iq's type, one of: get, set, result, or error.\n            query -- Optional namespace for adding a query element.\n        \"\"\"\n        iq = self.Iq()\n        iq['id'] = str(id)\n        iq['to'] = ito\n        iq['from'] = ifrom\n        iq['type'] = itype\n        iq['query'] = query\n        return iq", "output": "def make_iq(self, id=0, ifrom=None, ito=None, itype=None, query=None):\n        \"\"\"\n        Create a new Iq stanza with a given Id and from JID.\n\n        Arguments:\n            id    -- An ideally unique ID value for this stanza thread.\n                     Defaults to 0.\n            ifrom -- The from JID to use for this stanza.\n            ito   -- The destination JID for this stanza.\n            type  -- The Iq's type, one of: get, set, result, or error.\n            query -- Optional namespace for adding a query element.\n        \"\"\"\n        iq = self.Iq()\n        iq['id'] = str(id)\n        iq['to'] = ito\n        iq['from'] = ifrom\n        iq['type'] = itype\n        iq['query'] = query\n        return iq"}
{"input": "def sparseness(x, which='LTS'):\n    \"\"\"Calculate sparseness.\n\n    Sparseness comes in two flavors:\n\n    **Lifetime kurtosis (LTK)** quantifies the widths of tuning curves\n    (according to Muench & Galizia, 2016):\n\n    .. math::\n\n        S = \\\\Bigg\\\\{ \\\\frac{1}{N} \\\\sum^M_{i=1} \\\\Big[ \\\\frac{r_i - \\\\overline{r}}{\\\\sigma_r} \\\\Big] ^4  \\\\Bigg\\\\} - 3\n\n    where :math:`N` is the number of observations, :math:`r_i` the value of\n    observation :math:`i`, and :math:`\\\\overline{r}` and\n    :math:`\\\\sigma_r` the mean and the standard deviation of the observations'\n    values, respectively. LTK is assuming a normal, or at least symmetric\n    distribution.\n\n    **Lifetime sparseness (LTS)** quantifies selectivity\n    (Bhandawat et al., 2007):\n\n    .. math::\n\n        S = \\\\frac{1}{1-1/N} \\\\Bigg[1- \\\\frac{\\\\big(\\\\sum^N_{j=1} r_j / N\\\\big)^2}{\\\\sum^N_{j=1} r_j^2 / N} \\\\Bigg]\n\n    where :math:`N` is the number of observations, and :math:`r_j` is the\n    value of an observation.\n\n    Notes\n    -----\n    ``NaN`` values will be ignored. You can use that to e.g. ignore zero\n    values in a large connectivity matrix by changing these values to ``NaN``\n    before passing it to ``pymaid.sparseness``.\n\n\n    Parameters\n    ----------\n    x :         DataFrame | array-like\n                (N, M) dataset with N (rows) observations for M (columns)\n                neurons. One-dimensional data will be converted to two\n                dimensions (N rows, 1 column).\n    which :     \"LTS\" | \"LTK\"\n                Determines whether lifetime sparseness (LTS) or lifetime\n                kurtosis (LTK) is returned.\n\n    Returns\n    -------\n    sparseness\n                ``pandas.Series`` if input was pandas DataFrame, else\n                ``numpy.array``.\n\n    Examples\n    --------\n    Calculate sparseness of olfactory inputs to group of neurons:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> # Generate adjacency matrix\n    >>> adj = pymaid.adjacency_matrix(s='annotation:WTPN2017_excitatory_uPN_right',\n    ...                               t='annotation:ASB LHN')\n    >>> # Calculate lifetime sparseness\n    >>> S = pymaid.sparseness(adj, which='LTS')\n    >>> # Plot distribution\n    >>> ax = S.plot.hist(bins=np.arange(0, 1, .1))\n    >>> ax.set_xlabel('LTS')\n    >>> plt.show()\n\n    \"\"\"\n    if not isinstance(x, (pd.DataFrame, np.ndarray)):\n        x = np.array(x)\n\n    # Make sure we are working with 2 dimensional data\n    if isinstance(x, np.ndarray) and x.ndim == 1:\n        x = x.reshape(x.shape[0], 1)\n\n    N = np.sum(~np.isnan(x), axis=0)\n\n    if which == 'LTK':\n        return np.nansum(((x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0)) ** 4, axis=0) / N - 3\n    elif which == 'LTS':\n        return 1 / (1 - (1 / N)) * (1 - np.nansum(x / N, axis=0) ** 2 / np.nansum(x ** 2 / N, axis=0))\n    else:\n        raise ValueError('Parameter \"which\" must be either \"LTS\" or \"LTK\"')", "output": "def sparseness(x, which='LTS'):\n    \"\"\"Calculate sparseness.\n\n    Sparseness comes in two flavors:\n\n    **Lifetime kurtosis (LTK)** quantifies the widths of tuning curves\n    (according to Muench & Galizia, 2016):\n\n    .. math::\n\n        S = \\\\Bigg\\\\{ \\\\frac{1}{N} \\\\sum^N_{i=1} \\\\Big[ \\\\frac{r_i - \\\\overline{r}}{\\\\sigma_r} \\\\Big] ^4  \\\\Bigg\\\\} - 3\n\n    where :math:`N` is the number of observations, :math:`r_i` the value of\n    observation :math:`i`, and :math:`\\\\overline{r}` and\n    :math:`\\\\sigma_r` the mean and the standard deviation of the observations'\n    values, respectively. LTK is assuming a normal, or at least symmetric\n    distribution.\n\n    **Lifetime sparseness (LTS)** quantifies selectivity\n    (Bhandawat et al., 2007):\n\n    .. math::\n\n        S = \\\\frac{1}{1-1/N} \\\\Bigg[1- \\\\frac{\\\\big(\\\\sum^N_{j=1} r_j / N\\\\big)^2}{\\\\sum^N_{j=1} r_j^2 / N} \\\\Bigg]\n\n    where :math:`N` is the number of observations, and :math:`r_j` is the\n    value of an observation.\n\n    Notes\n    -----\n    ``NaN`` values will be ignored. You can use that to e.g. ignore zero\n    values in a large connectivity matrix by changing these values to ``NaN``\n    before passing it to ``pymaid.sparseness``.\n\n\n    Parameters\n    ----------\n    x :         DataFrame | array-like\n                (N, M) dataset with N (rows) observations for M (columns)\n                neurons. One-dimensional data will be converted to two\n                dimensions (N rows, 1 column).\n    which :     \"LTS\" | \"LTK\"\n                Determines whether lifetime sparseness (LTS) or lifetime\n                kurtosis (LTK) is returned.\n\n    Returns\n    -------\n    sparseness\n                ``pandas.Series`` if input was pandas DataFrame, else\n                ``numpy.array``.\n\n    Examples\n    --------\n    Calculate sparseness of olfactory inputs to group of neurons:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> # Generate adjacency matrix\n    >>> adj = pymaid.adjacency_matrix(s='annotation:WTPN2017_excitatory_uPN_right',\n    ...                               t='annotation:ASB LHN')\n    >>> # Calculate lifetime sparseness\n    >>> S = pymaid.sparseness(adj, which='LTS')\n    >>> # Plot distribution\n    >>> ax = S.plot.hist(bins=np.arange(0, 1, .1))\n    >>> ax.set_xlabel('LTS')\n    >>> plt.show()\n\n    \"\"\"\n    if not isinstance(x, (pd.DataFrame, np.ndarray)):\n        x = np.array(x)\n\n    # Make sure we are working with 2 dimensional data\n    if isinstance(x, np.ndarray) and x.ndim == 1:\n        x = x.reshape(x.shape[0], 1)\n\n    N = np.sum(~np.isnan(x), axis=0)\n\n    if which == 'LTK':\n        return np.nansum(((x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0)) ** 4, axis=0) / N - 3\n    elif which == 'LTS':\n        return 1 / (1 - (1 / N)) * (1 - np.nansum(x / N, axis=0) ** 2 / np.nansum(x ** 2 / N, axis=0))\n    else:\n        raise ValueError('Parameter \"which\" must be either \"LTS\" or \"LTK\"')"}
{"input": "def _get_namespace(m):\n    \"\"\"\n    This is used by _lambdify to parse its arguments.\n    \"\"\"\n    if isinstance(m, str):\n        _import(m)\n        return MODULES[m][0]\n    elif isinstance(m, dict):\n        return m\n    elif hasattr(m, \"__dict__\"):\n        return m.__dict__\n    else:\n        raise TypeError(\"Argument must be either a string, dict or module but it is: %s\" % m)", "output": "def _get_namespace(m):\n    \"\"\"\n    This is used by _lambdify to parse its arguments.\n    \"\"\"\n    if isinstance(m, string_types):\n        _import(m)\n        return MODULES[m][0]\n    elif isinstance(m, dict):\n        return m\n    elif hasattr(m, \"__dict__\"):\n        return m.__dict__\n    else:\n        raise TypeError(\"Argument must be either a string, dict or module but it is: %s\" % m)"}
{"input": "def _NH_SIPAccountManagerDidRemoveAccount(self, notification):\n        account = notification.data.account\n        action = (action for action in self.accounts_menu.actions() if action.data().toPyObject() is account).next()\n        self.account_menu.removeAction(action)\n        if isinstance(account, Account) and account.enabled and account.message_summary.enabled:\n            action = (action for action in self.voicemail_menu.actions() if action.data().toPyObject()[0] is account).next()\n            self.voicemail_menu.removeAction(action)", "output": "def _NH_SIPAccountManagerDidRemoveAccount(self, notification):\n        account = notification.data.account\n        action = (action for action in self.accounts_menu.actions() if action.data().toPyObject() is account).next()\n        self.accounts_menu.removeAction(action)\n        if isinstance(account, Account) and account.enabled and account.message_summary.enabled:\n            action = (action for action in self.voicemail_menu.actions() if action.data().toPyObject()[0] is account).next()\n            self.voicemail_menu.removeAction(action)"}
{"input": "def __init__(self, session, parent=None):\n        super(SessionWidget, self).__init__(parent)\n        with Resources.directory:\n            self.setupUi(self)\n        font = self.latency_label.font()\n        font.setPointSizeF(self.status_label.fontInfo().pointSizeF() - 1)\n        self.latency_label.setFont(font)\n        font = self.packet_loss_label.font()\n        font.setPointSizeF(self.status_label.fontInfo().pointSizeF() - 1)\n        self.packet_loss_label.setFont(font)\n        self.mute_button.type = LeftSegment\n        self.hold_button.type = MiddleSegment\n        self.record_button.type = MiddleSegment\n        self.hangup_button.type = RightSegment\n        self.selected = False\n        self.drop_indicator = False\n        self.conference_position = None\n        self._disable_dnd = False\n        self.setFocusProxy(parent)\n        self.mute_button.hidden.connect(self._mute_button_hidden)\n        self.mute_button.shown.connect(self._mute_button_shown)\n        self.mute_button.pressed.connect(self._tool_button_pressed)\n        self.hold_button.pressed.connect(self._tool_button_pressed)\n        self.record_button.pressed.connect(self._tool_button_pressed)\n        self.hangup_button.pressed.connect(self._tool_button_pressed)\n        self.mute_button.hide()\n        self.address_label.setText(session.name or session.uri)\n        self.stream_info_label.session_type = session.type\n        self.stream_info_label.session_type = session.codec_info\n        self.latency_label.value = session.latency\n        self.packet_loss_label.value = session.packet_loss\n        self.tls_label.setVisible(bool(session.tls))\n        self.srtp_label.setVisible(bool(session.srtp))", "output": "def __init__(self, session, parent=None):\n        super(SessionWidget, self).__init__(parent)\n        with Resources.directory:\n            self.setupUi(self)\n        font = self.latency_label.font()\n        font.setPointSizeF(self.status_label.fontInfo().pointSizeF() - 1)\n        self.latency_label.setFont(font)\n        font = self.packet_loss_label.font()\n        font.setPointSizeF(self.status_label.fontInfo().pointSizeF() - 1)\n        self.packet_loss_label.setFont(font)\n        self.mute_button.type = LeftSegment\n        self.hold_button.type = MiddleSegment\n        self.record_button.type = MiddleSegment\n        self.hangup_button.type = RightSegment\n        self.selected = False\n        self.drop_indicator = False\n        self.conference_position = None\n        self._disable_dnd = False\n        self.setFocusProxy(parent)\n        self.mute_button.hidden.connect(self._mute_button_hidden)\n        self.mute_button.shown.connect(self._mute_button_shown)\n        self.mute_button.pressed.connect(self._tool_button_pressed)\n        self.hold_button.pressed.connect(self._tool_button_pressed)\n        self.record_button.pressed.connect(self._tool_button_pressed)\n        self.hangup_button.pressed.connect(self._tool_button_pressed)\n        self.mute_button.hide()\n        self.address_label.setText(session.name or session.uri)\n        self.stream_info_label.session_type = session.type\n        self.stream_info_label.codec_info = session.codec_info\n        self.latency_label.value = session.latency\n        self.packet_loss_label.value = session.packet_loss\n        self.tls_label.setVisible(bool(session.tls))\n        self.srtp_label.setVisible(bool(session.srtp))"}
{"input": "def clear_transition_graph(self):\n        self.blocks = { self._projects.factory.block(self._addr) }\n        self._transition_graph = networkx.DiGraph()\n        self._transition_graph.add_node(self._addr)\n        self._local_transition_graph = None", "output": "def clear_transition_graph(self):\n        self.blocks = { self._project.factory.block(self._addr) }\n        self._transition_graph = networkx.DiGraph()\n        self._transition_graph.add_node(self._addr)\n        self._local_transition_graph = None"}
{"input": "def drive_compile(args):\n    \"\"\"Invoke tvmc.compiler module with command line arguments\n\n    Parameters\n    ----------\n    args: argparse.Namespace\n        Arguments from command line parser.\n\n    Returns\n    -------\n    int\n        Zero if successfully completed\n\n    \"\"\"\n    mod, params = frontends.load_model(args.FILE, args.model_format, args.input_shapes)\n\n    graph, lib, params, dumps = compile_model(\n        mod,\n        params,\n        args.target,\n        args.dump_code,\n        None,\n        args.tuning_records,\n        args.desired_layout,\n        args.disabled_pass,\n    )\n\n    if dumps:\n        save_dumps(args.output, dumps)\n\n    save_module(args.output, graph, lib, params, args.cross_compiler)\n    return 0", "output": "def drive_compile(args):\n    \"\"\"Invoke tvmc.compiler module with command line arguments\n\n    Parameters\n    ----------\n    args: argparse.Namespace\n        Arguments from command line parser.\n\n    Returns\n    -------\n    int\n        Zero if successfully completed\n\n    \"\"\"\n    mod, params = frontends.load_model(args.FILE, args.model_format, args.input_shapes)\n\n    graph, lib, params, dumps = compile_model(\n        mod,\n        params,\n        args.target,\n        args.dump_code,\n        None,\n        args.tuning_records,\n        args.desired_layout,\n        args.disable_pass,\n    )\n\n    if dumps:\n        save_dumps(args.output, dumps)\n\n    save_module(args.output, graph, lib, params, args.cross_compiler)\n    return 0"}
{"input": "def _create_main_frame(self):\n        \"\"\"\n        Helper to initialize the main gui frame.\n        \"\"\"\n        # data_modification = self._create_data_modification_tab()\n        self.event_finding_tab = tabs.EventFindingTab(self)\n        self.event_finding_tab.set_on_files_opened_callback(self._on_files_opened)\n        self.event_finding_tab.set_on_status_update_callback(self.set_status)\n        self.event_finding_tab.set_process_events_callback(self._process_events)\n\n        event_viewer = self._create_event_viewer_tab()\n        event_analysis = self._create_event_analysis_tab()\n        \n        # Layout holding everything        \n        self.main_tabwig = QtGui.QTabWidget()\n        # self.main_tabwig.addTab(data_modification, 'Data modification')\n        self.main_tabwig.addTab(self.event_finding_tab, 'Event Finding')\n        self.main_tabwig.addTab(event_viewer, 'Event View')\n        self.main_tabwig.addTab(event_analysis, 'Event Analysis')\n        self.main_tabwig.setMinimumSize(1000, 550)\n        \n        text = \"\"\"*********************\nWelcome to pyporegui!\n\nIf you are unfamiliar with the python console, feel free to ignore this console.\n\nHowever, you can use this console to interact with your data and the gui!\nType globals() to see globally defined variabels.\nType locals() to see application-specific variables.\n\nThe current namespace should include:\n    np        -    numpy\n    pg        -    pyqtgraph\n    ed        -    pypore.eventDatabase\n    currentPlot -  Top plot in the event finding tab.\n*********************\"\"\"\n\n        namespace = {'np': np, 'pg': pg, 'ed': ed, 'currentPlot': self.event_finding_tab.plotwid}\n        self.console = pgc.ConsoleWidget(namespace=namespace, text=text)\n        \n        frame = QtGui.QSplitter()\n        frame.setOrientation(QtCore.Qt.Vertical)\n        frame.addWidget(self.main_tabwig)\n        frame.addWidget(self.console)\n        \n        self.setCentralWidget(frame)", "output": "def _create_main_frame(self):\n        \"\"\"\n        Helper to initialize the main gui frame.\n        \"\"\"\n        # data_modification = self._create_data_modification_tab()\n        self.event_finding_tab = tabs.EventFindingTab(self)\n        self.event_finding_tab.set_on_files_opened_callback(self._on_files_opened)\n        self.event_finding_tab.set_on_status_update_callback(self.set_status)\n        self.event_finding_tab.set_process_events_callback(self._process_events)\n\n        event_viewer = self._create_event_viewer_tab()\n        event_analysis = self._create_event_analysis_tab()\n        \n        # Layout holding everything        \n        self.main_tabwig = QtGui.QTabWidget()\n        # self.main_tabwig.addTab(data_modification, 'Data modification')\n        self.main_tabwig.addTab(self.event_finding_tab, 'Event Finding')\n        self.main_tabwig.addTab(event_viewer, 'Event View')\n        self.main_tabwig.addTab(event_analysis, 'Event Analysis')\n        self.main_tabwig.setMinimumSize(1000, 550)\n        \n        text = \"\"\"*********************\nWelcome to pyporegui!\n\nIf you are unfamiliar with the python console, feel free to ignore this console.\n\nHowever, you can use this console to interact with your data and the gui!\nType globals() to see globally defined variabels.\nType locals() to see application-specific variables.\n\nThe current namespace should include:\n    np        -    numpy\n    pg        -    pyqtgraph\n    ed        -    pypore.eventDatabase\n    currentPlot -  Top plot in the event finding tab.\n*********************\"\"\"\n\n        namespace = {'np': np, 'pg': pg, 'ed': ed, 'currentPlot': self.event_finding_tab.plot_widget}\n        self.console = pgc.ConsoleWidget(namespace=namespace, text=text)\n        \n        frame = QtGui.QSplitter()\n        frame.setOrientation(QtCore.Qt.Vertical)\n        frame.addWidget(self.main_tabwig)\n        frame.addWidget(self.console)\n        \n        self.setCentralWidget(frame)"}
{"input": "def _bootstrap_rescue(self, install):\n        \"\"\"\n        Bootstrap everything needed in order to get Nix and the partitioner\n        usable in the rescue system. The latter is not only for partitioning\n        but also for mounting partitions.\n        \"\"\"\n        self.log_start(\"building Nix bootstrap installer...\")\n        bootstrap = subprocess.check_output([\n            \"nix-build\", \"<nixpkgs>\", \"--no-out-link\", \"-A\",\n            \"hetznerNixOpsInstaller\"\n        ]).rstrip()\n        self.log_end(\"done. ({0})\".format(bootstrap))\n\n        self.log_start(\"copying bootstrap files to rescue system...\")\n        tarstream = subprocess.Popen([bootstrap], stdout=subprocess.PIPE)\n        if not self.has_really_fast_connection():\n            stream = subprocess.Popen([\"gzip\", \"-c\"], stdin=tarstream.stdout,\n                                      stdout=subprocess.PIPE)\n            self.run_command(\"tar xz -C /\", stdin=stream.stdout)\n            stream.wait()\n        else:\n            self.run_command(\"tar x -C /\", stdin=tarstream.stdout)\n        tarstream.wait()\n        self.log_end(\"done.\")\n\n        if install:\n            self.log_start(\"partitioning disks...\")\n            out = self.run_command(\"nixpart -\", capture_stdout=True,\n                                   stdin_string=self.partitions)\n            self.fs_info = '\\n'.join(out.splitlines()[1:-1])\n        else:\n            self.log_start(\"mounting filesystems...\")\n            self.run_command(\"nixpart -m -\", stdin_string=self.partitions)\n        self.log_end(\"done.\")\n\n        if not install:\n            self.log_start(\"checking if system in /mnt is NixOS...\")\n            res = self.run_command(\"test -e /mnt/etc/NIXOS\", check=False)\n            if res == 0:\n                self.log_end(\"yes.\")\n            else:\n                self.log_end(\"NO! Not mounting special filesystems.\")\n                return\n\n        self.log_start(\"bind-mounting special filesystems...\")\n        for mountpoint in (\"/proc\", \"/dev\", \"/dev/shm\", \"/sys\"):\n            self.log_continue(\"{0}...\".format(mountpoint))\n            cmd = \"mkdir -m 0755 -p /mnt{0} && \".format(mountpoint)\n            cmd += \"mount --bind {0} /mnt{0}\".format(mountpoint)\n            self.run_command(cmd)\n        self.log_end(\"done.\")", "output": "def _bootstrap_rescue(self, install):\n        \"\"\"\n        Bootstrap everything needed in order to get Nix and the partitioner\n        usable in the rescue system. The latter is not only for partitioning\n        but also for mounting partitions.\n        \"\"\"\n        self.log_start(\"building Nix bootstrap installer...\")\n        bootstrap = subprocess.check_output([\n            \"nix-build\", \"<nixpkgs>\", \"--no-out-link\", \"-A\",\n            \"hetznerNixOpsInstaller\"\n        ]).rstrip()\n        self.log_end(\"done. ({0})\".format(bootstrap))\n\n        self.log_start(\"copying bootstrap files to rescue system...\")\n        tarstream = subprocess.Popen([bootstrap], stdout=subprocess.PIPE)\n        if not self.has_really_fast_connection():\n            stream = subprocess.Popen([\"gzip\", \"-c\"], stdin=tarstream.stdout,\n                                      stdout=subprocess.PIPE)\n            self.run_command(\"tar xz -C /\", stdin=stream.stdout)\n            stream.wait()\n        else:\n            self.run_command(\"tar x -C /\", stdin=tarstream.stdout)\n        tarstream.wait()\n        self.log_end(\"done.\")\n\n        if install:\n            self.log_start(\"partitioning disks...\")\n            out = self.run_command(\"nixpart -p -\", capture_stdout=True,\n                                   stdin_string=self.partitions)\n            self.fs_info = '\\n'.join(out.splitlines()[1:-1])\n        else:\n            self.log_start(\"mounting filesystems...\")\n            self.run_command(\"nixpart -m -\", stdin_string=self.partitions)\n        self.log_end(\"done.\")\n\n        if not install:\n            self.log_start(\"checking if system in /mnt is NixOS...\")\n            res = self.run_command(\"test -e /mnt/etc/NIXOS\", check=False)\n            if res == 0:\n                self.log_end(\"yes.\")\n            else:\n                self.log_end(\"NO! Not mounting special filesystems.\")\n                return\n\n        self.log_start(\"bind-mounting special filesystems...\")\n        for mountpoint in (\"/proc\", \"/dev\", \"/dev/shm\", \"/sys\"):\n            self.log_continue(\"{0}...\".format(mountpoint))\n            cmd = \"mkdir -m 0755 -p /mnt{0} && \".format(mountpoint)\n            cmd += \"mount --bind {0} /mnt{0}\".format(mountpoint)\n            self.run_command(cmd)\n        self.log_end(\"done.\")"}
{"input": "def specshow(X, sr=22050, hop_length=64, x_axis=None, y_axis=None, fmin=None, fmax=None, **kwargs):\n    \"\"\"Display a spectrogram. Wraps to `~matplotlib.pyplot.imshow` with some handy defaults.\n    \n    :parameters:\n      - X : np.ndarray\n          Matrix to display (eg, spectrogram)\n\n      - sr : int > 0\n          Sample rate. Used to determine time scale in x-axis\n\n      - hop_length : int > 0\n          Hop length. Also used to determine time scale in x-axis\n\n      - x_axis : None or {'time', 'frames', 'off'}\n          If None or 'off', no x axis is displayed.\n          If 'time', markers are shown as seconds, minutes, or hours.\n          If 'frames', markers are shown as frame counts.\n\n      - y_axis : None or {'linear', 'mel', 'chroma', 'off'}\n          If None or 'off', no y axis is displayed.\n          If 'linear', frequency range is determined by the FFT window and sample rate.\n          If 'mel', frequencies are determined by the mel scale.\n          If 'chroma', pitches are determined by the chroma filters.\n\n     - fmin, fmax : float > 0 or None\n          Used for setting the Mel frequency scale\n\n     - kwargs : dict\n          Additional arguments passed through to ``matplotlib.pyplot.imshow``.\n\n    :returns:\n     - image : ``matplotlib.image.AxesImage``\n          As returned from ``matplotlib.pyplot.imshow``.\n\n    \"\"\"\n\n    kwargs['aspect']        = kwargs.get('aspect',          'auto')\n    kwargs['origin']        = kwargs.get('origin',          'lower')\n    kwargs['interpolation'] = kwargs.get('interpolation',   'nearest')\n\n    kwargs['cmap']          = kwargs.get('cmap',            'Purples')\n\n    axes = plt.imshow(X, **kwargs)\n\n    # Set up the x ticks\n    x_pos = np.arange(0, X.shape[1]+1, max(1, X.shape[1] / 5))\n    if x_axis is 'time':\n        # Reformat into seconds, or minutes:seconds\n        x_val = x_pos * (hop_length / np.float(sr))\n\n        if max(x_val) > 3600.0:\n            # reformat into hours:minutes:seconds\n            x_val = map(lambda y: '%d:%02d:%02d' % (int(y / 3600), int(np.mod(y, 3600)), int(np.mod(y, 60))), x_val)\n        elif max(x_val) > 60.0:\n            # reformat into minutes:seconds\n            x_val = map(lambda y: '%d:%02d' % (int(y / 60), int(np.mod(y, 60))), x_val)\n        else:\n            # reformat into seconds, down to the millisecond\n            x_val = np.around(x_val, 3)\n\n        plt.xticks(x_pos, x_val)\n        plt.xlabel('Time')\n\n    elif x_axis is 'frames':\n        # Nothing to do here, plot is in frames\n        plt.xticks(x_pos, x_pos)\n        plt.xlabel('Frames')\n        pass\n    elif x_axis is None or x_axis is 'off':\n        plt.xticks([])\n        plt.xlabel('')\n        pass\n    else:\n        raise ValueError('Unknown x_axis parameter: %s' % x_axis)\n\n    # Set up the y ticks\n    y_pos = np.arange(0, X.shape[0], max(1, X.shape[0] / 6))\n    if y_axis is 'linear':\n        y_val = np.fft.fftfreq( (X.shape[0] -1) * 2, 1./sr)[y_pos].astype(np.int)\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Hz')\n        pass\n    elif y_axis is 'mel':\n        m_args = {}\n        if fmin is not None:\n            m_args['fmin'] = fmin\n        if fmax is not None:\n            m_args['fmax'] = fmax\n\n        y_val = librosa.core.mel_frequencies(X.shape[0], **m_args)[y_pos].astype(np.int)\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Hz')\n        pass\n    elif y_axis is 'chroma':\n        y_pos = np.arange(0, X.shape[0], max(1, X.shape[0] / 12))\n        y_val = ['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#']\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Note')\n        pass\n    elif y_axis is None or y_axis is 'off':\n        plt.yticks([])\n        plt.ylabel('')\n        pass\n    else:\n        raise ValueError('Unknown y_axis parameter: %s' % y_axis)\n    \n    return axes", "output": "def specshow(X, sr=22050, hop_length=64, x_axis=None, y_axis=None, fmin=None, fmax=None, **kwargs):\n    \"\"\"Display a spectrogram. Wraps to `~matplotlib.pyplot.imshow` with some handy defaults.\n    \n    :parameters:\n      - X : np.ndarray\n          Matrix to display (eg, spectrogram)\n\n      - sr : int > 0\n          Sample rate. Used to determine time scale in x-axis\n\n      - hop_length : int > 0\n          Hop length. Also used to determine time scale in x-axis\n\n      - x_axis : None or {'time', 'frames', 'off'}\n          If None or 'off', no x axis is displayed.\n          If 'time', markers are shown as seconds, minutes, or hours.\n          If 'frames', markers are shown as frame counts.\n\n      - y_axis : None or {'linear', 'mel', 'chroma', 'off'}\n          If None or 'off', no y axis is displayed.\n          If 'linear', frequency range is determined by the FFT window and sample rate.\n          If 'mel', frequencies are determined by the mel scale.\n          If 'chroma', pitches are determined by the chroma filters.\n\n     - fmin, fmax : float > 0 or None\n          Used for setting the Mel frequency scale\n\n     - kwargs : dict\n          Additional arguments passed through to ``matplotlib.pyplot.imshow``.\n\n    :returns:\n     - image : ``matplotlib.image.AxesImage``\n          As returned from ``matplotlib.pyplot.imshow``.\n\n    \"\"\"\n\n    kwargs['aspect']        = kwargs.get('aspect',          'auto')\n    kwargs['origin']        = kwargs.get('origin',          'lower')\n    kwargs['interpolation'] = kwargs.get('interpolation',   'nearest')\n\n    kwargs['cmap']          = kwargs.get('cmap',            'OrRd')\n\n    axes = plt.imshow(X, **kwargs)\n\n    # Set up the x ticks\n    x_pos = np.arange(0, X.shape[1]+1, max(1, X.shape[1] / 5))\n    if x_axis is 'time':\n        # Reformat into seconds, or minutes:seconds\n        x_val = x_pos * (hop_length / np.float(sr))\n\n        if max(x_val) > 3600.0:\n            # reformat into hours:minutes:seconds\n            x_val = map(lambda y: '%d:%02d:%02d' % (int(y / 3600), int(np.mod(y, 3600)), int(np.mod(y, 60))), x_val)\n        elif max(x_val) > 60.0:\n            # reformat into minutes:seconds\n            x_val = map(lambda y: '%d:%02d' % (int(y / 60), int(np.mod(y, 60))), x_val)\n        else:\n            # reformat into seconds, down to the millisecond\n            x_val = np.around(x_val, 3)\n\n        plt.xticks(x_pos, x_val)\n        plt.xlabel('Time')\n\n    elif x_axis is 'frames':\n        # Nothing to do here, plot is in frames\n        plt.xticks(x_pos, x_pos)\n        plt.xlabel('Frames')\n        pass\n    elif x_axis is None or x_axis is 'off':\n        plt.xticks([])\n        plt.xlabel('')\n        pass\n    else:\n        raise ValueError('Unknown x_axis parameter: %s' % x_axis)\n\n    # Set up the y ticks\n    y_pos = np.arange(0, X.shape[0], max(1, X.shape[0] / 6))\n    if y_axis is 'linear':\n        y_val = np.fft.fftfreq( (X.shape[0] -1) * 2, 1./sr)[y_pos].astype(np.int)\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Hz')\n        pass\n    elif y_axis is 'mel':\n        m_args = {}\n        if fmin is not None:\n            m_args['fmin'] = fmin\n        if fmax is not None:\n            m_args['fmax'] = fmax\n\n        y_val = librosa.core.mel_frequencies(X.shape[0], **m_args)[y_pos].astype(np.int)\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Hz')\n        pass\n    elif y_axis is 'chroma':\n        y_pos = np.arange(0, X.shape[0], max(1, X.shape[0] / 12))\n        y_val = ['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#']\n        plt.yticks(y_pos, y_val)\n        plt.ylabel('Note')\n        pass\n    elif y_axis is None or y_axis is 'off':\n        plt.yticks([])\n        plt.ylabel('')\n        pass\n    else:\n        raise ValueError('Unknown y_axis parameter: %s' % y_axis)\n    \n    return axes"}
{"input": "def get_vlc_mediactrl(self):\n        if sys.platform == 'win32':\n            oldcwd = os.getcwd()\n            os.chdir(os.path.join(self.installdir,'vlc'))\n    \n        # Arno: 2007-05-11: Don't ask me why but without the \"--verbose=0\" vlc will ignore the key redef.\n        params = [\"--verbose=2\"]\n        \n        \"\"\"\n        # To enable logging to file:\n        #[loghandle,logfilename] = mkstemp(\"vlc-log\")\n        #os.close(loghandle)\n        currwd = os.getcwd()\n        logfilename = os.path.join(currwd,\"vlc.log\")\n        params += [\"--file-logging\"]\n        params += [\"--logfile\",logfilename]\n        \"\"\"\n        \n        params += [\"--no-drop-late-frames\"] # Arno: 2007-11-19: don't seem to work as expected DEBUG\n        params += [\"--no-skip-frames\"]\n        params += [\"--quiet-synchro\"]\n        # JD: avoid \"fast catchup\" after frame loss by letting VLC have a flexible buffer\n        #params += [\"--access-filter\",\"timeshift\"]\n        #params += [\"--timeshift-force\"]\n        # Arno: attempt to improve robustness\n        params += [\"--http-reconnect\"]\n\n        # VLC wiki says: \"apply[ing] deinterlacing even if the original is not\n        # interlaced, is a really bad idea.\"\n        #params += [\"--vout-filter\",\"deinterlace\"]\n        #params += [\"--deinterlace-mode\",\"linear\"]\n        #params += [\"--demux=ts\"]\n        #params += [\"--codec=mp4\"]\n        #\n        params += [\"--no-plugins-cache\"]\n        params += [\"--key-fullscreen\", \"Esc\"] # must come last somehow on Win32\n        \n        if sys.platform == 'darwin':\n            params += [\"--plugin-path\", \"%s/macbinaries/vlc_plugins\" % (\n                 # location of plugins: next to tribler.py\n                 os.path.abspath(os.path.dirname(sys.argv[0]))\n                 )]\n            \n        media = vlc.MediaControl(params)\n            \n        if sys.platform == 'win32':\n                os.chdir(oldcwd)\n    \n        return media", "output": "def get_vlc_mediactrl(self):\n        if sys.platform == 'win32':\n            oldcwd = os.getcwd()\n            os.chdir(os.path.join(self.installdir,'vlc'))\n    \n        # Arno: 2007-05-11: Don't ask me why but without the \"--verbose=0\" vlc will ignore the key redef.\n        params = [\"--verbose=0\"]\n        \n        \"\"\"\n        # To enable logging to file:\n        #[loghandle,logfilename] = mkstemp(\"vlc-log\")\n        #os.close(loghandle)\n        currwd = os.getcwd()\n        logfilename = os.path.join(currwd,\"vlc.log\")\n        params += [\"--file-logging\"]\n        params += [\"--logfile\",logfilename]\n        \"\"\"\n        \n        params += [\"--no-drop-late-frames\"] # Arno: 2007-11-19: don't seem to work as expected DEBUG\n        params += [\"--no-skip-frames\"]\n        params += [\"--quiet-synchro\"]\n        # JD: avoid \"fast catchup\" after frame loss by letting VLC have a flexible buffer\n        #params += [\"--access-filter\",\"timeshift\"]\n        #params += [\"--timeshift-force\"]\n        # Arno: attempt to improve robustness\n        params += [\"--http-reconnect\"]\n\n        # VLC wiki says: \"apply[ing] deinterlacing even if the original is not\n        # interlaced, is a really bad idea.\"\n        #params += [\"--vout-filter\",\"deinterlace\"]\n        #params += [\"--deinterlace-mode\",\"linear\"]\n        #params += [\"--demux=ts\"]\n        #params += [\"--codec=mp4\"]\n        #\n        params += [\"--no-plugins-cache\"]\n        params += [\"--key-fullscreen\", \"Esc\"] # must come last somehow on Win32\n        \n        if sys.platform == 'darwin':\n            params += [\"--plugin-path\", \"%s/macbinaries/vlc_plugins\" % (\n                 # location of plugins: next to tribler.py\n                 os.path.abspath(os.path.dirname(sys.argv[0]))\n                 )]\n            \n        media = vlc.MediaControl(params)\n            \n        if sys.platform == 'win32':\n                os.chdir(oldcwd)\n    \n        return media"}
{"input": "def test_ensure_index_returned_on_root_path(self):\n        self.assertTrue('<html>' in self.request('/'))", "output": "def test_ensure_index_returned_on_root_path(self):\n        self.assertTrue('<html' in self.request('/'))"}
{"input": "def setUp(self):\n        \"\"\"\n        Set up for some of the tests.\n        \"\"\"\n        # load irregular 3-hour time series test rebin and mesh\n        tsfile = os.path.join(os.path.dirname(__file__), '..', 'data', 'simo_p_out.ts')\n        self.irreg_series = TsDB.fromfile(tsfile).get(name='tension_2_qs').x", "output": "def setUp(self):\n        \"\"\"\n        Set up for some of the tests.\n        \"\"\"\n        # load irregular 3-hour time series test rebin and mesh\n        tsfile = os.path.join(os.path.dirname(__file__), '..', 'data', 'simo_p_out.ts')\n        self.irreg_series = TsDB.fromfile(tsfile).get(name='Tension_2_qs').x"}
{"input": "def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            '''\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            '''\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].iteritems():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if norm_hyp[n] != 0:\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val", "output": "def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            '''\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            '''\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].iteritems():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val"}
{"input": "def bash_org_ru(type, jid, nick, text):\n\ttry: url = u'http://bash.org.ru/quote/'+str(int(text))\n\texcept: url = u'http://bash.org.ru/random'\n\tbody = html_encode(urllib.urlopen(url).read())\n\tif body.count('<div class=\"vote\">') > 1: msg = u'\u0426\u0438\u0442\u0430\u0442\u0430 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u0430!'\n\telse:\n\t\tbody = body.split('<div class=\"vote\">')[1].split('<div class=\"q\">')[0]\n\t\tmsg = u'http://bash.org.ru/quote/'+str(get_tag(body, 'a'))+u' '+replacer(body[body.find('[:||||:]'):].replace('</div>', '\\n').replace('[:||||:]', '::: ').replace('</a>\\n', ''))\n\tsend_msg(type, jid, nick, msg)", "output": "def bash_org_ru(type, jid, nick, text):\n\ttry: url = u'http://bash.org.ru/quote/'+str(int(text))\n\texcept: url = u'http://bash.org.ru/random'\n\tbody = html_encode(urllib.urlopen(url).read())\n\tif body.count('<div class=\"vote\">') > 1 and url.count('quote'): msg = u'\u0426\u0438\u0442\u0430\u0442\u0430 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u0430!'\n\telse:\n\t\tbody = body.split('<div class=\"vote\">')[1].split('<div class=\"q\">')[0]\n\t\tmsg = u'http://bash.org.ru/quote/'+str(get_tag(body, 'a'))+u' '+replacer(body[body.find('[:||||:]'):].replace('</div>', '\\n').replace('[:||||:]', '::: ').replace('</a>\\n', ''))\n\tsend_msg(type, jid, nick, msg)"}
{"input": "def rpc_run_task(self, task_uuid, model, method, *args, **kwargs):\n        \"\"\"Run/execute the task, which is a model method.\n\n        The idea is that Celery calls this by Odoo its external API,\n        whereas XML-RPC or a HTTP-controller.\n\n        The model-method can either be called as user:\n        - The \"celery\" (Odoo user) defined in the odoo.conf. This is the default, in case\n        the \"sudo\" setting isn't configured in the odoo.conf.\n        - \"admin\" (Odoo admin user), to circumvent model-access configuration for models\n        which run/process task. Therefor add \"sudo = True\" in the odoo.conf (see: example.odoo.conf).\n        \"\"\"\n\n        logger.info('CELERY rpc_run_task uuid {uuid}'.format(uuid=task_uuid))\n        \n        exist = self.search_count([('uuid', '=', task_uuid)])\n        if exist == 0:\n            msg = \"Task doesn't exist (anymore). Task-UUID: %s\" % task_uuid\n            logger.error(msg)\n            return (TASK_NOT_FOUND, msg)\n\n        model_obj = self.env[model]\n        task = self.search([('uuid', '=', task_uuid), ('state', 'in', [STATE_PENDING, STATE_RETRY, STATE_FAILURE])], limit=1)\n\n        if not task:\n            return ('OK', 'Task already processed')\n\n        # Start / Retry (refactor to absraction/neater code)\n        celery_retry = kwargs.get('celery_retry')\n        if celery_retry and task.retry and task.state == STATE_RETRY:\n            return (STATE_RETRY, 'Task is already executing a retry.')\n        elif celery_retry and task.celery_retry:\n            task.state = STATE_RETRY\n            vals = {'state': STATE_RETRY, 'state_date': fields.Datetime.now()}\n        else:\n            vals = {'state': STATE_STARTED, 'started_date': fields.Datetime.now()}\n\n        user, password, sudo = _get_celery_user_config()\n\n        # TODO\n        # Re-raise Exception if not called by XML-RPC, but directly from model/Odoo.\n        # This supports unit-tests and scripting purposes.\n        result = False\n        response = False\n        with registry(self._cr.dbname).cursor() as cr:\n            # Transaction/cursror for the exception handler.\n            env = api.Environment(cr, self._uid, {})\n            try:\n                if bool(sudo) and sudo:\n                    res = getattr(model_obj.with_env(env).sudo(), method)(task_uuid, **kwargs)\n                else:\n                    res = getattr(model_obj.with_env(env), method)(task_uuid, **kwargs)\n\n                if not bool(res):\n                    msg = \"No result/return value for Task UUID: %s. Ensure the task-method returns a value.\" % task_uuid\n                    logger.error(msg)\n                    raise CeleryTaskNoResultError(msg)\n\n                if isinstance(res, dict):\n                    result = res.get('result', True)\n                    vals.update({'res_model': res.get('res_model'), 'res_ids': res.get('res_ids')})\n                else:\n                    result = res\n                vals.update({'state': STATE_SUCCESS, 'state_date': fields.Datetime.now(), 'result': result, 'exc_info': False})\n            except Exception as e:\n                \"\"\" The Exception-handler does a rollback. So we need a new\n                transaction/cursor to store data about RETRY and exception info/traceback. \"\"\"\n\n                exc_info = traceback.format_exc()\n                vals.update({'state': STATE_RETRY, 'state_date': fields.Datetime.now(), 'exc_info': exc_info})\n                logger.warning('Retry... exception (see task form) from rpc_run_task {uuid}: {exc}.'.format(uuid=task_uuid, exc=e))\n                logger.debug('Exception rpc_run_task: {exc_info}'.format(uuid=task_uuid, exc_info=exc_info))\n                cr.rollback()\n            finally:\n                with registry(self._cr.dbname).cursor() as result_cr:\n                    env = api.Environment(result_cr, self._uid, {})\n                    task.with_env(env).write(vals)\n                response = (vals.get('state'), result)\n                return response", "output": "def rpc_run_task(self, task_uuid, model, method, *args, **kwargs):\n        \"\"\"Run/execute the task, which is a model method.\n\n        The idea is that Celery calls this by Odoo its external API,\n        whereas XML-RPC or a HTTP-controller.\n\n        The model-method can either be called as user:\n        - The \"celery\" (Odoo user) defined in the odoo.conf. This is the default, in case\n        the \"sudo\" setting isn't configured in the odoo.conf.\n        - \"admin\" (Odoo admin user), to circumvent model-access configuration for models\n        which run/process task. Therefor add \"sudo = True\" in the odoo.conf (see: example.odoo.conf).\n        \"\"\"\n\n        logger.info('CELERY rpc_run_task uuid {uuid}'.format(uuid=task_uuid))\n        \n        exist = self.search_count([('uuid', '=', task_uuid)])\n        if exist == 0:\n            msg = \"Task doesn't exist (anymore). Task-UUID: %s\" % task_uuid\n            logger.error(msg)\n            return (TASK_NOT_FOUND, msg)\n\n        model_obj = self.env[model]\n        task = self.search([('uuid', '=', task_uuid), ('state', 'in', [STATE_PENDING, STATE_RETRY, STATE_FAILURE])], limit=1)\n\n        if not task:\n            return ('OK', 'Task already processed')\n\n        # Start / Retry (refactor to absraction/neater code)\n        celery_retry = kwargs.get('celery_retry')\n        if celery_retry and task.retry and task.state == STATE_RETRY:\n            return (STATE_RETRY, 'Task is already executing a retry.')\n        elif celery_retry and task.celery_retry:\n            task.state = STATE_RETRY\n            vals = {'state': STATE_RETRY, 'state_date': fields.Datetime.now()}\n        else:\n            vals = {'state': STATE_STARTED, 'started_date': fields.Datetime.now()}\n\n        user, password, sudo = _get_celery_user_config()\n\n        # TODO\n        # Re-raise Exception if not called by XML-RPC, but directly from model/Odoo.\n        # This supports unit-tests and scripting purposes.\n        result = False\n        response = False\n        with registry(self._cr.dbname).cursor() as cr:\n            # Transaction/cursror for the exception handler.\n            env = api.Environment(cr, self._uid, {})\n            try:\n                if bool(sudo) and sudo:\n                    res = getattr(model_obj.with_env(env).sudo(), method)(task_uuid, **kwargs)\n                else:\n                    res = getattr(model_obj.with_env(env), method)(task_uuid, **kwargs)\n\n                if res != False and not bool(res):\n                    msg = \"No result/return value for Task UUID: %s. Ensure the task-method returns a value.\" % task_uuid\n                    logger.error(msg)\n                    raise CeleryTaskNoResultError(msg)\n\n                if isinstance(res, dict):\n                    result = res.get('result', True)\n                    vals.update({'res_model': res.get('res_model'), 'res_ids': res.get('res_ids')})\n                else:\n                    result = res\n                vals.update({'state': STATE_SUCCESS, 'state_date': fields.Datetime.now(), 'result': result, 'exc_info': False})\n            except Exception as e:\n                \"\"\" The Exception-handler does a rollback. So we need a new\n                transaction/cursor to store data about RETRY and exception info/traceback. \"\"\"\n\n                exc_info = traceback.format_exc()\n                vals.update({'state': STATE_RETRY, 'state_date': fields.Datetime.now(), 'exc_info': exc_info})\n                logger.warning('Retry... exception (see task form) from rpc_run_task {uuid}: {exc}.'.format(uuid=task_uuid, exc=e))\n                logger.debug('Exception rpc_run_task: {exc_info}'.format(uuid=task_uuid, exc_info=exc_info))\n                cr.rollback()\n            finally:\n                with registry(self._cr.dbname).cursor() as result_cr:\n                    env = api.Environment(result_cr, self._uid, {})\n                    task.with_env(env).write(vals)\n                response = (vals.get('state'), result)\n                return response"}
{"input": "def _make_new_fasta_no_multi_match_lines(self, scrapped_fasta_as_list):\n        new_scrapped_fasta = []\n        for i in range(0, len(scrapped_fasta_as_list), 2):\n            if not 'multipleMatches' in scrapped_fasta_as_list[i]:\n                new_scrapped_fasta.extend([scrapped_fasta_as_list[i], scrapped_fasta_as_list[i + 1]])\n        return new_scrapped_fasta", "output": "def _make_new_fasta_no_multi_match_lines(self, scrapped_fasta_as_list):\n        new_scrapped_fasta = []\n        for i in range(0, len(scrapped_fasta_as_list), 2):\n            if not 'multipleMatches' in scrapped_fasta_as_list[i] and len(scrapped_fasta_as_list[i + 1]) > 1:\n                new_scrapped_fasta.extend([scrapped_fasta_as_list[i], scrapped_fasta_as_list[i + 1]])\n        return new_scrapped_fasta"}
{"input": "def execute_payment():\n    req = request.json\n\n    try:\n        validate_execute_request(req)\n    except Exception:\n        traceback.print_exc(file=sys.stderr)\n        raise\n\n    if int(request.args.get(\"validate_only\", 0)) and app.config[\"APP_MODE\"] == \"development\":\n        return jsonify({\"success\": \"Provided JSON looks good\"})\n\n    query = Transaction.query.filter_by(payment_id=req[\"paymentID\"])\n    donation = query[0].donation\n\n    try:\n        body = {\n            \"payer_id\": req[\"payerID\"]\n        }\n\n        payment_execute_request = payments.PaymentExecuteRequest(req[\"paymentID\"])\n        payment_execute_request.request_body(body)\n\n        try:\n            payment_execute_response = pp_client.execute(payment_execute_request, parse=False)\n            r = payment_execute_response\n            if r.status_code < 200 or r.status_code > 299:\n                raise braintreehttp.http_error.HttpError(r.text, r.status_code, r.headers)\n        except IOError as ioe:\n            ioe.body = body\n            if isinstance(ioe, braintreehttp.http_error.HttpError):\n                donation.transaction.state = json.loads(ioe.message)[\"name\"]\n            else:\n                donation.transaction.state = str(ioe.__class__.__name__)\n            db.session.commit()\n            raise\n\n        jresult = r.json()\n\n        # Store payment execution into database\n        database.register_transaction(req[\"paymentID\"], req[\"payerID\"], jresult, jresult[\"state\"])\n\n        # Send confirmation email\n        mail_error = False\n        if app.config[\"APP_MAILER\"] is not None:\n            to_addrs = set()\n            if donation.reference:\n                to_addrs.update({donation.reference.email})\n\n            paypal_addr = get_paypal_email(jresult)\n            if paypal_addr:\n                to_addrs.update({paypal_addr})\n            to_addrs = list(to_addrs)\n\n            lang = \"en\"\n            if \"lang\" in req:\n                lang = req[\"lang\"]\n            elif donation.reference and donation.reference.lang:\n                lang = donation.reference.lang\n\n            if len(to_addrs) == 0:\n                warnings.warn(\"User has no email address! Donation ID: {}\".format(donation.pretty_id))\n            else:\n                try:\n                    mail.send_confirmation_email(to_addrs, donation, lang)\n                except Exception as e:\n                    mail_error = True\n                    app.log_exception(e)\n                    app.log_exception(DonationError(donation, e))\n\n        rjson = {\n            \"result\": jresult[\"state\"],\n            \"donation_id\": donation.pretty_id\n        }\n\n        if mail_error:\n            rjson[\"error\"] = {\n                \"type\": \"_MAIL_ERROR\",\n                \"message\": \"The donation has been recorded, however an error occurred while trying to send you a \"\n                           \"confirmation email. As the email is required to pick up the gadgets, you should contact \"\n                           \"us at info@poliedro-polimi.it and send us the donation ID ({}), so we can send it to you \"\n                           \"manually.\".format(\n                    donation.pretty_id)\n            }\n\n        response = jsonify(rjson)\n        response.status_code = r.status_code\n        return response\n\n    except Exception as e:\n        raise DonationError(donation, e)", "output": "def execute_payment():\n    req = request.json\n\n    try:\n        validate_execute_request(req)\n    except Exception:\n        traceback.print_exc(file=sys.stderr)\n        raise\n\n    if int(request.args.get(\"validate_only\", 0)) and app.config[\"APP_MODE\"] == \"development\":\n        return jsonify({\"success\": \"Provided JSON looks good\"})\n\n    query = Transaction.query.filter_by(payment_id=req[\"paymentID\"])\n    donation = query[0].donation\n\n    try:\n        body = {\n            \"payer_id\": req[\"payerID\"]\n        }\n\n        payment_execute_request = payments.PaymentExecuteRequest(req[\"paymentID\"])\n        payment_execute_request.request_body(body)\n\n        try:\n            payment_execute_response = pp_client.execute(payment_execute_request, parse=False)\n            r = payment_execute_response\n            if r.status_code < 200 or r.status_code > 299:\n                raise braintreehttp.http_error.HttpError(r.text, r.status_code, r.headers)\n        except IOError as ioe:\n            ioe.body = body\n            if isinstance(ioe, braintreehttp.http_error.HttpError) and json.loads(ioe.message).get(\"name\"):\n                donation.transaction.state = json.loads(ioe.message)[\"name\"]\n            else:\n                donation.transaction.state = str(ioe.__class__.__name__)\n            db.session.commit()\n            raise\n\n        jresult = r.json()\n\n        # Store payment execution into database\n        database.register_transaction(req[\"paymentID\"], req[\"payerID\"], jresult, jresult[\"state\"])\n\n        # Send confirmation email\n        mail_error = False\n        if app.config[\"APP_MAILER\"] is not None:\n            to_addrs = set()\n            if donation.reference:\n                to_addrs.update({donation.reference.email})\n\n            paypal_addr = get_paypal_email(jresult)\n            if paypal_addr:\n                to_addrs.update({paypal_addr})\n            to_addrs = list(to_addrs)\n\n            lang = \"en\"\n            if \"lang\" in req:\n                lang = req[\"lang\"]\n            elif donation.reference and donation.reference.lang:\n                lang = donation.reference.lang\n\n            if len(to_addrs) == 0:\n                warnings.warn(\"User has no email address! Donation ID: {}\".format(donation.pretty_id))\n            else:\n                try:\n                    mail.send_confirmation_email(to_addrs, donation, lang)\n                except Exception as e:\n                    mail_error = True\n                    app.log_exception(e)\n                    app.log_exception(DonationError(donation, e))\n\n        rjson = {\n            \"result\": jresult[\"state\"],\n            \"donation_id\": donation.pretty_id\n        }\n\n        if mail_error:\n            rjson[\"error\"] = {\n                \"type\": \"_MAIL_ERROR\",\n                \"message\": \"The donation has been recorded, however an error occurred while trying to send you a \"\n                           \"confirmation email. As the email is required to pick up the gadgets, you should contact \"\n                           \"us at info@poliedro-polimi.it and send us the donation ID ({}), so we can send it to you \"\n                           \"manually.\".format(\n                    donation.pretty_id)\n            }\n\n        response = jsonify(rjson)\n        response.status_code = r.status_code\n        return response\n\n    except Exception as e:\n        raise DonationError(donation, e)"}
{"input": "def connect_to_db(self):\n        '''Connects to the database'''\n        if self.db_type == DB_MYSQL:\n            try:\n                import pymysql\n            except ImportError as _err:\n                # TODO: Add python3-mysql when available or consider mysql-connector when available for python3\n                raise Exception(import_error_to_help(_module=\"pymysql\", _err_obj=_err, _pip_package=\"pymysql3\",\n                                                     _apt_package=None, _win_package=None))\n\n            _connection = pymysql.connect (host = self.db_server,\n                            db = self.db_databasename,\n                            user = self.db_username,\n                            passwd = self.db_password,\n                            )\n            \n\n        elif self.db_type == DB_POSTGRESQL:\n\n            try:\n                import postgresql.driver as pg_driver\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"postgresql\", _err_obj=_err,\n                                                     _pip_package=\"py-postgresql\",\n                                                     _apt_package=\"python3-postgresql\",\n                                                     _win_package=None,\n                                                     _import_comment=\"2014-04-16: If using apt-get, \" +\n                                                                     \"check so version is > 1.0.3-2\" +\n                                                     \" as there is a severe bug in the 1.02 version. \" +\n                                                     \"See https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=724597\"))\n\n            if self.db_port == None or self.db_port == \"\" or self.db_port == 0:\n                _port = 5432\n            else:\n                _port = self.db_port\n            _connection = pg_driver.connect(host = self.db_server,\n                                                database =  self.db_databasename, \n                                                user = self.db_username, \n                                                password = self.db_password,\n                                                port = _port)\n                            \n        elif self.db_type in [DB_SQLSERVER, DB_DB2]:\n            _connection_string = None\n            try:\n                import pyodbc\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"pyodbc\", _err_obj=_err,\n                                                     _pip_package=\"pyodbc\",\n                                                     _apt_package=\"None\",\n                                                     _win_package=None,\n                                                     _import_comment=\"2014-04-16: \" +\n                                                                     \"No apt package (python3-pyodbc)\"+\n                                                                     \" available at this time.\"))\n            import platform\n\n            #TODO: Investigate if there is any more adapting needed, platform.release() can also be used.\n\n            if self.db_type == DB_SQLSERVER:\n                if platform.system().lower() == 'linux':\n                    _connection_string = \"DRIVER={FreeTDS};SERVER=\" + self.db_server + \";DATABASE=\" + \\\n                                         self.db_databasename +\";TDS_VERSION=8.0;UID=\" + self.db_username + \";PWD=\" + \\\n                                         self.db_password + \";PORT=\"+str(self.db_port) + \";Trusted_Connection=no;\"\n                elif platform.system().lower() == 'windows':\n                    _connection_string = \"Driver={SQL Server};Server=\" + self.db_server + \";DATABASE=\" + \\\n                                         self.db_databasename +\";UID=\" + self.db_username + \";PWD=\" + self.db_password +\\\n                                         \";PORT=\"+str(self.db_port) + \";Trusted_Connection=no\"\n                else:\n                    raise Exception(\"connect_to_db: ODBC connections on \" + platform.system() + \" not supported yet.\")\n\n\n            elif self.db_type == DB_DB2:\n\n                if platform.system().lower() == 'linux':\n                    drivername = \"DB2\"\n                elif platform.system().lower() == 'windows':\n                    drivername = \"{IBM DATA SERVER DRIVER for ODBC - C:/PROGRA~1/IBM}\"\n                else:\n                    raise Exception(\"connect_to_db: DB2 connections on \" + platform.system() + \" not supported yet.\")\n\n                # DSN-less?{IBM DB2 ODBC DRIVER} ?? http://www.webmasterworld.com/forum88/4434.htm\n                _connection_string =  \"Driver=\" + drivername + \";Database=\" + self.db_databasename +\";hostname=\" + \\\n                                      self.db_server + \";port=\"+str(self.db_port) + \";protocol=TCPIP; uid=\" + \\\n                                      self.db_username + \"; pwd=\" + self.db_password\n                #_connection_string = \"DSN=\" + self.db_server + \";UID=\" + self.db_username + \";PWD=\" + self.db_password\n                _connection = pyodbc.connect(_connection_string, autocommit=self.db_autocommit)\n\n\n                # DSN-less?{IBM DB2 ODBC DRIVER} ?? http://www.webmasterworld.com/forum88/4434.htm\n                _connection_string =  \"Driver=\" + drivername + \";Database=\" + self.db_databasename +\";hostname=\" + \\\n                                      self.db_server + \";port=\"+str(self.db_port) + \";protocol=TCPIP; uid=\" + \\\n                                      self.db_username + \"; pwd=\" + self.db_password\n                #_connection_string = \"DSN=\" + self.db_server + \";UID=\" + self.db_username + \";PWD=\" + self.db_password\n\n            print(\"Connect to database using connection string:  \" + _connection_string)\n            _connection = pyodbc.connect(_connection_string, autocommit=self.db_autocommit)\n        \n        # cx_Oracle in python 3.X not checked yet.\n        elif self.db_type == DB_ORACLE:\n            try:\n                import cx_Oracle\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"cx_Oracle\", _err_obj=_err,\n                                                     _pip_package=\"cx_Oracle\",\n                                                     _apt_package=\"None\",\n                                                     _win_package=\"Download and install binary .msi package from \" +\n                                                                  \"http://cx-oracle.sourceforge.net/ and install.\",\n                                                     _import_comment=\"2014-04-16: No python3-pyodbc available at\" +\n                                                                     \" build time.\"))\n\n            _connection_string = self.db_username + '/' +  self.db_password + '@' + self.db_server + ':' + \\\n                                 str(self.db_port) + '/' + self.db_instance\n            print(\"Connect to database using connection string:  \" + _connection_string)\n            _connection = cx_Oracle.connect(_connection_string)\n            _connection.autocommit=self.db_autocommit\n                  \n        else:\n            raise Exception(\"connect_to_db: Invalid database type.\")              \n      \n        \n        self.db_connection = _connection\n        \n        if self.on_connect:\n            self.on_connect() \n        self.connected = True\n            \n        return _connection", "output": "def connect_to_db(self):\n        '''Connects to the database'''\n        if self.db_type == DB_MYSQL:\n            try:\n                import pymysql\n            except ImportError as _err:\n                # TODO: Add python3-mysql when available or consider mysql-connector when available for python3\n                raise Exception(import_error_to_help(_module=\"pymysql\", _err_obj=_err, _pip_package=\"pymysql3\",\n                                                     _apt_package=None, _win_package=None))\n\n            _connection = pymysql.connect (host = self.db_server,\n                            db = self.db_databasename,\n                            user = self.db_username,\n                            passwd = self.db_password,\n                            )\n            \n\n        elif self.db_type == DB_POSTGRESQL:\n\n            try:\n                import postgresql.driver as pg_driver\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"postgresql\", _err_obj=_err,\n                                                     _pip_package=\"py-postgresql\",\n                                                     _apt_package=\"python3-postgresql\",\n                                                     _win_package=None,\n                                                     _import_comment=\"2014-04-16: If using apt-get, \" +\n                                                                     \"check so version is > 1.0.3-2\" +\n                                                     \" as there is a severe bug in the 1.02 version. \" +\n                                                     \"See https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=724597\"))\n\n            if self.db_port == None or self.db_port == \"\" or self.db_port == 0:\n                _port = 5432\n            else:\n                _port = self.db_port\n            _connection = pg_driver.connect(host = self.db_server,\n                                                database =  self.db_databasename, \n                                                user = self.db_username, \n                                                password = self.db_password,\n                                                port = _port)\n                            \n        elif self.db_type in [DB_SQLSERVER, DB_DB2]:\n            _connection_string = None\n            try:\n                import pyodbc\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"pyodbc\", _err_obj=_err,\n                                                     _pip_package=\"pyodbc\",\n                                                     _apt_package=\"None\",\n                                                     _win_package=None,\n                                                     _import_comment=\"Linux 2014-04-16: \" +\n                                                                     \"No apt package (python3-pyodbc)\"+\n                                                                     \" available at this time.\"))\n            import platform\n\n            #TODO: Investigate if there is any more adapting needed, platform.release() can also be used.\n\n            if self.db_type == DB_SQLSERVER:\n                if platform.system().lower() == 'linux':\n                    _connection_string = \"DRIVER={FreeTDS};SERVER=\" + self.db_server + \";DATABASE=\" + \\\n                                         self.db_databasename +\";TDS_VERSION=8.0;UID=\" + self.db_username + \";PWD=\" + \\\n                                         self.db_password + \";PORT=\"+str(self.db_port) + \";Trusted_Connection=no;\"\n                elif platform.system().lower() == 'windows':\n                    _connection_string = \"Driver={SQL Server};Server=\" + self.db_server + \";DATABASE=\" + \\\n                                         self.db_databasename +\";UID=\" + self.db_username + \";PWD=\" + self.db_password +\\\n                                         \";PORT=\"+str(self.db_port) + \";Trusted_Connection=no\"\n                else:\n                    raise Exception(\"connect_to_db: ODBC connections on \" + platform.system() + \" not supported yet.\")\n\n\n            elif self.db_type == DB_DB2:\n\n                if platform.system().lower() == 'linux':\n                    drivername = \"DB2\"\n                elif platform.system().lower() == 'windows':\n                    drivername = \"{IBM DATA SERVER DRIVER for ODBC - C:/PROGRA~1/IBM}\"\n                else:\n                    raise Exception(\"connect_to_db: DB2 connections on \" + platform.system() + \" not supported yet.\")\n\n                # DSN-less?{IBM DB2 ODBC DRIVER} ?? http://www.webmasterworld.com/forum88/4434.htm\n                _connection_string =  \"Driver=\" + drivername + \";Database=\" + self.db_databasename +\";hostname=\" + \\\n                                      self.db_server + \";port=\"+str(self.db_port) + \";protocol=TCPIP; uid=\" + \\\n                                      self.db_username + \"; pwd=\" + self.db_password\n                #_connection_string = \"DSN=\" + self.db_server + \";UID=\" + self.db_username + \";PWD=\" + self.db_password\n                _connection = pyodbc.connect(_connection_string, autocommit=self.db_autocommit)\n\n\n                # DSN-less?{IBM DB2 ODBC DRIVER} ?? http://www.webmasterworld.com/forum88/4434.htm\n                _connection_string =  \"Driver=\" + drivername + \";Database=\" + self.db_databasename +\";hostname=\" + \\\n                                      self.db_server + \";port=\"+str(self.db_port) + \";protocol=TCPIP; uid=\" + \\\n                                      self.db_username + \"; pwd=\" + self.db_password\n                #_connection_string = \"DSN=\" + self.db_server + \";UID=\" + self.db_username + \";PWD=\" + self.db_password\n\n            print(\"Connect to database using connection string:  \" + _connection_string)\n            _connection = pyodbc.connect(_connection_string, autocommit=self.db_autocommit)\n        \n        # cx_Oracle in python 3.X not checked yet.\n        elif self.db_type == DB_ORACLE:\n            try:\n                import cx_Oracle\n            except ImportError as _err:\n                raise Exception(import_error_to_help(_module=\"cx_Oracle\", _err_obj=_err,\n                                                     _pip_package=\"cx_Oracle\",\n                                                     _apt_package=\"None\",\n                                                     _win_package=\"Download and install binary .msi package from \" +\n                                                                  \"http://cx-oracle.sourceforge.net/ and install.\",\n                                                     _import_comment=\"2014-04-16: No python3-pyodbc available at\" +\n                                                                     \" build time.\"))\n\n            _connection_string = self.db_username + '/' +  self.db_password + '@' + self.db_server + ':' + \\\n                                 str(self.db_port) + '/' + self.db_instance\n            print(\"Connect to database using connection string:  \" + _connection_string)\n            _connection = cx_Oracle.connect(_connection_string)\n            _connection.autocommit=self.db_autocommit\n                  \n        else:\n            raise Exception(\"connect_to_db: Invalid database type.\")              \n      \n        \n        self.db_connection = _connection\n        \n        if self.on_connect:\n            self.on_connect() \n        self.connected = True\n            \n        return _connection"}
{"input": "def get_pr(repo, owner, sha):\n    url = (\"https://api.github.com/search/issues?q=type:pr+is:merged+repo:%s/%s+%s\" %\n           (repo, owner, sha))\n    try:\n        resp = urllib2.urlopen(url)\n        body = resp.read()\n    except Exception as e:\n        logger.error(e)\n        return None\n\n    if resp.code != 200:\n        logger.error(\"Got HTTP status %s. Response:\" % resp.code)\n        logger.error(body)\n        return None\n\n    try:\n        data = json.loads(body)\n    except ValueError:\n        logger.error(\"Failed to read response as JSON:\")\n        logger.error(body)\n        return None\n\n    items = data[\"items\"]\n    if len(items) == 0:\n        logger.error(\"No PR found for %s\" % sha)\n        return None\n    if len(items) > 1:\n        logger.warning(\"Found multiple PRs for %s\" % sha)\n\n    pr = items[0]\n\n    return pr[\"number\"]", "output": "def get_pr(repo, owner, sha):\n    url = (\"https://api.github.com/search/issues?q=type:pr+is:merged+repo:%s/%s+sha:%s\" %\n           (repo, owner, sha))\n    try:\n        resp = urllib2.urlopen(url)\n        body = resp.read()\n    except Exception as e:\n        logger.error(e)\n        return None\n\n    if resp.code != 200:\n        logger.error(\"Got HTTP status %s. Response:\" % resp.code)\n        logger.error(body)\n        return None\n\n    try:\n        data = json.loads(body)\n    except ValueError:\n        logger.error(\"Failed to read response as JSON:\")\n        logger.error(body)\n        return None\n\n    items = data[\"items\"]\n    if len(items) == 0:\n        logger.error(\"No PR found for %s\" % sha)\n        return None\n    if len(items) > 1:\n        logger.warning(\"Found multiple PRs for %s\" % sha)\n\n    pr = items[0]\n\n    return pr[\"number\"]"}
{"input": "def name_is_webdriver(self):\n        \"\"\"Check if the file name matches the conditions for the file to\n        be a webdriver spec test file\"\"\"\n        # wdspec tests are in subdirectories of /webdriver excluding __init__.py\n        # files.\n        rel_dir_tree = self.rel_path.split(os.path.sep)\n        return (rel_dir_tree[0] == \"webdriver\" and\n                len(rel_dir_tree) > 1 and\n                self.filename != \"__init__.py\" and\n                fnmatch(self.filename, wd_pattern))", "output": "def name_is_webdriver(self):\n        \"\"\"Check if the file name matches the conditions for the file to\n        be a webdriver spec test file\"\"\"\n        # wdspec tests are in subdirectories of /webdriver excluding __init__.py\n        # files.\n        rel_dir_tree = self.rel_path.split(os.path.sep)\n        return (rel_dir_tree[0] == \"webdriver\" and\n                len(rel_dir_tree) > 1 and\n                self.filename not in (\"__init__.py\", \"conftest.py\") and\n                fnmatch(self.filename, wd_pattern))"}
{"input": "def eval(cls, z):\n        if z is S.NaN:\n            return S.NaN\n        elif z is S.NegativeOne:\n            return S.NegativeInfinity\n        elif z is S.Zero:\n            return S.Zero\n        elif z is S.One:\n            return S.Infinity\n\n        if (isinstance(z, erf)) and z.args[0].is_real:\n            return z.args[0]\n\n        # Try to pull out factors of -1\n        nz = z.extract_multiplicatively(-1)\n        if nz is not None and ((isinstance(nz, erf)) and (nz.args[0]).is_real):\n            return -nz.args[0]", "output": "def eval(cls, z):\n        if z is S.NaN:\n            return S.NaN\n        elif z is S.NegativeOne:\n            return S.NegativeInfinity\n        elif z is S.Zero:\n            return S.Zero\n        elif z is S.One:\n            return S.Infinity\n\n        if isinstance(z, erf) and z.args[0].is_real:\n            return z.args[0]\n\n        # Try to pull out factors of -1\n        nz = z.extract_multiplicatively(-1)\n        if nz is not None and ((isinstance(nz, erf)) and (nz.args[0]).is_real):\n            return -nz.args[0]"}
{"input": "def upgrade(migrate_engine):\n    # Upgrade operations go here. Don't create your own engine; bind migrate_engine\n    # to your metadata \n    meta = MetaData(migrate_engine)\n    \n    # change db defaults\n    sql_update_db = \"alter database `changebyus_demo` DEFAULT CHARACTER SET = 'utf8' DEFAULT COLLATE = 'utf8_general_ci'\"\n    migrate_engine.execute(sql_update_db)\n    \n    # populate ALL tables in metadata\n    meta.reflect(migrate_engine)\n    \n    #change table defaults\n    tbls = meta.tables.keys()\n    \n    for item in tbls:\n        sql_update_table = \"alter table `%s` DEFAULT CHARACTER SET 'utf8' DEFAULT COLLATE 'utf8_general_ci'\" % item\n        migrate_engine.execute(sql_update_table)", "output": "def upgrade(migrate_engine):\n    # Upgrade operations go here. Don't create your own engine; bind migrate_engine\n    # to your metadata \n    meta = MetaData(migrate_engine)\n    \n    # change db defaults\n    sql_update_db = \"alter database `changebyus_demo` DEFAULT CHARACTER SET = 'utf8' DEFAULT COLLATE = 'utf8_general_ci'\"\n    migrate_engine.execute(sql_update_db)\n    \n    # populate ALL tables in metadata\n    meta.reflect(migrate_engine)\n    \n    #change table defaults\n    tbls = meta.tables.keys()\n    \n    for item in tbls:\n        sql_update_table = \"alter table `%s` CHARACTER SET 'utf8' COLLATE 'utf8_general_ci'\" % item\n        migrate_engine.execute(sql_update_table)"}
{"input": "def kill(pid, signal):\n    \"\"\"Kill a process with a signal.\"\"\"\n    for chld in reversed(_fetch_child(pid)[1:]):\n        try:\n            os.kill(chld, signal)\n        except OSError as error:\n            message = ('failed to send signal to process %d '\n                       'with error message: %s') % (chld, error)\n            warnings.showwarning(message, ResourceWarning, __file__, 34)", "output": "def kill(pid, signal):\n    \"\"\"Kill a process with a signal.\"\"\"\n    for chld in reversed(_fetch_child(pid)[1:]):\n        try:\n            os.kill(chld, signal)\n        except OSError as error:\n            message = ('failed to send signal to process %d '\n                       'with error message: %s') % (chld, error)\n            warnings.showwarning(message, ResourceWarning, __file__, 33)"}
{"input": "def __init__(self, project, start=None, starts=None, max_concurrency=None, max_active=None, pickle_paths=None, find=(), avoid=(), restrict=(), min_depth=0, max_depth=None, max_repeats=10, num_find=1, num_avoid=None, num_deviate=1, num_loop=None, cut_lost=None):\n\t\t'''\n\t\tExplores the path space until a block containing a specified address is\n\t\tfound. Parameters (other than for Surveyor):\n\n\t\t@param find: a tuple containing the addresses to search for\n\t\t@param avoid: a tuple containing the addresses to avoid\n\t\t@param restrict: a tuple containing the addresses to restrict the\n\t\t\t\t\t\t analysis to (i.e., avoid all others)\n\t\t@param min_depth: the minimum number of SimRuns in the resulting path\n\t\t@param max_depth: the maximum number of SimRuns in the resulting path\n\n\t\t@param num_find: the minimum number of paths to find (default: 1)\n\t\t@param num_avoid: the minimum number of paths to avoid\n\t\t\t\t\t\t  (default: infinite)\n\t\t@param num_deviate: the minimum number of paths to deviate\n\t\t\t\t\t\t\t(default: infinite)\n\t\t@param num_loop: the minimum number of paths to loop\n\t\t\t\t\t\t (default: infinite)\n\t\t@param cut_lost: cut any paths that have no chance of going to the target\n\t\t'''\n\t\tSurveyor.__init__(self, project, start=start, starts=starts, max_concurrency=max_concurrency, max_active=max_active, pickle_paths=pickle_paths)\n\n\t\t# initialize the counter\n\t\tself._instruction_counter = collections.Counter()\n\n\t\tself._find = self._arg_to_set(find)\n\t\tself._avoid = self._arg_to_set(avoid)\n\t\tself._restrict = self._arg_to_set(restrict)\n\t\tself._max_repeats = max_repeats\n\t\tself._max_depth = max_depth\n\t\tself._min_depth = min_depth\n\n\t\tself.found = [ ]\n\t\tself.avoided = [ ]\n\t\tself.deviating = [ ]\n\t\tself.looping = [ ]\n\t\tself.lost = [ ]\n\n\t\tself._num_find = num_find\n\t\tself._num_avoid = num_avoid\n\t\tself._num_deviate = num_deviate\n\t\tself._num_loop = num_loop\n\n\t\tself._cut_lost = len(self._find) == 0 and self._project._cfg is not None if cut_lost is None else cut_lost\n\n\t\tif self._cut_lost and self._project._cfg is None:\n\t\t\traise AngrSurveyorError(\"cut_lost requires a CFG\")\n\t\tif self._cut_lost:\n\t\t\tgood_find = set()\n\t\t\tfor f in self._find:\n\t\t\t\tif self._project._cfg.get_any_irsb(f) is None:\n\t\t\t\t\tl.warning(\"No node 0x%x in CFG. This will be automatically cut.\", f)\n\t\t\t\telse:\n\t\t\t\t\tgood_find.add(f)\n\t\t\tself._find = good_find", "output": "def __init__(self, project, start=None, starts=None, max_concurrency=None, max_active=None, pickle_paths=None, find=(), avoid=(), restrict=(), min_depth=0, max_depth=None, max_repeats=10000000, num_find=1, num_avoid=None, num_deviate=1, num_loop=None, cut_lost=None):\n\t\t'''\n\t\tExplores the path space until a block containing a specified address is\n\t\tfound. Parameters (other than for Surveyor):\n\n\t\t@param find: a tuple containing the addresses to search for\n\t\t@param avoid: a tuple containing the addresses to avoid\n\t\t@param restrict: a tuple containing the addresses to restrict the\n\t\t\t\t\t\t analysis to (i.e., avoid all others)\n\t\t@param min_depth: the minimum number of SimRuns in the resulting path\n\t\t@param max_depth: the maximum number of SimRuns in the resulting path\n\n\t\t@param num_find: the minimum number of paths to find (default: 1)\n\t\t@param num_avoid: the minimum number of paths to avoid\n\t\t\t\t\t\t  (default: infinite)\n\t\t@param num_deviate: the minimum number of paths to deviate\n\t\t\t\t\t\t\t(default: infinite)\n\t\t@param num_loop: the minimum number of paths to loop\n\t\t\t\t\t\t (default: infinite)\n\t\t@param cut_lost: cut any paths that have no chance of going to the target\n\t\t'''\n\t\tSurveyor.__init__(self, project, start=start, starts=starts, max_concurrency=max_concurrency, max_active=max_active, pickle_paths=pickle_paths)\n\n\t\t# initialize the counter\n\t\tself._instruction_counter = collections.Counter()\n\n\t\tself._find = self._arg_to_set(find)\n\t\tself._avoid = self._arg_to_set(avoid)\n\t\tself._restrict = self._arg_to_set(restrict)\n\t\tself._max_repeats = max_repeats\n\t\tself._max_depth = max_depth\n\t\tself._min_depth = min_depth\n\n\t\tself.found = [ ]\n\t\tself.avoided = [ ]\n\t\tself.deviating = [ ]\n\t\tself.looping = [ ]\n\t\tself.lost = [ ]\n\n\t\tself._num_find = num_find\n\t\tself._num_avoid = num_avoid\n\t\tself._num_deviate = num_deviate\n\t\tself._num_loop = num_loop\n\n\t\tself._cut_lost = len(self._find) == 0 and self._project._cfg is not None if cut_lost is None else cut_lost\n\n\t\tif self._cut_lost and self._project._cfg is None:\n\t\t\traise AngrSurveyorError(\"cut_lost requires a CFG\")\n\t\tif self._cut_lost:\n\t\t\tgood_find = set()\n\t\t\tfor f in self._find:\n\t\t\t\tif self._project._cfg.get_any_irsb(f) is None:\n\t\t\t\t\tl.warning(\"No node 0x%x in CFG. This will be automatically cut.\", f)\n\t\t\t\telse:\n\t\t\t\t\tgood_find.add(f)\n\t\t\tself._find = good_find"}
{"input": "def _generate_ca(self):\n        # Generate key\n        self.key = OpenSSL.crypto.PKey()\n        self.key.generate_key(OpenSSL.crypto.TYPE_RSA, 2048)\n\n        # Generate certificate\n        self.cert = OpenSSL.crypto.X509()\n        self.cert.set_version(3)\n        # avoid sec_error_reused_issuer_and_serial\n        self.cert.set_serial_number(random.randint(0,2**64-1))\n        self.cert.get_subject().CN = 'Warcprox CA on {}'.format(socket.gethostname())[:64]\n        self.cert.gmtime_adj_notBefore(0)                # now\n        self.cert.gmtime_adj_notAfter(100*365*24*60*60)  # 100 yrs in future\n        self.cert.set_issuer(self.cert.get_subject())\n        self.cert.set_pubkey(self.key)\n        self.cert.add_extensions([\n            OpenSSL.crypto.X509Extension(b\"basicConstraints\", True, b\"CA:TRUE, pathlen:0\"),\n            OpenSSL.crypto.X509Extension(b\"keyUsage\", True, b\"keyCertSign, cRLSign\"),\n            OpenSSL.crypto.X509Extension(b\"subjectKeyIdentifier\", False, b\"hash\", subject=self.cert),\n            ])\n        self.cert.sign(self.key, \"sha1\")\n\n        with open(self.ca_file, 'wb+') as f:\n            f.write(OpenSSL.crypto.dump_privatekey(OpenSSL.SSL.FILETYPE_PEM, self.key))\n            f.write(OpenSSL.crypto.dump_certificate(OpenSSL.SSL.FILETYPE_PEM, self.cert))\n\n        self.logger.info('generated CA key+cert and wrote to {}'.format(self.ca_file))", "output": "def _generate_ca(self):\n        # Generate key\n        self.key = OpenSSL.crypto.PKey()\n        self.key.generate_key(OpenSSL.crypto.TYPE_RSA, 2048)\n\n        # Generate certificate\n        self.cert = OpenSSL.crypto.X509()\n        self.cert.set_version(2)\n        # avoid sec_error_reused_issuer_and_serial\n        self.cert.set_serial_number(random.randint(0,2**64-1))\n        self.cert.get_subject().CN = 'Warcprox CA on {}'.format(socket.gethostname())[:64]\n        self.cert.gmtime_adj_notBefore(0)                # now\n        self.cert.gmtime_adj_notAfter(100*365*24*60*60)  # 100 yrs in future\n        self.cert.set_issuer(self.cert.get_subject())\n        self.cert.set_pubkey(self.key)\n        self.cert.add_extensions([\n            OpenSSL.crypto.X509Extension(b\"basicConstraints\", True, b\"CA:TRUE, pathlen:0\"),\n            OpenSSL.crypto.X509Extension(b\"keyUsage\", True, b\"keyCertSign, cRLSign\"),\n            OpenSSL.crypto.X509Extension(b\"subjectKeyIdentifier\", False, b\"hash\", subject=self.cert),\n            ])\n        self.cert.sign(self.key, \"sha1\")\n\n        with open(self.ca_file, 'wb+') as f:\n            f.write(OpenSSL.crypto.dump_privatekey(OpenSSL.SSL.FILETYPE_PEM, self.key))\n            f.write(OpenSSL.crypto.dump_certificate(OpenSSL.SSL.FILETYPE_PEM, self.cert))\n\n        self.logger.info('generated CA key+cert and wrote to {}'.format(self.ca_file))"}
{"input": "def _make_table(start=-200000, end=100000):\n    G = EcGroup(nid=713)\n    g = G.generator()\n    o = G.order()\n\n    i_table = {}\n    n_table = {}\n    ix = start * g\n\n    for i in range(start, end):\n        i_table[str(ix)] = str(i)\n        n_table[str((o + i) % o)] = str(ix)\n        ix = ix + g\n        #print type(ix)\n        #print type(ix.export())\n\n    return i_table, n_table", "output": "def _make_table(start=-200000, end=2000):\n    G = EcGroup(nid=713)\n    g = G.generator()\n    o = G.order()\n\n    i_table = {}\n    n_table = {}\n    ix = start * g\n\n    for i in range(start, end):\n        i_table[str(ix)] = str(i)\n        n_table[str((o + i) % o)] = str(ix)\n        ix = ix + g\n        #print type(ix)\n        #print type(ix.export())\n\n    return i_table, n_table"}
{"input": "def __init__(self):\n        super(SpotifySkill, self).__init__()\n        self.index = 0\n        self.spotify = None\n        self.process = None\n        self.device_name = None\n        self.dev_id = None\n        self.idle_count = 0\n        self.ducking = False\n        self.mouth_text = None\n\n        self.__device_list = None\n        self.__devices_fetched = 0\n        self.OAUTH_ID = 1\n        self.DEFAULT_VOLUME = 65\n        self._playlists = None", "output": "def __init__(self):\n        super(SpotifySkill, self).__init__()\n        self.index = 0\n        self.spotify = None\n        self.process = None\n        self.device_name = None\n        self.dev_id = None\n        self.idle_count = 0\n        self.ducking = False\n        self.mouth_text = None\n\n        self.__device_list = None\n        self.__devices_fetched = 0\n        self.OAUTH_ID = 1\n        self.DEFAULT_VOLUME = 80\n        self._playlists = None"}
{"input": "def as_docker_auths(self):\n        '''\n        returns a representation of the credentials from this registry-cfg as \"docker-auths\",\n        which can be used to populate a docker-cfg file ($HOME/.docker/config.json) below the\n        `auths` attr\n        '''\n        auth_str = f'{self.credentials().username()}:{self.credentials().passwd()}'\n        auth_str = base64.b64encode(auth_str.encode('utf-8'))\n\n        auths = {\n            host: {'auth': auth_str} for host in self.image_reference_prefixes()\n        }\n\n        return auths", "output": "def as_docker_auths(self):\n        '''\n        returns a representation of the credentials from this registry-cfg as \"docker-auths\",\n        which can be used to populate a docker-cfg file ($HOME/.docker/config.json) below the\n        `auths` attr\n        '''\n        auth_str = f'{self.credentials().username()}:{self.credentials().passwd()}'\n        auth_str = base64.b64encode(auth_str.encode('utf-8')).decode('utf-8')\n\n        auths = {\n            host: {'auth': auth_str} for host in self.image_reference_prefixes()\n        }\n\n        return auths"}
{"input": "def find_title(url):\n    \"\"\"Return the title for the given URL.\"\"\"\n    content = web.get(url)\n    # Some cleanup that I don't really grok, but was in the original, so\n    # we'll keep it (with the compiled regexes made global) for now.\n    content = title_tag_data.sub(r'<\\1title>', content)\n    content = quoted_title.sub('', content)\n\n    start = content.find('<title>')\n    end = content.find('</title>')\n    if start == -1 or end == -1:\n        return\n    title = content[start + 7:end]\n    title = title.strip()[:200]\n\n    def get_unicode_entity(match):\n        entity = match.group()\n        if entity.startswith('&#x'):\n            cp = int(entity[3:-1], 16)\n        elif entity.startswith('&#'):\n            cp = int(entity[2:-1])\n        else:\n            cp = name2codepoint[entity[1:-1]]\n        return unichr(cp)\n\n    title = r_entity.sub(get_unicode_entity, title)\n\n    title = ' '.join(title.split())  # cleanly remove multiple spaces\n\n    # More cryptic regex substitutions. This one looks to be myano's invention.\n    title = re_dcc.sub('', title)\n\n    return title or None", "output": "def find_title(url):\n    \"\"\"Return the title for the given URL.\"\"\"\n    content = web.get(url).decode('utf8')\n    # Some cleanup that I don't really grok, but was in the original, so\n    # we'll keep it (with the compiled regexes made global) for now.\n    content = title_tag_data.sub(r'<\\1title>', content)\n    content = quoted_title.sub('', content)\n\n    start = content.find('<title>')\n    end = content.find('</title>')\n    if start == -1 or end == -1:\n        return\n    title = content[start + 7:end]\n    title = title.strip()[:200]\n\n    def get_unicode_entity(match):\n        entity = match.group()\n        if entity.startswith('&#x'):\n            cp = int(entity[3:-1], 16)\n        elif entity.startswith('&#'):\n            cp = int(entity[2:-1])\n        else:\n            cp = name2codepoint[entity[1:-1]]\n        return unichr(cp)\n\n    title = r_entity.sub(get_unicode_entity, title)\n\n    title = ' '.join(title.split())  # cleanly remove multiple spaces\n\n    # More cryptic regex substitutions. This one looks to be myano's invention.\n    title = re_dcc.sub('', title)\n\n    return title or None"}
{"input": "def _build_context(self):\n        ctx = self._context.copy()\n\n        versions = ctx[\"versions\"]\n        for name, mod in sys.modules.items():\n            if name.startswith(\"_\"):\n                continue\n            if hasattr(mod, \"__version__\"):\n                versions[name] = mod.__version__\n\n        return ctx", "output": "def _build_context(self):\n        ctx = self._context.copy()\n\n        versions = ctx[\"versions\"]\n        for name, mod in sys.modules.copy().items():\n            if name.startswith(\"_\"):\n                continue\n            if hasattr(mod, \"__version__\"):\n                versions[name] = mod.__version__\n\n        return ctx"}
{"input": "def render(self, retries: int = 8) -> None:\n        \"\"\"Reloads the response in Chromium, and replaces HTML content\n        with an updated version, with JavaScript executed.\n\n        Warning: the first time you run this method, it will download\n        Chromium into your home directory (``~/.pyppeteer``).\n        \"\"\"\n        async def _async_render(url: str):\n            try:\n                browser = pyppeteer.launch(headless=True)\n                page = await browser.newPage()\n\n                # Load the given page (GET request, obviously.)\n                await page.goto(url)\n\n                # Return the content of the page, JavaScript evaluated.\n                return await page.content()\n            except TimeoutError:\n                return None\n\n        loop = asyncio.get_event_loop()\n        content = None\n\n        for i in range(retries):\n            if not content:\n                try:\n                    content = loop.run_until_complete(_async_render(url=self.url))\n                except TimeoutError:\n                    pass\n\n        html = HTML(url=self.url, html=content, default_encoding=DEFAULT_ENCODING)\n        self.__dict__.update(html.__dict__)", "output": "def render(self, retries: int = 8) -> None:\n        \"\"\"Reloads the response in Chromium, and replaces HTML content\n        with an updated version, with JavaScript executed.\n\n        Warning: the first time you run this method, it will download\n        Chromium into your home directory (``~/.pyppeteer``).\n        \"\"\"\n        async def _async_render(url: str):\n            try:\n                browser = pyppeteer.launch(headless=True)\n                page = await browser.newPage()\n\n                # Load the given page (GET request, obviously.)\n                await page.goto(url)\n\n                # Return the content of the page, JavaScript evaluated.\n                return await page.content()\n            except TimeoutError:\n                return None\n\n        loop = asyncio.get_event_loop()\n        content = None\n\n        for i in range(retries):\n            if not content:\n                try:\n                    content = loop.run_until_complete(_async_render(url=self.url))\n                except TimeoutError:\n                    pass\n\n        html = HTML(url=self.url, html=content.encode(DEFAULT_ENCODING), default_encoding=DEFAULT_ENCODING)\n        self.__dict__.update(html.__dict__)"}
{"input": "def run(self):\n        if 'alt' in self.options and self.ignore_alt:\n            LOGGER.warning(\"Graphviz: the :alt: option is ignored, it's better to set the title of your graph.\")\n        if self.arguments:\n            if self.content:\n                LOGGER.warning(\"Graphviz: this directive can't have both content and a filename argument. Ignoring content.\")\n            f_name = self.arguments[0]\n            # TODO: be smart about where exactly that file is located\n            with open(f_name, 'rb') as inf:\n                data = inf.read().decode('utf-8')\n        else:\n            data = '\\n'.join(self.content)\n        node_list = []\n        try:\n            p = Popen([self.dot_path, '-Tsvg'], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            svg_data, errors = p.communicate(input=data)\n            code = p.wait()\n            if code:  # Some error\n                document = self.state.document\n                return [document.reporter.error(\n                        'Error processing graph: {0}'.format(errors), line=self.lineno)]\n            if self.embed_graph:  # SVG embedded in the HTML\n                if 'inline' in self.options:\n                    svg_data = '<span class=\"graphviz\">{0}</span>'.format(svg_data)\n                else:\n                    svg_data = '<p class=\"graphviz\">{0}</p>'.format(svg_data)\n\n            else:  # External SVG file\n                # TODO: there is no reason why this branch needs to be a raw\n                # directive. It could generate regular docutils nodes and\n                # be useful for any writer.\n                makedirs(self.output_folder)\n                f_name = hashlib.md5(svg_data).hexdigest() + '.svg'\n                img_path = self.graph_path + f_name\n                f_path = os.path.join(self.output_folder, f_name)\n                alt = self.options.get('alt', '')\n                with open(f_path, 'wb+') as outf:\n                    outf.write(svg_data)\n                    self.state.document.settings.record_dependencies.add(f_path)\n                if 'inline' in self.options:\n                    svg_data = '<span class=\"graphviz\"><img src=\"{0}\" alt=\"{1}\"></span>'.format(img_path, alt)\n                else:\n                    svg_data = '<p class=\"graphviz\"><img src=\"{0}\" alt=\"{1}\"></p>'.format(img_path, alt)\n\n            node_list.append(nodes.raw('', svg_data, format='html'))\n            if 'caption' in self.options and 'inline' not in self.options:\n                node_list.append(\n                    nodes.raw('', '<p class=\"caption\">{0}</p>'.format(self.options['caption']),\n                              format='html'))\n            return node_list\n        except OSError:\n            LOGGER.error(\"Can't execute 'dot'\")\n            raise", "output": "def run(self):\n        if 'alt' in self.options and self.ignore_alt:\n            LOGGER.warning(\"Graphviz: the :alt: option is ignored, it's better to set the title of your graph.\")\n        if self.arguments:\n            if self.content:\n                LOGGER.warning(\"Graphviz: this directive can't have both content and a filename argument. Ignoring content.\")\n            f_name = self.arguments[0]\n            # TODO: be smart about where exactly that file is located\n            with open(f_name, 'rb') as inf:\n                data = inf.read().decode('utf-8')\n        else:\n            data = '\\n'.join(self.content)\n        node_list = []\n        try:\n            p = Popen([self.dot_path, '-Tsvg'], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            svg_data, errors = p.communicate(input=data.encode('utf8'))\n            code = p.wait()\n            if code:  # Some error\n                document = self.state.document\n                return [document.reporter.error(\n                        'Error processing graph: {0}'.format(errors), line=self.lineno)]\n            if self.embed_graph:  # SVG embedded in the HTML\n                if 'inline' in self.options:\n                    svg_data = '<span class=\"graphviz\">{0}</span>'.format(svg_data)\n                else:\n                    svg_data = '<p class=\"graphviz\">{0}</p>'.format(svg_data)\n\n            else:  # External SVG file\n                # TODO: there is no reason why this branch needs to be a raw\n                # directive. It could generate regular docutils nodes and\n                # be useful for any writer.\n                makedirs(self.output_folder)\n                f_name = hashlib.md5(svg_data).hexdigest() + '.svg'\n                img_path = self.graph_path + f_name\n                f_path = os.path.join(self.output_folder, f_name)\n                alt = self.options.get('alt', '')\n                with open(f_path, 'wb+') as outf:\n                    outf.write(svg_data)\n                    self.state.document.settings.record_dependencies.add(f_path)\n                if 'inline' in self.options:\n                    svg_data = '<span class=\"graphviz\"><img src=\"{0}\" alt=\"{1}\"></span>'.format(img_path, alt)\n                else:\n                    svg_data = '<p class=\"graphviz\"><img src=\"{0}\" alt=\"{1}\"></p>'.format(img_path, alt)\n\n            node_list.append(nodes.raw('', svg_data, format='html'))\n            if 'caption' in self.options and 'inline' not in self.options:\n                node_list.append(\n                    nodes.raw('', '<p class=\"caption\">{0}</p>'.format(self.options['caption']),\n                              format='html'))\n            return node_list\n        except OSError:\n            LOGGER.error(\"Can't execute 'dot'\")\n            raise"}
{"input": "def __init__(self, filename, mode='r', force_overwrite=True, compression='zlib'):\n        self._open = False  # is the file handle currently open?\n        self.mode = mode  # the mode in which the file was opened?\n\n        if not mode in ['r', 'w', 'a']:\n            raise ValueError(\"mode must be one of ['r', 'w', 'a']\")\n\n        if mode == 'w' and not force_overwrite and os.path.exists(filename):\n            raise IOError('\"%s\" already exists' % filename)\n\n        # import tables\n        self.tables = import_('tables')\n\n        if compression == 'zlib':\n            compression = self.tables.Filters(complib='zlib', shuffle=True, complevel=1)\n        elif compression is None:\n            compression = None\n        else:\n            raise ValueError('compression must be either \"zlib\" or None')\n\n        self._handle = self._open_file(filename, mode=mode, filters=compression)\n        self._open = True\n\n        if mode == 'w':\n            # what frame are we currently reading or writing at?\n            self._frame_index = 0\n            # do we need to write the header information?\n            self._needs_initialization = True\n            if not filename.endswith('.h5'):\n                warnings.warn('The .h5 extension is recommended.')\n\n        elif mode == 'a':\n            try:\n                self._frame_index = len(self._handle.root.coordinates)\n                self._needs_initialization = False\n            except self.tables.NoSuchNodeError:\n                self._frame_index = 0\n                self._needs_initialization = False\n        elif mode == 'r':\n            self._frame_index = 0\n            self._needs_initialization = False", "output": "def __init__(self, filename, mode='r', force_overwrite=True, compression='zlib'):\n        self._open = False  # is the file handle currently open?\n        self.mode = mode  # the mode in which the file was opened?\n\n        if not mode in ['r', 'w', 'a']:\n            raise ValueError(\"mode must be one of ['r', 'w', 'a']\")\n\n        if mode == 'w' and not force_overwrite and os.path.exists(filename):\n            raise IOError('\"%s\" already exists' % filename)\n\n        # import tables\n        self.tables = import_('tables')\n\n        if compression == 'zlib':\n            compression = self.tables.Filters(complib='zlib', shuffle=True, complevel=1)\n        elif compression is None:\n            compression = None\n        else:\n            raise ValueError('compression must be either \"zlib\" or None')\n\n        self._handle = self._open_file(filename, mode=mode, filters=compression)\n        self._open = True\n\n        if mode == 'w':\n            # what frame are we currently reading or writing at?\n            self._frame_index = 0\n            # do we need to write the header information?\n            self._needs_initialization = True\n            if not filename.endswith('.h5'):\n                warnings.warn('The .h5 extension is recommended.')\n\n        elif mode == 'a':\n            try:\n                self._frame_index = len(self._handle.root.coordinates)\n                self._needs_initialization = False\n            except self.tables.NoSuchNodeError:\n                self._frame_index = 0\n                self._needs_initialization = True\n        elif mode == 'r':\n            self._frame_index = 0\n            self._needs_initialization = False"}
{"input": "def get_or_create_user(self, username, ldap_user):\n        # type: (str, _LDAPUser) -> Tuple[UserProfile, bool]\n        try:\n            user_profile = get_user_profile_by_email(username)\n            if not user_profile.is_active or user_profile.realm.deactivated:\n                raise ZulipLDAPException(\"Realm has been deactivated\")\n            return user_profile, False\n        except UserProfile.DoesNotExist:\n            domain = resolve_email_to_domain(username)\n            realm = get_realm(domain)\n            # No need to check for an inactive user since they don't exist yet\n            if realm.deactivated:\n                raise ZulipLDAPException(\"Realm has been deactivated\")\n\n            full_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"full_name\"]\n            short_name = full_name = ldap_user.attrs[full_name_attr][0]\n            if \"short_name\" in settings.AUTH_LDAP_USER_ATTR_MAP:\n                short_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"short_name\"]\n                short_name = ldap_user.attrs[short_name_attr][0]\n\n            user_profile = do_create_user(username, None, realm, full_name, short_name)\n            return user_profile, False", "output": "def get_or_create_user(self, username, ldap_user):\n        # type: (str, _LDAPUser) -> Tuple[UserProfile, bool]\n        try:\n            user_profile = get_user_profile_by_email(username)\n            if not user_profile.is_active or user_profile.realm.deactivated:\n                raise ZulipLDAPException(\"Realm has been deactivated\")\n            return user_profile, False\n        except UserProfile.DoesNotExist:\n            domain = resolve_email_to_domain(username)\n            realm = get_realm(domain)\n            # No need to check for an inactive user since they don't exist yet\n            if realm.deactivated:\n                raise ZulipLDAPException(\"Realm has been deactivated\")\n\n            full_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"full_name\"]\n            short_name = full_name = ldap_user.attrs[full_name_attr][0]\n            if \"short_name\" in settings.AUTH_LDAP_USER_ATTR_MAP:\n                short_name_attr = settings.AUTH_LDAP_USER_ATTR_MAP[\"short_name\"]\n                short_name = ldap_user.attrs[short_name_attr][0]\n\n            user_profile = do_create_user(username, None, realm, full_name, short_name)\n            return user_profile, True"}
{"input": "def peek(self, draw_limb=True, draw_grid=False, gamma=None,\n                   colorbar=True, basic_plot=False, **matplot_args):\n        \"\"\"Displays the map in a new figure\n\n        Parameters\n        ----------\n        draw_limb : bool\n            Whether the solar limb should be plotted.\n        draw_grid : bool or number\n            Whether solar meridians and parallels are plotted. If float then sets\n            degree difference between parallels and meridians.\n        gamma : float\n            Gamma value to use for the color map\n        colorbar : bool\n            Whether to display a colorbar next to the plot\n        basic_plot : bool\n            If true, the data is plotted by itself at it's natural scale; no\n            title, labels, or axes are shown.\n        **matplot_args : dict\n            Matplotlib Any additional imshow arguments that should be used\n            when plotting the image.\n        \"\"\"\n\n        # Create a figure and add title and axes\n        figure = plt.figure(frameon=not basic_plot)\n\n        # Basic plot\n        if basic_plot:\n            axes = plt.Axes(figure, [0., 0., 1., 1.])\n            axes.set_axis_off()\n            figure.add_axes(axes)\n            matplot_args.update({'annotate':False})\n\n        # Normal plot\n        else:\n            axes = figure.gca()\n\n        im = self.plot(axes=axes,**matplot_args)\n\n        if colorbar and not basic_plot:\n            figure.colorbar(im)\n\n        if draw_limb:\n            self.draw_limb(axes=axes)\n\n        if isinstance(draw_grid, bool):\n            if draw_grid:\n                self.draw_grid(axes=axes)\n        elif isinstance(draw_grid, (int, long, float)):\n            self.draw_grid(axes=axes, grid_spacing=draw_grid)\n        else:\n            raise TypeError(\"draw_grid should be bool, int, long or float\")\n\n        figure.show()\n\n        return figure", "output": "def peek(self, draw_limb=False, draw_grid=False, gamma=None,\n                   colorbar=True, basic_plot=False, **matplot_args):\n        \"\"\"Displays the map in a new figure\n\n        Parameters\n        ----------\n        draw_limb : bool\n            Whether the solar limb should be plotted.\n        draw_grid : bool or number\n            Whether solar meridians and parallels are plotted. If float then sets\n            degree difference between parallels and meridians.\n        gamma : float\n            Gamma value to use for the color map\n        colorbar : bool\n            Whether to display a colorbar next to the plot\n        basic_plot : bool\n            If true, the data is plotted by itself at it's natural scale; no\n            title, labels, or axes are shown.\n        **matplot_args : dict\n            Matplotlib Any additional imshow arguments that should be used\n            when plotting the image.\n        \"\"\"\n\n        # Create a figure and add title and axes\n        figure = plt.figure(frameon=not basic_plot)\n\n        # Basic plot\n        if basic_plot:\n            axes = plt.Axes(figure, [0., 0., 1., 1.])\n            axes.set_axis_off()\n            figure.add_axes(axes)\n            matplot_args.update({'annotate':False})\n\n        # Normal plot\n        else:\n            axes = figure.gca()\n\n        im = self.plot(axes=axes,**matplot_args)\n\n        if colorbar and not basic_plot:\n            figure.colorbar(im)\n\n        if draw_limb:\n            self.draw_limb(axes=axes)\n\n        if isinstance(draw_grid, bool):\n            if draw_grid:\n                self.draw_grid(axes=axes)\n        elif isinstance(draw_grid, (int, long, float)):\n            self.draw_grid(axes=axes, grid_spacing=draw_grid)\n        else:\n            raise TypeError(\"draw_grid should be bool, int, long or float\")\n\n        figure.show()\n\n        return figure"}
{"input": "def admin_post():\n    if settings.app.demo_mode:\n        return utils.demo_blocked()\n\n    if not flask.g.administrator.super_user:\n        return utils.jsonify({\n            'error': REQUIRES_SUPER_USER,\n            'error_msg': REQUIRES_SUPER_USER_MSG,\n        }, 400)\n\n    username = utils.filter_str(flask.request.json['username']).lower()\n    password = flask.request.json['password']\n    yubikey_id = flask.request.json.get('yubikey_id') or None\n    yubikey_id = yubikey_id[:12] if yubikey_id else None\n    otp_auth = flask.request.json.get('otp_auth', False)\n    auth_api = flask.request.json.get('auth_api', False)\n    disabled = flask.request.json.get('disabled', False)\n    super_user = flask.request.json.get('super_user', False)\n    remote_addr = utils.get_remote_addr()\n\n    try:\n        admin = auth.new_admin(\n            username=username,\n            password=password,\n            yubikey_id=yubikey_id,\n            default=True,\n            otp_auth=otp_auth,\n            auth_api=auth_api,\n            disabled=disabled,\n            super_user=super_user,\n        )\n    except pymongo.errors.DuplicateKeyError:\n        return utils.jsonify({\n            'error': ADMIN_USERNAME_EXISTS,\n            'error_msg': ADMIN_USERNAME_EXISTS_MSG,\n        }, 400)\n\n    admin.audit_event('admin_created',\n        'Administrator created',\n        remote_addr=remote_addr,\n    )\n\n    journal.entry(\n        journal.ADMIN_CREATE,\n        admin.journal_data,\n        event_long='Administrator created',\n        remote_addr=remote_addr,\n    )\n\n    event.Event(type=ADMINS_UPDATED)\n\n    return utils.jsonify(admin.dict())", "output": "def admin_post():\n    if settings.app.demo_mode:\n        return utils.demo_blocked()\n\n    if not flask.g.administrator.super_user:\n        return utils.jsonify({\n            'error': REQUIRES_SUPER_USER,\n            'error_msg': REQUIRES_SUPER_USER_MSG,\n        }, 400)\n\n    username = utils.filter_str(flask.request.json['username']).lower()\n    password = flask.request.json['password']\n    yubikey_id = flask.request.json.get('yubikey_id') or None\n    yubikey_id = yubikey_id[:12] if yubikey_id else None\n    otp_auth = flask.request.json.get('otp_auth', False)\n    auth_api = flask.request.json.get('auth_api', False)\n    disabled = flask.request.json.get('disabled', False)\n    super_user = flask.request.json.get('super_user', False)\n    remote_addr = utils.get_remote_addr()\n\n    try:\n        admin = auth.new_admin(\n            username=username,\n            password=password,\n            yubikey_id=yubikey_id,\n            default=False,\n            otp_auth=otp_auth,\n            auth_api=auth_api,\n            disabled=disabled,\n            super_user=super_user,\n        )\n    except pymongo.errors.DuplicateKeyError:\n        return utils.jsonify({\n            'error': ADMIN_USERNAME_EXISTS,\n            'error_msg': ADMIN_USERNAME_EXISTS_MSG,\n        }, 400)\n\n    admin.audit_event('admin_created',\n        'Administrator created',\n        remote_addr=remote_addr,\n    )\n\n    journal.entry(\n        journal.ADMIN_CREATE,\n        admin.journal_data,\n        event_long='Administrator created',\n        remote_addr=remote_addr,\n    )\n\n    event.Event(type=ADMINS_UPDATED)\n\n    return utils.jsonify(admin.dict())"}
{"input": "def upgrade(full=True):\n    \"\"\"\n    Upgrade all packages.\n    \"\"\"\n    manager = MANAGER\n    cmds = {'pkgin': {False: 'ug', True: 'fug'}}\n    cmd = cmds[manager][full]\n    sudo(\"%(manager)s -y %(cmd)s\" % locals())", "output": "def upgrade(full=False):\n    \"\"\"\n    Upgrade all packages.\n    \"\"\"\n    manager = MANAGER\n    cmds = {'pkgin': {False: 'ug', True: 'fug'}}\n    cmd = cmds[manager][full]\n    sudo(\"%(manager)s -y %(cmd)s\" % locals())"}
{"input": "def quality(wavelength: Union[Quantity, ndarray], flux: Union[Quantity, ndarray]) -> Union[\n    float64, Quantity]:\n    \"\"\"Calculation of the spectral Quality, Q, for a spectrum.\n\n    Parameters\n    ----------\n    wavelength: array-like or Quantity array\n        Wavelength of spectrum.\n    flux: array-like or Quantity array\n        Flux of spectrum.\n\n    Returns\n    -------\n    sqrt{sum{W(i)}}: float or Quantity scalar\n       Spectral quality\n\n    Notes\n    -----\n    Extract from https://arxiv.org/pdf/1511.07468v1.pdf\n\n        Q = sqrt{sum{W(i)}} / sqrt{sum{A_0{i}}\n\n    where, W(i), is the optimal pixel weights\n\n        W(i) = lambda(i)**2 (d'A_0(i) / d'lambda(i))**2 / (A_0(i) + sigma_D**2)\n\n    in which lambda(i) and A_0(i) are the values of each pixel wave-length and\n    flux, respectively. The weight will be proportional to the information\n    content of the spectrum, given by the derivative of the amplitude, and\n    calculated following Connes (1985).\n\n    The spectral quality, Q, is indpendant of the flux level and is only\n    a function of the spectral profile.\n\n    \"\"\"\n    if not isinstance(wavelength, np.ndarray):\n        print(\"Your wavelength and flux should really be numpy arrays! Converting them here.\")\n        wavelength = np.asarray(wavelength)\n    if not isinstance(flux, np.ndarray):\n        flux = np.asarray(flux)\n    \n    flux = flux = u.dimensionless_unscaled # Turn into Quantity if not already\n    flux = flux / flux.unit  # Remove units from flux (sqrt(N_e) is unitless)\n\n    wis = sqrt_sum_wis(wavelength, flux)\n\n    return wis / np.sqrt(np.sum(flux))", "output": "def quality(wavelength: Union[Quantity, ndarray], flux: Union[Quantity, ndarray]) -> Union[\n    float64, Quantity]:\n    \"\"\"Calculation of the spectral Quality, Q, for a spectrum.\n\n    Parameters\n    ----------\n    wavelength: array-like or Quantity array\n        Wavelength of spectrum.\n    flux: array-like or Quantity array\n        Flux of spectrum.\n\n    Returns\n    -------\n    sqrt{sum{W(i)}}: float or Quantity scalar\n       Spectral quality\n\n    Notes\n    -----\n    Extract from https://arxiv.org/pdf/1511.07468v1.pdf\n\n        Q = sqrt{sum{W(i)}} / sqrt{sum{A_0{i}}\n\n    where, W(i), is the optimal pixel weights\n\n        W(i) = lambda(i)**2 (d'A_0(i) / d'lambda(i))**2 / (A_0(i) + sigma_D**2)\n\n    in which lambda(i) and A_0(i) are the values of each pixel wave-length and\n    flux, respectively. The weight will be proportional to the information\n    content of the spectrum, given by the derivative of the amplitude, and\n    calculated following Connes (1985).\n\n    The spectral quality, Q, is indpendant of the flux level and is only\n    a function of the spectral profile.\n\n    \"\"\"\n    if not isinstance(wavelength, np.ndarray):\n        print(\"Your wavelength and flux should really be numpy arrays! Converting them here.\")\n        wavelength = np.asarray(wavelength)\n    if not isinstance(flux, np.ndarray):\n        flux = np.asarray(flux)\n    \n    flux = flux * u.dimensionless_unscaled # Turn into Quantity if not already\n    flux = flux / flux.unit  # Remove units from flux (sqrt(N_e) is unitless)\n\n    wis = sqrt_sum_wis(wavelength, flux)\n\n    return wis / np.sqrt(np.sum(flux))"}
{"input": "def fill_graphs(self):\n        # bin size calculations\n        minimum, maximum = stats.describe(self.data_prices, bias=False, nan_policy=\"omit\")[1]\n        minimum -= .05\n        maximum -= .05\n        difference = maximum - minimum\n        bin_size = difference / self.bins\n\n        # counts for every bin\n        counts = [0] * (self.bins + 1)\n        for price in self.data_prices:\n            counts[int((price - minimum) / bin_size)] += 1\n\n        # puts the bin data into the sheet\n        for cell_index in range(0, self.bins):\n            bin_name = minimum + (((cell_index * bin_size) + ((cell_index + 1) * bin_size)) / 2)\n            self.sheet[\"D\" + str(cell_index + 9)] = bin_name\n            self.sheet[\"E\" + str(cell_index + 9)] = counts[cell_index]\n\n        # puts in the bar chart\n        bar_chart = openpyxl.chart.BarChart()\n        bar_chart.shape = 4\n        bar_chart.type = \"col\"\n        bar_chart.style = 10\n        bar_chart.title = self.sheet.title + \" HISTOGRAM\"\n        bar_chart.x_axis_title = \"BIN AVERAGE\"\n        bar_chart.y_axis_title = \"FREQUENCY\"\n        bar_data = openpyxl.chart.Reference(self.sheet, min_col = 5, min_row = 8, max_row = 9 + self.bins - 1, max_col = 5)\n        bar_categories = openpyxl.chart.Reference(self.sheet, min_col = 4, min_row = 9, max_row = 9 + self.bins - 1)\n        bar_chart.add_data(bar_data, titles_from_data = True)\n        bar_chart.set_categories(bar_categories)\n        self.sheet.add_chart(bar_chart, \"G4\")\n\n        # puts in the 3 month line chart\n        line_chart = openpyxl.chart.LineChart()\n        line_chart.style = 12\n        line_chart.title = self.sheet.title + \" LINECHART\"\n        line_chart.x_axis_title = \"DATE\"\n        line_chart.y_axis_title = \"PRICE\"\n        line_data = openpyxl.chart.Reference(self.sheet, min_col = 2, min_row = 1, max_row = 1 + len(self.data_prices))\n        line_categories = openpyxl.chart.Reference(self.sheet, min_col = 1, min_row = 2, max_row = 1 + len(self.data_prices))\n        line_chart.add_data(line_data)\n        line_chart.set_categories(line_categories)\n        # style the line chart\n        style = line_chart.series[0]\n        style.graphicalProperties.line.solidFill = \"00AAAA\"\n        style.graphicalProperties.line.dashStyle = \"sysDot\"\n        style.graphicalProperties.line.width = 100050\n        self.sheet.add_chart(line_chart, \"G22\")", "output": "def fill_graphs(self):\n        # bin size calculations\n        minimum, maximum = stats.describe(self.data_prices, bias=False, nan_policy=\"omit\")[1]\n        minimum -= .05\n        maximum += .05\n        difference = maximum - minimum\n        bin_size = difference / self.bins\n\n        # counts for every bin\n        counts = [0] * (self.bins + 1)\n        for price in self.data_prices:\n            counts[int((price - minimum) / bin_size)] += 1\n\n        # puts the bin data into the sheet\n        for cell_index in range(0, self.bins):\n            bin_name = minimum + (((cell_index * bin_size) + ((cell_index + 1) * bin_size)) / 2)\n            self.sheet[\"D\" + str(cell_index + 9)] = bin_name\n            self.sheet[\"E\" + str(cell_index + 9)] = counts[cell_index]\n\n        # puts in the bar chart\n        bar_chart = openpyxl.chart.BarChart()\n        bar_chart.shape = 4\n        bar_chart.type = \"col\"\n        bar_chart.style = 10\n        bar_chart.title = self.sheet.title + \" HISTOGRAM\"\n        bar_chart.x_axis_title = \"BIN AVERAGE\"\n        bar_chart.y_axis_title = \"FREQUENCY\"\n        bar_data = openpyxl.chart.Reference(self.sheet, min_col = 5, min_row = 8, max_row = 9 + self.bins - 1, max_col = 5)\n        bar_categories = openpyxl.chart.Reference(self.sheet, min_col = 4, min_row = 9, max_row = 9 + self.bins - 1)\n        bar_chart.add_data(bar_data, titles_from_data = True)\n        bar_chart.set_categories(bar_categories)\n        self.sheet.add_chart(bar_chart, \"G4\")\n\n        # puts in the 3 month line chart\n        line_chart = openpyxl.chart.LineChart()\n        line_chart.style = 12\n        line_chart.title = self.sheet.title + \" LINECHART\"\n        line_chart.x_axis_title = \"DATE\"\n        line_chart.y_axis_title = \"PRICE\"\n        line_data = openpyxl.chart.Reference(self.sheet, min_col = 2, min_row = 1, max_row = 1 + len(self.data_prices))\n        line_categories = openpyxl.chart.Reference(self.sheet, min_col = 1, min_row = 2, max_row = 1 + len(self.data_prices))\n        line_chart.add_data(line_data)\n        line_chart.set_categories(line_categories)\n        # style the line chart\n        style = line_chart.series[0]\n        style.graphicalProperties.line.solidFill = \"00AAAA\"\n        style.graphicalProperties.line.dashStyle = \"sysDot\"\n        style.graphicalProperties.line.width = 100050\n        self.sheet.add_chart(line_chart, \"G22\")"}
{"input": "def add_edge(self, edge, wt=1, label=\"\"):\n        \"\"\"\n        Add an edge to the graph connecting two nodes.\n        An edge, here, is a pair of node like C(m, n) or a tuple\n        with m as head and n as tail :  m -> n\n        \"\"\"\n        u, v = edge\n        if (v not in self.node_neighbors[u]):\n            self.node_neighbors[u][v] = wt\n        else:\n            raise Exception(\"Edge (%s, %s) already added in the graph\" % (u, v))", "output": "def add_edge(self, edge, wt=DEFAULT_WEIGHT, label=\"\"):\n        \"\"\"\n        Add an edge to the graph connecting two nodes.\n        An edge, here, is a pair of node like C(m, n) or a tuple\n        with m as head and n as tail :  m -> n\n        \"\"\"\n        u, v = edge\n        if (v not in self.node_neighbors[u]):\n            self.node_neighbors[u][v] = wt\n        else:\n            raise Exception(\"Edge (%s, %s) already added in the graph\" % (u, v))"}
{"input": "def request(self, *args, **kwargs):\n        r = super(BrowserSession, self).request(*args, **kwargs)\n\n        r._content = self.render(r.text).encode(DEFAULT_ENCODING)\n        r.encoding = 'utf-8'\n\n        r.html = HTML(url=r.url, html=r.text, default_encoding=r.encoding)\n\n        return r", "output": "def request(self, *args, **kwargs):\n        r = super(BrowserSession, self).request(*args, **kwargs)\n\n        r._content = self.render(r.text).encode(DEFAULT_ENCODING)\n        r.encoding = DEFAULT_ENCODING\n\n        r.html = HTML(url=r.url, html=r.text, default_encoding=r.encoding)\n\n        return r"}
{"input": "def get_percentiles(results,sim_number=None):\n    \"\"\"\n    Get 5,25,50,75 and 95 percentiles of your simulations\n    \n    :results: Expects an numpy array which should of an index \"simulation\" for simulations \n    :type: array    \n    \n    :sim_number: Optional, Number of your simulation, needed when working with multiple lists of simulations\n    :type: int\n    \n    :return: Percentiles of simulations \n    :rtype: int and float\n    \"\"\" \n    p5,p25,p50,p75,p95=[],[],[],[],[]\n    fields=[word for word in results.dtype.names if word.startswith('simulation'+str(sim_number))]\n    for i in range(len(fields)):\n        p5.append(np.percentile(list(results[fields[i]]),5))\n        p25.append(np.percentile(list(results[fields[i]]),25))\n        p50.append(np.percentile(list(results[fields[i]]),50))\n        p75.append(np.percentile(list(results[fields[i]]),75))    \n        p95.append(np.percentile(list(results[fields[i]]),95))\n    return p5,p25,p50,p75,p95", "output": "def get_percentiles(results,sim_number=''):\n    \"\"\"\n    Get 5,25,50,75 and 95 percentiles of your simulations\n    \n    :results: Expects an numpy array which should of an index \"simulation\" for simulations \n    :type: array    \n    \n    :sim_number: Optional, Number of your simulation, needed when working with multiple lists of simulations\n    :type: int\n    \n    :return: Percentiles of simulations \n    :rtype: int and float\n    \"\"\" \n    p5,p25,p50,p75,p95=[],[],[],[],[]\n    fields=[word for word in results.dtype.names if word.startswith('simulation'+str(sim_number))]\n    for i in range(len(fields)):\n        p5.append(np.percentile(list(results[fields[i]]),5))\n        p25.append(np.percentile(list(results[fields[i]]),25))\n        p50.append(np.percentile(list(results[fields[i]]),50))\n        p75.append(np.percentile(list(results[fields[i]]),75))    \n        p95.append(np.percentile(list(results[fields[i]]),95))\n    return p5,p25,p50,p75,p95"}
{"input": "def _get_next_addr_to_search(self, alignment=None):\n        # TODO: Take care of those functions that are already generated\n        curr_addr = self._next_addr\n        # Determine the size of that IRSB\n        # Note: we don't care about SimProcedure at this moment, as we want to\n        # get as many functions as possible\n        # s_irsb = None\n        # while s_irsb is None:\n        #     s_ex = self._project.exit_to(addr=curr_addr, \\\n        #                     state=self._project.initial_state(mode=\"static\"))\n        #     try:\n        #         s_irsb = self._project.sim_block(s_ex)\n        #     except simuvex.s_irsb.SimIRSBError:\n        #         # We cannot build functions there\n        #         # Move on to next possible position\n        #         s_irsb = None\n        #         # TODO: Handle strings\n        #         curr_addr = \\\n        #             self._seg_list.next_free_pos(curr_addr)\n        if self._seg_list.has_blocks:\n            curr_addr = self._seg_list.next_free_pos(curr_addr)\n\n        if alignment is not None:\n            if curr_addr % alignment > 0:\n                curr_addr = curr_addr - curr_addr % alignment + alignment\n        # block_size = s_irsb.irsb.size()\n        # self._next_addr = curr_addr + block_size\n        self._next_addr = curr_addr\n        if curr_addr < self._ending_point:\n            l.debug(\"Returning new recon address: 0x%08x\", curr_addr)\n            return curr_addr\n        else:\n            l.debug(\"0x%08x is beyond the ending point.\", curr_addr)\n            return None", "output": "def _get_next_addr_to_search(self, alignment=None):\n        # TODO: Take care of those functions that are already generated\n        curr_addr = self._next_addr\n        # Determine the size of that IRSB\n        # Note: we don't care about SimProcedure at this moment, as we want to\n        # get as many functions as possible\n        # s_irsb = None\n        # while s_irsb is None:\n        #     s_ex = self._project.exit_to(addr=curr_addr, \\\n        #                     state=self._project.initial_state(mode=\"static\"))\n        #     try:\n        #         s_irsb = self._project.sim_block(s_ex)\n        #     except simuvex.s_irsb.SimIRSBError:\n        #         # We cannot build functions there\n        #         # Move on to next possible position\n        #         s_irsb = None\n        #         # TODO: Handle strings\n        #         curr_addr = \\\n        #             self._seg_list.next_free_pos(curr_addr)\n        if self._seg_list.has_blocks:\n            curr_addr = self._seg_list.next_free_pos(curr_addr)\n\n        if alignment is not None:\n            if curr_addr % alignment > 0:\n                curr_addr = curr_addr - curr_addr % alignment + alignment\n        # block_size = s_irsb.irsb.size()\n        # self._next_addr = curr_addr + block_size\n        self._next_addr = curr_addr\n        if self._ending_point is None or curr_addr < self._ending_point:\n            l.debug(\"Returning new recon address: 0x%08x\", curr_addr)\n            return curr_addr\n        else:\n            l.debug(\"0x%08x is beyond the ending point.\", curr_addr)\n            return None"}
{"input": "def mobile(request, template=None):\n    if not request.MOBILE:\n        return redirect_to(\n            request, 'products.product', slug='mobile', permanent=False)\n\n    docs = MOBILE_DOCS_FOR_MOBILE\n    return jingo.render(request, template,\n                        _data(docs, request.locale, 'mobile', 'mobile'))", "output": "def mobile(request, template=None):\n    if not request.MOBILE or waffle.flag_is_active(request, 'new-theme'):\n        return redirect_to(\n            request, 'products.product', slug='mobile', permanent=False)\n\n    docs = MOBILE_DOCS_FOR_MOBILE\n    return jingo.render(request, template,\n                        _data(docs, request.locale, 'mobile', 'mobile'))"}
{"input": "def check(self, instance):\n\n        if 'host' not in instance:\n            instance['host'] = 'localhost'\n        if 'extension_length' not in instance:\n            self.log.error('extension_length not defined, skipping')\n            return\n        if 'manager_user' not in instance:\n            self.log.error('manager_user not defined, skipping')\n            return\n        if 'manager_secret' not in instance:\n            self.log.error('manager_secret not defined, skipping')\n            return\n            \n\n######  Connect\n        mgr = asterisk.manager.Manager()\n        try:\n            if 'port' in instance:\n                mgr.connect(instance['host'],instance['port'])\n            else:\n                mgr.connect(instance['host'])\n            mgr.login(instance['manager_user'],instance['manager_secret'])\n        except asterisk.manager.ManagerSocketException as e:\n            self.log.error('Error connecting to Asterisk Manager Interface')\n            mgr.close()\n            return\n        except asterisk.manager.ManagerAuthException as e:\n            self.log.error('Error Logging in to Asterisk Manager Interface')\n            mgr.close()\n            return\n\n##### Call Volume\n        call_volume = mgr.command('core show calls')\n\n        current_call_vol = call_volume.data.split('\\n')\n\n        procesed_call_vol = current_call_vol[1].replace(' calls processed','')\n        current_call_vol = current_call_vol[0].replace('active call','')\n        current_call_vol = current_call_vol.replace('s','')\n        current_call_vol = current_call_vol.replace(' ','')\n\n        self.gauge('asterisk.callsprocesed',procesed_call_vol)\n        self.gauge('asterisk.callvolume',current_call_vol)\n\n##### Internal, Inbound Outbound Calls\n\n        extensionLength = instance['extension_length']\n\n        current_channels = mgr.command('core show channels verbose')\n        current_channels = current_channels.data.split('\\n')\n        current_channels[0] = None\n        current_channels_size = len(current_channels)\n        current_channels[current_channels_size-1] = None\n        current_channels[current_channels_size-2] = None\n        current_channels[current_channels_size-3] = None\n        current_channels[current_channels_size-4] = None\n        current_channels[current_channels_size-5] = None\n\n        currentChannelsArray = []\n        currentCalls = []\n\n        for chan in current_channels:\n            if chan != None:\n                channel     = re.sub(' +',' ',chan[0:21]).lstrip(' ').rstrip(' ')\n                context     = re.sub(' +',' ',chan[21:42]).lstrip(' ').rstrip(' ')\n                extension   = re.sub(' +',' ',chan[42:59]).lstrip(' ').rstrip(' ')\n                priority    = re.sub(' +',' ',chan[59:64]).lstrip(' ').rstrip(' ')\n                state       = re.sub(' +',' ',chan[64:72]).lstrip(' ').rstrip(' ')\n                application = re.sub(' +',' ',chan[72:85]).lstrip(' ').rstrip(' ')\n                data        = re.sub(' +',' ',chan[85:111]).lstrip(' ').rstrip(' ')\n                callerid    = re.sub(' +',' ',chan[111:127]).lstrip(' ').rstrip(' ')\n                duration    = re.sub(' +',' ',chan[127:136]).lstrip(' ').rstrip(' ')\n                accountcode = re.sub(' +',' ',chan[136:148]).lstrip(' ').rstrip(' ')\n                peeraccount = re.sub(' +',' ',chan[148:160]).lstrip(' ').rstrip(' ')\n                bridgedto   = re.sub(' +',' ',chan[160:181]).lstrip(' ').rstrip(' ')\n                currentChannel = Channel(channel,context,extension,priority,state,application,data,callerid,duration,accountcode,peeraccount,bridgedto)\n                currentChannelsArray.append(currentChannel)\n                \n        internalCalls = 0\n        outboundCalls = 0\n        inboundCalls  = 0\n\n        for currentChannel in currentChannelsArray:\n            caller = \"N/A\"\n            called = \"N/A\"\n            callType = \"N/A\"\n\n            if \"Dial\" == currentChannel.Application:\n                currentCall = Call(\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\")\n                currentCall.Caller = currentChannel.CallerId\n                currentCall.CallerChannel = currentChannel.Channel\n                currentCall.BridgedChannel = currentChannel.BridgedTo\n                currentCalls.append(currentCall)\n\n        for currentCall in currentCalls:\n            caller = \"N/A\"\n            called = \"N/A\"\n            callType = \"N/A\"\n            for currentChannel in currentChannelsArray:\n                if \"None\" not in currentChannel.BridgedTo:\n                    if currentCall.BridgedChannel == currentChannel.Channel:\n                        currentCall.Called = currentChannel.CallerId\n                        currentCall.CalledChannel = currentChannel.Channel\n\n        for currentCall in currentCalls:\n            if len(currentCall.Caller) <= extensionLength and len(currentCall.Called) <= extensionLength:\n                currentCall.CallType = \"Internal\"\n                internalCalls = internalCalls +1\n            if len(currentCall.Caller) > extensionLength and len(currentCall.Called) <= extensionLength:\n                currentCall.CallType = \"Inbound\"\n                inboundCalls = inboundCalls + 1\n            if len(currentCall.Caller) <= extensionLength and len(currentCall.Called) > extensionLength:\n                currentCall.CallType = \"Outbound\"\n                outboundCalls = outboundCalls + 1\n\n        self.gauge('asterisk.calls.internal',internalCalls)\n        self.gauge('asterisk.calls.inbound',inboundCalls)\n        self.gauge('asterisk.calls.outbound',outboundCalls)\n\n##### SIP Peers\n        sip_result = mgr.command('sip show peers')\n\n        sip_results = sip_result.data.split('\\n')\n\n        siptotals = sip_results[len(sip_results)-3]\n\n        siptotal = re.findall(r'([0-9]+) sip peer',siptotals)[0]\n\n        monitored_peers = re.findall(r'Monitored: ([0-9]+) online, ([0-9]+) offline',siptotals)[0]\n        unmonitored_peers = re.findall(r'Unmonitored: ([0-9]+) online, ([0-9]+) offline',siptotals)[0]\n\n        self.gauge('asterisk.sip.peers',siptotal)\n        self.gauge('asterisk.sip.monitored.online',monitored_peers[0])\n        self.gauge('asterisk.sip.monitored.offline',monitored_peers[1])\n        self.gauge('asterisk.sip.unmonitored.online',unmonitored_peers[0])\n        self.gauge('asterisk.sip.unmonitored.offline',unmonitored_peers[1])\n\n##### SIP Trunks (You have to add '-trunk' string into your SIP trunk name to detect it as a Trunk)\n        sip_total_trunks = 0\n        sip_online_trunks = 0\n        sip_offline_trunks = 0\n\n        trunks = re.finditer('^.*-trunk.*([OK|UN].*)', sip_result.data, re.MULTILINE)\n\n        for trunk in trunks:\n            sip_total_trunks +=1\n            if 'OK' in trunk.group():\n                sip_online_trunks += 1\n            else:\n                sip_offline_trunks += 1\n      \n        self.gauge('asterisk.sip.trunks.total',sip_total_trunks)\n        self.gauge('asterisk.sip.trunks.online',sip_online_trunks)\n        self.gauge('asterisk.sip.trunks.offline',sip_offline_trunks)\n\n##### PRI In Use\n\n        pri = mgr.command('pri show channels')\n\n        pri_channels = pri.data.split('\\n')\n\n        pri_channels[0] = None\n        pri_channels[1] = None\n\n        openchannels = 0\n        for chan in pri_channels:\n            if chan != None:\n                chan_data = chan.split()\n                if len(chan_data) > 2 and chan_data[3] == \"No\":\n                    openchannels += 1\n\n        self.gauge('asterisk.pri.channelsinuse',openchannels)\n\n##### IAX2 Peers\n\n        iax_result = mgr.command('iax2 show peers')\n\n        iax_results = iax_result.data.split('\\n')\n\n        iax_total_line = iax_results[len(iax_results)-3]\n\n        iax_peers_total = re.findall(r'([0-9]+) iax2 peers',iax_total_line)[0]\n        iax_peers_online = re.findall(r'\\[([0-9]+) online',iax_total_line)[0]\n        iax_peers_offline = re.findall(r'([0-9]+) offline',iax_total_line)[0]\n        iax_peers_unmonitored = re.findall(r'([0-9]+) unmonitored',iax_total_line)[0]\n\n        self.gauge('asterisk.iax2.peers',iax_peers_total)\n        self.gauge('asterisk.iax2.online',iax_peers_online)\n        self.gauge('asterisk.iax2.offline',iax_peers_offline)\n        self.gauge('asterisk.iax2.unmonitored',iax_peers_unmonitored)\n   \n##### DAHDI Channels  \n    \n        dahdi_result = mgr.command('dahdi show status')\n\n        dahdi_results = dahdi_result.data.split('\\n')\n\n        dahdi_total_trunks = len(dahdi_results)-3\n\n        dahdi_results[0] = None\n\n        dahdi_online_trunks = 0\n        dahdi_offline_trunks = 0\n\n        for chan in dahdi_results:\n            if chan != None:\n                chan_data = chan.split()\n\n                if len(chan_data) > 1:\n                    if \"Wildcard\" in chan_data[0]:\n                        if len(chan_data) > 2 and chan_data[2] == \"OK\":\n                            dahdi_online_trunks += 1\n                        if len(chan_data) > 2 and chan_data[2] == \"RED\":\n                            dahdi_offline_trunks += 1\n\n                    if \"wanpipe\" in chan_data[0]:\n                        if len(chan_data) > 2 and chan_data[3] == \"OK\":\n                            dahdi_online_trunks += 1\n                        if len(chan_data) > 2 and chan_data[3] == \"RED\":\n                            dahdi_offline_trunks += 1\n\n        self.gauge('asterisk.dahdi.total',dahdi_total_trunks)\n        self.gauge('asterisk.dahdi.online',dahdi_online_trunks)\n        self.gauge('asterisk.dahdi.offline',dahdi_offline_trunks)\n        \n##### G729 Codecs \n        \n        g729_result = mgr.command('g729 show licenses')\n\n        g729_results = g729_result.data.split('\\n')\n\n        g729_total_line = g729_results[0]\n\n        g729_total = re.findall(r'([0-9]+) licensed',g729_total_line)[0]\n        g729_encoders = re.split('/',g729_total_line)[0]\n        g729_decoders = re.findall(r'([0-9]+) encoders/decoders',g729_total_line)[0]\n\n        self.gauge('asterisk.g729.total',g729_total)\n        self.gauge('asterisk.g729.encoders',g729_encoders)\n        self.gauge('asterisk.g729.decoders',g729_decoders)\n        \n\n##### Asterisk Uptime\n\n        uptime_result = mgr.command('core show uptime')\n        \n        uptime_results = uptime_result.data.split('\\n')\n        \n        system_total_line = uptime_results[0]\n        asterisk_total_line = uptime_results[1]\n        \n        system_uptime_days = 0\n        system_uptime_hours = 0\n        system_uptime_minutes = 0\n        system_uptime_seconds = 0\n        \n        system_uptime_days = 0\n        system_uptime_hours = 0\n        system_uptime_minutes = 0\n        system_uptime_seconds = 0\n\n        if \"day\" in system_total_line:\n            system_uptime_days = re.findall(r'([0-9]+) day',system_total_line)[0]\n        if \"hour\" in system_total_line:\n            system_uptime_hours = re.findall(r'([0-9]+) hour',system_total_line)[0]\n        if \"minute\" in system_total_line:\n            system_uptime_minutes = re.findall(r'([0-9]+) minute',system_total_line)[0]\n        if \"second\" in system_total_line:\n            system_uptime_seconds = re.findall(r'([0-9]+) second',system_total_line)[0]\n\n        system_uptime = ( int(system_uptime_days) * 86400) +  ( int(system_uptime_hours) * 3600) + ( int(system_uptime_minutes) * 60) + int(system_uptime_seconds)\n        \n        asterisk_last_reload_days = 0\n        asterisk_last_reload_hours = 0\n        asterisk_last_reload_minutes = 0\n        asterisk_last_reload_seconds = 0\n        \n        if \"day\" in asterisk_total_line:\n            asterisk_last_reload_days = re.findall(r'([0-9]+) day',asterisk_total_line)[0]\n        if \"hour\" in asterisk_total_line:\n            asterisk_last_reload_hours = re.findall(r'([0-9]+) hour',asterisk_total_line)[0]\n        if \"minute\" in asterisk_total_line:\n            asterisk_last_reload_minutes = re.findall(r'([0-9]+) minute',asterisk_total_line)[0]\n        if \"second\" in asterisk_total_line:\n            asterisk_last_reload_seconds = re.findall(r' ([0-9]+) second',asterisk_total_line)[0]\n\n        asterisk_last_reload = ( int(asterisk_last_reload_days) * 86400) + ( int(asterisk_last_reload_hours) * 3600) + ( int(asterisk_last_reload_minutes) * 60) + int(asterisk_last_reload_seconds)\n\n        self.gauge('asterisk.system.uptime',system_uptime)\n        self.gauge('asterisk.last.reload',asterisk_last_reload)\n        \n##### MFCR2 Channels\n\n        mfcr2_result = mgr.command('mfcr2 show channels')\n\n        mfcr2_results = mfcr2_result.data.split('\\n')\n\n        mfcr2_total_channels = len(mfcr2_results)-3\n\n        mfcr2_results[0] = None\n\n        mfcr2_inuse_channels = 0\n        mfcr2_available_channels = 0\n        mfcr2_blocked_channels = 0\n\n        for chan in mfcr2_results:\n            if chan != None:\n                chan_data = chan.split()\n                if len(chan_data) > 2:\n                    if \"IDLE\" in chan_data[6] and \"IDLE\" in chan_data[7] :\n                        mfcr2_available_channels += 1\n                    if \"ANSWER\" in chan_data[6] or \"ANSWER\" in chan_data[7] :\n                        mfcr2_inuse_channels += 1\n                    if \"BLOCK\" in chan_data[6] or \"BLOCK\" in chan_data[7] :\n                        mfcr2_blocked_channels += 1\n                        \n        self.gauge('asterisk.mfcr2.total.channels',mfcr2_total_channels)\n        self.gauge('asterisk.mfcr2.available.channels',mfcr2_available_channels)\n        self.gauge('asterisk.mfcr2.inuse.channels',mfcr2_inuse_channels)\n        self.gauge('asterisk.mfcr2.blocked.channels',mfcr2_blocked_channels)\n\n##### SCCP Devices\n\n        sccp_total_devices = 0\n        sccp_online_devices = 0\n        sccp_offline_devices = 0\n        \n        sccp_result = mgr.command('sccp show devices')\n\n        if \"No such command\" not in sccp_result.data:\n            \n            sccp_devices = re.finditer('^.*.SEP.*', sccp_result.data, re.MULTILINE)\n\n            for sccp_device in sccp_devices:\n                sccp_total_devices +=1\n                if '--' in sccp_device.group():\n                    sccp_offline_devices += 1\n                else:\n                    sccp_online_devices += 1\n\n        self.gauge('asterisk.sccp.devices.total',sccp_total_devices)\n        self.gauge('asterisk.sccp.devices.online',sccp_online_devices)\n        self.gauge('asterisk.sccp.devices.offline',sccp_offline_devices)\n                    \n\n##### Close connection\n\n        mgr.close()", "output": "def check(self, instance):\n\n        if 'host' not in instance:\n            instance['host'] = 'localhost'\n        if 'extension_length' not in instance:\n            self.log.error('extension_length not defined, skipping')\n            return\n        if 'manager_user' not in instance:\n            self.log.error('manager_user not defined, skipping')\n            return\n        if 'manager_secret' not in instance:\n            self.log.error('manager_secret not defined, skipping')\n            return\n            \n\n######  Connect\n        mgr = asterisk.manager.Manager()\n        try:\n            if 'port' in instance:\n                mgr.connect(instance['host'],instance['port'])\n            else:\n                mgr.connect(instance['host'])\n            mgr.login(instance['manager_user'],instance['manager_secret'])\n        except asterisk.manager.ManagerSocketException as e:\n            self.log.error('Error connecting to Asterisk Manager Interface')\n            mgr.close()\n            return\n        except asterisk.manager.ManagerAuthException as e:\n            self.log.error('Error Logging in to Asterisk Manager Interface')\n            mgr.close()\n            return\n\n##### Call Volume\n        call_volume = mgr.command('core show calls')\n\n        current_call_vol = call_volume.data.split('\\n')\n\n        procesed_call_vol = current_call_vol[1].replace(' calls processed','')\n        current_call_vol = current_call_vol[0].replace('active call','')\n        current_call_vol = current_call_vol.replace('s','')\n        current_call_vol = current_call_vol.replace(' ','')\n\n        self.gauge('asterisk.callsprocesed',procesed_call_vol)\n        self.gauge('asterisk.callvolume',current_call_vol)\n\n##### Internal, Inbound Outbound Calls\n\n        extensionLength = instance['extension_length']\n\n        current_channels = mgr.command('core show channels verbose')\n        current_channels = current_channels.data.split('\\n')\n        current_channels[0] = None\n        current_channels_size = len(current_channels)\n        current_channels[current_channels_size-1] = None\n        current_channels[current_channels_size-2] = None\n        current_channels[current_channels_size-3] = None\n        current_channels[current_channels_size-4] = None\n        current_channels[current_channels_size-5] = None\n\n        currentChannelsArray = []\n        currentCalls = []\n\n        for chan in current_channels:\n            if chan != None:\n                channel     = re.sub(' +',' ',chan[0:21]).lstrip(' ').rstrip(' ')\n                context     = re.sub(' +',' ',chan[21:42]).lstrip(' ').rstrip(' ')\n                extension   = re.sub(' +',' ',chan[42:59]).lstrip(' ').rstrip(' ')\n                priority    = re.sub(' +',' ',chan[59:64]).lstrip(' ').rstrip(' ')\n                state       = re.sub(' +',' ',chan[64:72]).lstrip(' ').rstrip(' ')\n                application = re.sub(' +',' ',chan[72:85]).lstrip(' ').rstrip(' ')\n                data        = re.sub(' +',' ',chan[85:111]).lstrip(' ').rstrip(' ')\n                callerid    = re.sub(' +',' ',chan[111:127]).lstrip(' ').rstrip(' ')\n                duration    = re.sub(' +',' ',chan[127:136]).lstrip(' ').rstrip(' ')\n                accountcode = re.sub(' +',' ',chan[136:148]).lstrip(' ').rstrip(' ')\n                peeraccount = re.sub(' +',' ',chan[148:160]).lstrip(' ').rstrip(' ')\n                bridgedto   = re.sub(' +',' ',chan[160:181]).lstrip(' ').rstrip(' ')\n                currentChannel = Channel(channel,context,extension,priority,state,application,data,callerid,duration,accountcode,peeraccount,bridgedto)\n                currentChannelsArray.append(currentChannel)\n\n        internalCalls = 0\n        outboundCalls = 0\n        inboundCalls  = 0\n\n        for currentChannel in currentChannelsArray:\n            caller = \"N/A\"\n            called = \"N/A\"\n            callType = \"N/A\"\n\n            if \"Dial\" == currentChannel.Application or \"Queue\" == currentChannel.Application:\n                currentCall = Call(\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\")\n                currentCall.Caller = currentChannel.CallerId\n                currentCall.CallerChannel = currentChannel.Channel\n                currentCall.BridgedChannel = currentChannel.BridgedTo\n                currentCalls.append(currentCall)\n\n        for currentCall in currentCalls:\n            caller = \"N/A\"\n            called = \"N/A\"\n            callType = \"N/A\"\n            for currentChannel in currentChannelsArray:\n                if \"None\" not in currentChannel.BridgedTo:\n                    if currentCall.BridgedChannel == currentChannel.Channel:\n                        currentCall.Called = currentChannel.CallerId\n                        currentCall.CalledChannel = currentChannel.Channel\n\n        for currentCall in currentCalls:\n            if len(currentCall.Caller) <= extensionLength and len(currentCall.Called) <= extensionLength:\n                currentCall.CallType = \"Internal\"\n                internalCalls = internalCalls +1\n            if len(currentCall.Caller) > extensionLength and len(currentCall.Called) <= extensionLength:\n                currentCall.CallType = \"Inbound\"\n                inboundCalls = inboundCalls + 1\n            if len(currentCall.Caller) <= extensionLength and len(currentCall.Called) > extensionLength:\n                currentCall.CallType = \"Outbound\"\n                outboundCalls = outboundCalls + 1\n\n        self.gauge('asterisk.calls.internal',internalCalls)\n        self.gauge('asterisk.calls.inbound',inboundCalls)\n        self.gauge('asterisk.calls.outbound',outboundCalls)\n\n##### SIP Peers\n        sip_result = mgr.command('sip show peers')\n\n        sip_results = sip_result.data.split('\\n')\n\n        siptotals = sip_results[len(sip_results)-3]\n\n        siptotal = re.findall(r'([0-9]+) sip peer',siptotals)[0]\n\n        monitored_peers = re.findall(r'Monitored: ([0-9]+) online, ([0-9]+) offline',siptotals)[0]\n        unmonitored_peers = re.findall(r'Unmonitored: ([0-9]+) online, ([0-9]+) offline',siptotals)[0]\n\n        self.gauge('asterisk.sip.peers',siptotal)\n        self.gauge('asterisk.sip.monitored.online',monitored_peers[0])\n        self.gauge('asterisk.sip.monitored.offline',monitored_peers[1])\n        self.gauge('asterisk.sip.unmonitored.online',unmonitored_peers[0])\n        self.gauge('asterisk.sip.unmonitored.offline',unmonitored_peers[1])\n\n##### SIP Trunks (You have to add '-trunk' string into your SIP trunk name to detect it as a Trunk)\n        sip_total_trunks = 0\n        sip_online_trunks = 0\n        sip_offline_trunks = 0\n\n        trunks = re.finditer('^.*-trunk.*([OK|UN].*)', sip_result.data, re.MULTILINE)\n\n        for trunk in trunks:\n            sip_total_trunks +=1\n            if 'OK' in trunk.group():\n                sip_online_trunks += 1\n            else:\n                sip_offline_trunks += 1\n      \n        self.gauge('asterisk.sip.trunks.total',sip_total_trunks)\n        self.gauge('asterisk.sip.trunks.online',sip_online_trunks)\n        self.gauge('asterisk.sip.trunks.offline',sip_offline_trunks)\n\n##### PRI In Use\n\n        pri = mgr.command('pri show channels')\n\n        pri_channels = pri.data.split('\\n')\n\n        pri_channels[0] = None\n        pri_channels[1] = None\n\n        openchannels = 0\n        for chan in pri_channels:\n            if chan != None:\n                chan_data = chan.split()\n                if len(chan_data) > 2 and chan_data[3] == \"No\":\n                    openchannels += 1\n\n        self.gauge('asterisk.pri.channelsinuse',openchannels)\n\n##### IAX2 Peers\n\n        iax_result = mgr.command('iax2 show peers')\n\n        iax_results = iax_result.data.split('\\n')\n\n        iax_total_line = iax_results[len(iax_results)-3]\n\n        iax_peers_total = re.findall(r'([0-9]+) iax2 peers',iax_total_line)[0]\n        iax_peers_online = re.findall(r'\\[([0-9]+) online',iax_total_line)[0]\n        iax_peers_offline = re.findall(r'([0-9]+) offline',iax_total_line)[0]\n        iax_peers_unmonitored = re.findall(r'([0-9]+) unmonitored',iax_total_line)[0]\n\n        self.gauge('asterisk.iax2.peers',iax_peers_total)\n        self.gauge('asterisk.iax2.online',iax_peers_online)\n        self.gauge('asterisk.iax2.offline',iax_peers_offline)\n        self.gauge('asterisk.iax2.unmonitored',iax_peers_unmonitored)\n   \n##### DAHDI Channels  \n    \n        dahdi_result = mgr.command('dahdi show status')\n\n        dahdi_results = dahdi_result.data.split('\\n')\n\n        dahdi_total_trunks = len(dahdi_results)-3\n\n        dahdi_results[0] = None\n\n        dahdi_online_trunks = 0\n        dahdi_offline_trunks = 0\n\n        for chan in dahdi_results:\n            if chan != None:\n                chan_data = chan.split()\n\n                if len(chan_data) > 1:\n                    if \"Wildcard\" in chan_data[0]:\n                        if len(chan_data) > 2 and chan_data[2] == \"OK\":\n                            dahdi_online_trunks += 1\n                        if len(chan_data) > 2 and chan_data[2] == \"RED\":\n                            dahdi_offline_trunks += 1\n\n                    if \"wanpipe\" in chan_data[0]:\n                        if len(chan_data) > 2 and chan_data[3] == \"OK\":\n                            dahdi_online_trunks += 1\n                        if len(chan_data) > 2 and chan_data[3] == \"RED\":\n                            dahdi_offline_trunks += 1\n\n        self.gauge('asterisk.dahdi.total',dahdi_total_trunks)\n        self.gauge('asterisk.dahdi.online',dahdi_online_trunks)\n        self.gauge('asterisk.dahdi.offline',dahdi_offline_trunks)\n        \n##### G729 Codecs \n        \n        g729_result = mgr.command('g729 show licenses')\n\n        g729_results = g729_result.data.split('\\n')\n\n        g729_total_line = g729_results[0]\n\n        g729_total = re.findall(r'([0-9]+) licensed',g729_total_line)[0]\n        g729_encoders = re.split('/',g729_total_line)[0]\n        g729_decoders = re.findall(r'([0-9]+) encoders/decoders',g729_total_line)[0]\n\n        self.gauge('asterisk.g729.total',g729_total)\n        self.gauge('asterisk.g729.encoders',g729_encoders)\n        self.gauge('asterisk.g729.decoders',g729_decoders)\n        \n\n##### Asterisk Uptime\n\n        uptime_result = mgr.command('core show uptime')\n        \n        uptime_results = uptime_result.data.split('\\n')\n        \n        system_total_line = uptime_results[0]\n        asterisk_total_line = uptime_results[1]\n        \n        system_uptime_days = 0\n        system_uptime_hours = 0\n        system_uptime_minutes = 0\n        system_uptime_seconds = 0\n        \n        system_uptime_days = 0\n        system_uptime_hours = 0\n        system_uptime_minutes = 0\n        system_uptime_seconds = 0\n\n        if \"day\" in system_total_line:\n            system_uptime_days = re.findall(r'([0-9]+) day',system_total_line)[0]\n        if \"hour\" in system_total_line:\n            system_uptime_hours = re.findall(r'([0-9]+) hour',system_total_line)[0]\n        if \"minute\" in system_total_line:\n            system_uptime_minutes = re.findall(r'([0-9]+) minute',system_total_line)[0]\n        if \"second\" in system_total_line:\n            system_uptime_seconds = re.findall(r'([0-9]+) second',system_total_line)[0]\n\n        system_uptime = ( int(system_uptime_days) * 86400) +  ( int(system_uptime_hours) * 3600) + ( int(system_uptime_minutes) * 60) + int(system_uptime_seconds)\n        \n        asterisk_last_reload_days = 0\n        asterisk_last_reload_hours = 0\n        asterisk_last_reload_minutes = 0\n        asterisk_last_reload_seconds = 0\n        \n        if \"day\" in asterisk_total_line:\n            asterisk_last_reload_days = re.findall(r'([0-9]+) day',asterisk_total_line)[0]\n        if \"hour\" in asterisk_total_line:\n            asterisk_last_reload_hours = re.findall(r'([0-9]+) hour',asterisk_total_line)[0]\n        if \"minute\" in asterisk_total_line:\n            asterisk_last_reload_minutes = re.findall(r'([0-9]+) minute',asterisk_total_line)[0]\n        if \"second\" in asterisk_total_line:\n            asterisk_last_reload_seconds = re.findall(r' ([0-9]+) second',asterisk_total_line)[0]\n\n        asterisk_last_reload = ( int(asterisk_last_reload_days) * 86400) + ( int(asterisk_last_reload_hours) * 3600) + ( int(asterisk_last_reload_minutes) * 60) + int(asterisk_last_reload_seconds)\n\n        self.gauge('asterisk.system.uptime',system_uptime)\n        self.gauge('asterisk.last.reload',asterisk_last_reload)\n        \n##### MFCR2 Channels\n\n        mfcr2_result = mgr.command('mfcr2 show channels')\n\n        mfcr2_results = mfcr2_result.data.split('\\n')\n\n        mfcr2_total_channels = len(mfcr2_results)-3\n\n        mfcr2_results[0] = None\n\n        mfcr2_inuse_channels = 0\n        mfcr2_available_channels = 0\n        mfcr2_blocked_channels = 0\n\n        for chan in mfcr2_results:\n            if chan != None:\n                chan_data = chan.split()\n                if len(chan_data) > 2:\n                    if \"IDLE\" in chan_data[6] and \"IDLE\" in chan_data[7] :\n                        mfcr2_available_channels += 1\n                    if \"ANSWER\" in chan_data[6] or \"ANSWER\" in chan_data[7] :\n                        mfcr2_inuse_channels += 1\n                    if \"BLOCK\" in chan_data[6] or \"BLOCK\" in chan_data[7] :\n                        mfcr2_blocked_channels += 1\n                        \n        self.gauge('asterisk.mfcr2.total.channels',mfcr2_total_channels)\n        self.gauge('asterisk.mfcr2.available.channels',mfcr2_available_channels)\n        self.gauge('asterisk.mfcr2.inuse.channels',mfcr2_inuse_channels)\n        self.gauge('asterisk.mfcr2.blocked.channels',mfcr2_blocked_channels)\n\n##### SCCP Devices\n\n        sccp_total_devices = 0\n        sccp_online_devices = 0\n        sccp_offline_devices = 0\n        \n        sccp_result = mgr.command('sccp show devices')\n\n        if \"No such command\" not in sccp_result.data:\n            \n            sccp_devices = re.finditer('^.*.SEP.*', sccp_result.data, re.MULTILINE)\n\n            for sccp_device in sccp_devices:\n                sccp_total_devices +=1\n                if '--' in sccp_device.group():\n                    sccp_offline_devices += 1\n                else:\n                    sccp_online_devices += 1\n\n        self.gauge('asterisk.sccp.devices.total',sccp_total_devices)\n        self.gauge('asterisk.sccp.devices.online',sccp_online_devices)\n        self.gauge('asterisk.sccp.devices.offline',sccp_offline_devices)\n                    \n\n##### Close connection\n\n        mgr.close()"}
{"input": "def all_keys(module, keyring, short_format):\n    if keyring:\n        cmd = \"apt-key --keyring %s adv --list-public-keys --keyid-format=long\" % keyring\n    else:\n        cmd = \"apt-key adv --list-public-keys --keyid-format=long\"\n    (rc, out, err) = module.run_command(cmd)\n    results = []\n    lines = out.split('\\n')\n    for line in lines:\n        if line.startswith(\"pub\"):\n            tokens = line.split()\n            code = tokens[1]\n            (len_type, real_code) = code.split(\"/\")\n            results.append(real_code)\n    if short_format:\n        results = shorten_key_ids(results)\n    return results", "output": "def all_keys(module, keyring, short_format):\n    if keyring:\n        cmd = \"apt-key --keyring %s adv --list-public-keys --keyid-format=long\" % keyring\n    else:\n        cmd = \"apt-key adv --list-public-keys --keyid-format=long\"\n    (rc, out, err) = module.run_command(cmd)\n    results = []\n    lines = out.split('\\n')\n    for line in lines:\n        if line.startswith(\"pub\") or line.startswith(\"sub\"):\n            tokens = line.split()\n            code = tokens[1]\n            (len_type, real_code) = code.split(\"/\")\n            results.append(real_code)\n    if short_format:\n        results = shorten_key_ids(results)\n    return results"}
{"input": "def validate_hstore(value, is_serialized=False):\n    \"\"\" HSTORE validation. \"\"\"\n    # if empty\n    if value == '' or value == 'null':\n        value = '{}'\n\n    # ensure valid JSON\n    try:\n        # convert strings to dictionaries\n        if isinstance(value, six.string_types):\n            dictionary = json.loads(value)\n\n            # if serialized field, deserialize values\n            if is_serialized and isinstance(dictionary, dict):\n                dictionary = dict((k, json.loads(v)) for k, v in dictionary.items())  # TODO: modify to use field's deserializer\n        # if not a string we'll check at the next control if it's a dict\n        else:\n            dictionary = value\n    except ValueError as e:\n        raise ValidationError(ugettext(u'Invalid JSON: {0}').format(e))\n\n    # ensure is a dictionary\n    if not isinstance(dictionary, dict):\n        raise ValidationError(ugettext(u'No lists or values allowed, only dictionaries'))\n\n    # convert any non string object into string\n    for key, value in dictionary.items():\n        if isinstance(value, dict) or isinstance(value, list):\n            dictionary[key] = json.dumps(value)\n        if isinstance(value, bool) or isinstance(value, int) or isinstance(value, float):\n            if not is_serialized:  # Only convert if not from serializedfield\n                dictionary[key] = six.text_type(value).lower()\n\n    return dictionary", "output": "def validate_hstore(value, is_serialized=False):\n    \"\"\" HSTORE validation. \"\"\"\n    # if empty\n    if value is None or value == '' or value == 'null':\n        value = '{}'\n\n    # ensure valid JSON\n    try:\n        # convert strings to dictionaries\n        if isinstance(value, six.string_types):\n            dictionary = json.loads(value)\n\n            # if serialized field, deserialize values\n            if is_serialized and isinstance(dictionary, dict):\n                dictionary = dict((k, json.loads(v)) for k, v in dictionary.items())  # TODO: modify to use field's deserializer\n        # if not a string we'll check at the next control if it's a dict\n        else:\n            dictionary = value\n    except ValueError as e:\n        raise ValidationError(ugettext(u'Invalid JSON: {0}').format(e))\n\n    # ensure is a dictionary\n    if not isinstance(dictionary, dict):\n        raise ValidationError(ugettext(u'No lists or values allowed, only dictionaries'))\n\n    # convert any non string object into string\n    for key, value in dictionary.items():\n        if isinstance(value, dict) or isinstance(value, list):\n            dictionary[key] = json.dumps(value)\n        if isinstance(value, bool) or isinstance(value, int) or isinstance(value, float):\n            if not is_serialized:  # Only convert if not from serializedfield\n                dictionary[key] = six.text_type(value).lower()\n\n    return dictionary"}
{"input": "def build_module():\n    csrc = MODULE_PATH[0:-3] + '.c'\n    po = Popen(['cc', '-o', MODULE_PATH, '-shared', csrc])\n    po.communicate()\n    if po.returncode != 0:\n        raise Exception('Failed to compile module')", "output": "def build_module():\n    csrc = MODULE_PATH[0:-3] + '.c'\n    po = Popen(['cc', '-o', MODULE_PATH, '-shared', '-fPIC', csrc])\n    po.communicate()\n    if po.returncode != 0:\n        raise Exception('Failed to compile module')"}
{"input": "def add(request):\n    if request.method == 'POST':\n        form = CredForm(request.user, request.POST)\n        if form.is_valid():\n            form.save()\n            CredAudit(audittype=CredAudit.CREDADD, cred=form.instance, user=request.user).save()\n            return HttpResponseRedirect('/cred/list')\n    else:\n        form = CredForm(request.user)\n\n    return render(request, 'cred_edit.html', {'form': form, 'action':\n        '/cred/add/'})", "output": "def add(request):\n    if request.method == 'POST':\n        form = CredForm(request.user, request.POST)\n        if form.is_valid():\n            form.save()\n            CredAudit(audittype=CredAudit.CREDADD, cred=form.instance, user=request.user).save()\n            return HttpResponseRedirect('/cred/list')\n    else:\n        form = CredForm(request.user)\n\n    return render(request, 'cred_edit.html', {'form': form, 'action':\n      '/cred/add/', 'icons': CredIcon.objects.all()})"}
{"input": "def test_unparse_parse(self):\n        for u in ['Python', './Python','x-newscheme://foo.com/stuff']:\n            self.assertEqual(urlparse.urlunsplit(urlparse.urlsplit(u)), u)\n            self.assertEqual(urlparse.urlunparse(urlparse.urlparse(u)), u)", "output": "def test_unparse_parse(self):\n        for u in ['Python', './Python','x-newscheme://foo.com/stuff','x://y','x:/y','x:/','/',]:\n            self.assertEqual(urlparse.urlunsplit(urlparse.urlsplit(u)), u)\n            self.assertEqual(urlparse.urlunparse(urlparse.urlparse(u)), u)"}
{"input": "def convert_detection_postprocess(self, op):\n        \"\"\"Convert TFLite_Detection_PostProcess\"\"\"\n        flexbuffer = op.CustomOptionsAsNumpy().tobytes()\n        custom_options = FlexBufferDecoder(flexbuffer).decode()\n\n        if \"use_regular_nms\" in custom_options:\n            if custom_options[\"use_regular_nms\"]:\n                raise tvm.error.OpAttributeUnImplemented(\n                    \"use_regular_nms=True is not yet supported for operator {}.\".format(\n                        \"TFLite_Detection_PostProcess\"\n                    )\n                )\n\n        inputs = self.get_input_tensors(op)\n        assert len(inputs) == 3, \"inputs length should be 3\"\n        cls_pred = self.get_expr(inputs[1].tensor_idx)\n        loc_prob = self.get_expr(inputs[0].tensor_idx)\n        batch_size = inputs[1].tensor.Shape(0)\n        anchor_values = self.get_tensor_value(inputs[2])\n        anchor_boxes = len(anchor_values)\n        anchor_type = self.get_tensor_type_str(inputs[2].tensor.Type())\n        anchor_expr = self.exp_tab.new_const(anchor_values, dtype=anchor_type)\n\n        if inputs[0].qnn_params:\n            loc_prob = _qnn.op.dequantize(\n                data=loc_prob,\n                input_scale=inputs[0].qnn_params[\"scale\"],\n                input_zero_point=inputs[0].qnn_params[\"zero_point\"],\n            )\n        if inputs[1].qnn_params:\n            cls_pred = _qnn.op.dequantize(\n                data=cls_pred,\n                input_scale=inputs[1].qnn_params[\"scale\"],\n                input_zero_point=inputs[1].qnn_params[\"zero_point\"],\n            )\n        if inputs[2].qnn_params:\n            anchor_expr = _qnn.op.dequantize(\n                data=anchor_expr,\n                input_scale=inputs[2].qnn_params[\"scale\"],\n                input_zero_point=inputs[2].qnn_params[\"zero_point\"],\n            )\n\n        # reshape the cls_pred and loc_prob tensors so\n        # they can be consumed by multibox_transform_loc\n        cls_pred = _op.transpose(cls_pred, [0, 2, 1])\n        # loc_prob coords are in yxhw format\n        # need to convert to xywh\n        loc_coords = _op.split(loc_prob, 4, axis=2)\n        loc_prob = _op.concatenate(\n            [loc_coords[1], loc_coords[0], loc_coords[3], loc_coords[2]], axis=2\n        )\n        loc_prob = _op.reshape(loc_prob, [batch_size, anchor_boxes * 4])\n\n        # anchor coords are in yxhw format\n        # need to convert to ltrb\n        anchor_coords = _op.split(anchor_expr, 4, axis=1)\n        anchor_y = anchor_coords[0]\n        anchor_x = anchor_coords[1]\n        anchor_h = anchor_coords[2]\n        anchor_w = anchor_coords[3]\n        plus_half = _expr.const(0.5, dtype=\"float32\")\n        minus_half = _expr.const(-0.5, dtype=\"float32\")\n        anchor_l = _op.add(anchor_x, _op.multiply(anchor_w, minus_half))\n        anchor_r = _op.add(anchor_x, _op.multiply(anchor_w, plus_half))\n        anchor_t = _op.add(anchor_y, _op.multiply(anchor_h, minus_half))\n        anchor_b = _op.add(anchor_y, _op.multiply(anchor_h, plus_half))\n        anchor_expr = _op.concatenate([anchor_l, anchor_t, anchor_r, anchor_b], axis=1)\n        anchor_expr = _op.expand_dims(anchor_expr, 0)\n\n        # attributes for multibox_transform_loc\n        multibox_transform_loc_attrs = {}\n        multibox_transform_loc_attrs[\"clip\"] = False\n        multibox_transform_loc_attrs[\"threshold\"] = custom_options[\"nms_score_threshold\"]\n        multibox_transform_loc_attrs[\"variances\"] = (\n            1 / custom_options[\"x_scale\"],\n            1 / custom_options[\"y_scale\"],\n            1 / custom_options[\"w_scale\"],\n            1 / custom_options[\"h_scale\"],\n        )\n\n        # attributes for non_max_suppression\n        non_max_suppression_attrs = {}\n        non_max_suppression_attrs[\"return_indices\"] = False\n        non_max_suppression_attrs[\"iou_threshold\"] = custom_options[\"nms_iou_threshold\"]\n        non_max_suppression_attrs[\"force_suppress\"] = False\n        non_max_suppression_attrs[\"top_k\"] = anchor_boxes\n        non_max_suppression_attrs[\"max_output_size\"] = custom_options[\"max_detections\"]\n        non_max_suppression_attrs[\"invalid_to_bottom\"] = False\n\n        ret = _op.vision.multibox_transform_loc(\n            cls_pred, loc_prob, anchor_expr, **multibox_transform_loc_attrs\n        )\n        ret = _op.vision.non_max_suppression(ret[0], ret[1], ret[1], **non_max_suppression_attrs)\n        ret = _op.vision.get_valid_counts(ret, 0)\n        valid_count = ret[0]\n        # keep only the top 'max_detections' rows\n        ret = _op.strided_slice(\n            ret[1], [0, 0, 0], [batch_size, custom_options[\"max_detections\"], anchor_boxes]\n        )\n        # the output needs some reshaping to match tflite\n        ret = _op.split(ret, 6, axis=2)\n        cls_ids = _op.reshape(ret[0], [batch_size, -1])\n        scores = _op.reshape(ret[1], [batch_size, -1])\n        boxes = _op.concatenate([ret[3], ret[2], ret[5], ret[4]], axis=2)\n        ret = _expr.TupleWrapper(_expr.Tuple([boxes, cls_ids, scores, valid_count]), size=4)\n        return ret", "output": "def convert_detection_postprocess(self, op):\n        \"\"\"Convert TFLite_Detection_PostProcess\"\"\"\n        flexbuffer = op.CustomOptionsAsNumpy().tobytes()\n        custom_options = FlexBufferDecoder(flexbuffer).decode()\n\n        if \"use_regular_nms\" in custom_options:\n            if custom_options[\"use_regular_nms\"]:\n                raise tvm.error.OpAttributeUnImplemented(\n                    \"use_regular_nms=True is not yet supported for operator {}.\".format(\n                        \"TFLite_Detection_PostProcess\"\n                    )\n                )\n\n        inputs = self.get_input_tensors(op)\n        assert len(inputs) == 3, \"inputs length should be 3\"\n        cls_pred = self.get_expr(inputs[1].tensor_idx)\n        loc_prob = self.get_expr(inputs[0].tensor_idx)\n        batch_size = inputs[1].tensor.Shape(0)\n        anchor_values = self.get_tensor_value(inputs[2])\n        anchor_boxes = len(anchor_values)\n        anchor_type = self.get_tensor_type_str(inputs[2].tensor.Type())\n        anchor_expr = self.exp_tab.new_const(anchor_values, dtype=anchor_type)\n\n        if inputs[0].qnn_params:\n            loc_prob = _qnn.op.dequantize(\n                data=loc_prob,\n                input_scale=inputs[0].qnn_params[\"scale\"],\n                input_zero_point=inputs[0].qnn_params[\"zero_point\"],\n            )\n        if inputs[1].qnn_params:\n            cls_pred = _qnn.op.dequantize(\n                data=cls_pred,\n                input_scale=inputs[1].qnn_params[\"scale\"],\n                input_zero_point=inputs[1].qnn_params[\"zero_point\"],\n            )\n        if inputs[2].qnn_params:\n            anchor_expr = _qnn.op.dequantize(\n                data=anchor_expr,\n                input_scale=inputs[2].qnn_params[\"scale\"],\n                input_zero_point=inputs[2].qnn_params[\"zero_point\"],\n            )\n\n        # reshape the cls_pred and loc_prob tensors so\n        # they can be consumed by multibox_transform_loc\n        cls_pred = _op.transpose(cls_pred, [0, 2, 1])\n        # loc_prob coords are in yxhw format\n        # need to convert to xywh\n        loc_coords = _op.split(loc_prob, 4, axis=2)\n        loc_prob = _op.concatenate(\n            [loc_coords[1], loc_coords[0], loc_coords[3], loc_coords[2]], axis=2\n        )\n        loc_prob = _op.reshape(loc_prob, [batch_size, anchor_boxes * 4])\n\n        # anchor coords are in yxhw format\n        # need to convert to ltrb\n        anchor_coords = _op.split(anchor_expr, 4, axis=1)\n        anchor_y = anchor_coords[0]\n        anchor_x = anchor_coords[1]\n        anchor_h = anchor_coords[2]\n        anchor_w = anchor_coords[3]\n        plus_half = _expr.const(0.5, dtype=\"float32\")\n        minus_half = _expr.const(-0.5, dtype=\"float32\")\n        anchor_l = _op.add(anchor_x, _op.multiply(anchor_w, minus_half))\n        anchor_r = _op.add(anchor_x, _op.multiply(anchor_w, plus_half))\n        anchor_t = _op.add(anchor_y, _op.multiply(anchor_h, minus_half))\n        anchor_b = _op.add(anchor_y, _op.multiply(anchor_h, plus_half))\n        anchor_expr = _op.concatenate([anchor_l, anchor_t, anchor_r, anchor_b], axis=1)\n        anchor_expr = _op.expand_dims(anchor_expr, 0)\n\n        # attributes for multibox_transform_loc\n        multibox_transform_loc_attrs = {}\n        multibox_transform_loc_attrs[\"clip\"] = False\n        multibox_transform_loc_attrs[\"threshold\"] = custom_options[\"nms_score_threshold\"]\n        multibox_transform_loc_attrs[\"variances\"] = (\n            1 / custom_options[\"x_scale\"],\n            1 / custom_options[\"y_scale\"],\n            1 / custom_options[\"w_scale\"],\n            1 / custom_options[\"h_scale\"],\n        )\n\n        # attributes for non_max_suppression\n        non_max_suppression_attrs = {}\n        non_max_suppression_attrs[\"return_indices\"] = False\n        non_max_suppression_attrs[\"iou_threshold\"] = custom_options[\"nms_iou_threshold\"]\n        non_max_suppression_attrs[\"force_suppress\"] = False\n        non_max_suppression_attrs[\"top_k\"] = anchor_boxes\n        non_max_suppression_attrs[\"max_output_size\"] = custom_options[\"max_detections\"]\n        non_max_suppression_attrs[\"invalid_to_bottom\"] = False\n\n        ret = _op.vision.multibox_transform_loc(\n            cls_pred, loc_prob, anchor_expr, **multibox_transform_loc_attrs\n        )\n        ret = _op.vision.non_max_suppression(ret[0], ret[1], ret[1], **non_max_suppression_attrs)\n        ret = _op.vision.get_valid_counts(ret, 0)\n        valid_count = ret[0]\n        # keep only the top 'max_detections' rows\n        ret = _op.strided_slice(\n            ret[1], [0, 0, 0], [batch_size, custom_options[\"max_detections\"], 6]\n        )\n        # the output needs some reshaping to match tflite\n        ret = _op.split(ret, 6, axis=2)\n        cls_ids = _op.reshape(ret[0], [batch_size, -1])\n        scores = _op.reshape(ret[1], [batch_size, -1])\n        boxes = _op.concatenate([ret[3], ret[2], ret[5], ret[4]], axis=2)\n        ret = _expr.TupleWrapper(_expr.Tuple([boxes, cls_ids, scores, valid_count]), size=4)\n        return ret"}
{"input": "def main():\n    module = AnsibleModule(\n        argument_spec = dict(\n            src = dict(required=True),\n            dest = dict(required=True),\n            dest_port = dict(default=None, type='int'),\n            delete = dict(default='no', type='bool'),\n            private_key = dict(default=None),\n            rsync_path = dict(default=None),\n            _local_rsync_path = dict(default='rsync', type='path'),\n            _substitute_controller = dict(default='no', type='bool'),\n            archive = dict(default='yes', type='bool'),\n            checksum = dict(default='no', type='bool'),\n            compress = dict(default='yes', type='bool'),\n            existing_only = dict(default='no', type='bool'),\n            dirs  = dict(default='no', type='bool'),\n            recursive = dict(type='bool'),\n            links = dict(type='bool'),\n            copy_links = dict(default='no', type='bool'),\n            perms = dict(type='bool'),\n            times = dict(type='bool'),\n            owner = dict(type='bool'),\n            group = dict(type='bool'),\n            set_remote_user = dict(default='yes', type='bool'),\n            rsync_timeout = dict(type='int', default=0),\n            rsync_opts = dict(type='list'),\n            ssh_args = dict(type='str'),\n            partial = dict(default='no', type='bool'),\n            verify_host = dict(default='no', type='bool'),\n            mode = dict(default='push', choices=['push', 'pull']),\n        ),\n        supports_check_mode = True\n    )\n\n    if module.params['_substitute_controller']:\n        try:\n            source = substitute_controller(module.params['src'])\n            dest = substitute_controller(module.params['dest'])\n        except ValueError:\n            module.fail_json(msg='Could not determine controller hostname for rsync to send to')\n    else:\n        source = module.params['src']\n        dest = module.params['dest']\n    dest_port = module.params['dest_port']\n    delete = module.params['delete']\n    private_key = module.params['private_key']\n    rsync_path = module.params['rsync_path']\n    rsync = module.params.get('_local_rsync_path', 'rsync')\n    rsync_timeout = module.params.get('rsync_timeout', 'rsync_timeout')\n    archive = module.params['archive']\n    checksum = module.params['checksum']\n    compress = module.params['compress']\n    existing_only = module.params['existing_only']\n    dirs = module.params['dirs']\n    partial = module.params['partial']\n    # the default of these params depends on the value of archive\n    recursive = module.params['recursive']\n    links = module.params['links']\n    copy_links = module.params['copy_links']\n    perms = module.params['perms']\n    times = module.params['times']\n    owner = module.params['owner']\n    group = module.params['group']\n    rsync_opts = module.params['rsync_opts']\n    ssh_args = module.params['ssh_args']\n    verify_host = module.params['verify_host']\n\n    if '/' not in rsync:\n        rsync = module.get_bin_path(rsync, required=True)\n\n    cmd = [rsync, '--delay-updates', '-F']\n    if compress:\n        cmd.append('--compress')\n    if rsync_timeout:\n        cmd.append('--timeout=%s' % rsync_timeout)\n    if module.check_mode:\n        cmd.append('--dry-run')\n    if delete:\n        cmd.append('--delete-after')\n    if existing_only:\n        cmd.append('--existing')\n    if checksum:\n        cmd.append('--checksum')\n    if copy_links:\n        cmd.append('--copy-links')\n    if archive:\n        cmd.append('--archive')\n        if recursive is False:\n            cmd.append('--no-recursive')\n        if links is False:\n            cmd.append('--no-links')\n        if perms is False:\n            cmd.append('--no-perms')\n        if times is False:\n            cmd.append('--no-times')\n        if owner is False:\n            cmd.append('--no-owner')\n        if group is False:\n            cmd.append('--no-group')\n    else:\n        if recursive is True:\n            cmd.append('--recursive')\n        if links is True:\n            cmd.append('--links')\n        if perms is True:\n            cmd.append('--perms')\n        if times is True:\n            cmd.append('--times')\n        if owner is True:\n            cmd.append('--owner')\n        if group is True:\n            cmd.append('--group')\n    if dirs:\n        cmd.append('--dirs')\n\n    if source.startswith('rsync://') and dest.startswith('rsync://'):\n        module.fail_json(msg='either src or dest must be a localhost', rc=1)\n\n    if is_rsh_needed(source, dest):\n        ssh_cmd = [module.get_bin_path('ssh', required=True), '-S', 'none']\n        if private_key is not None:\n            ssh_cmd.extend(['-i', private_key])\n        # If the user specified a port value\n        # Note:  The action plugin takes care of setting this to a port from\n        # inventory if the user didn't specify an explicit dest_port\n        if dest_port is not None:\n            ssh_cmd.extend(['-o', 'Port=%s' % dest_port])\n        if not verify_host:\n            ssh_cmd.extend(['-o', 'StrictHostKeyChecking=no'])\n        ssh_cmd_str = ' '.join(shlex_quote(arg) for arg in ssh_cmd)\n        if ssh_args:\n            ssh_cmd_str += ' %s' % ssh_args\n        cmd.append('--rsh=%s' % ssh_cmd_str)\n\n    if rsync_path:\n        cmd.append('--rsync-path=%s' % rsync_path)\n\n    if rsync_opts:\n        cmd.extend(rsync_opts)\n\n    if partial:\n        cmd.append('--partial')\n\n    changed_marker = '<<CHANGED>>'\n    cmd.append('--out-format=' + changed_marker + '%i %n%L')\n\n    # expand the paths\n    if '@' not in source:\n        source = os.path.expanduser(source)\n    if '@' not in dest:\n        dest = os.path.expanduser(dest)\n\n    cmd.append(source)\n    cmd.append(dest)\n    cmdstr = ' '.join(cmd)\n    (rc, out, err) = module.run_command(cmd)\n    if rc:\n        return module.fail_json(msg=err, rc=rc, cmd=cmdstr)\n    else:\n        changed = changed_marker in out\n        out_clean = out.replace(changed_marker, '')\n        out_lines = out_clean.split('\\n')\n        while '' in out_lines:\n            out_lines.remove('')\n        if module._diff:\n            diff = {'prepared': out_clean}\n            return module.exit_json(changed=changed, msg=out_clean,\n                                    rc=rc, cmd=cmdstr, stdout_lines=out_lines,\n                                    diff=diff)\n        else:\n            return module.exit_json(changed=changed, msg=out_clean,\n                                    rc=rc, cmd=cmdstr, stdout_lines=out_lines)", "output": "def main():\n    module = AnsibleModule(\n        argument_spec = dict(\n            src = dict(required=True),\n            dest = dict(required=True),\n            dest_port = dict(default=None, type='int'),\n            delete = dict(default='no', type='bool'),\n            private_key = dict(default=None),\n            rsync_path = dict(default=None),\n            _local_rsync_path = dict(default='rsync', type='path'),\n            _substitute_controller = dict(default='no', type='bool'),\n            archive = dict(default='yes', type='bool'),\n            checksum = dict(default='no', type='bool'),\n            compress = dict(default='yes', type='bool'),\n            existing_only = dict(default='no', type='bool'),\n            dirs  = dict(default='no', type='bool'),\n            recursive = dict(type='bool'),\n            links = dict(type='bool'),\n            copy_links = dict(default='no', type='bool'),\n            perms = dict(type='bool'),\n            times = dict(type='bool'),\n            owner = dict(type='bool'),\n            group = dict(type='bool'),\n            set_remote_user = dict(default='yes', type='bool'),\n            rsync_timeout = dict(type='int', default=0),\n            rsync_opts = dict(type='list'),\n            ssh_args = dict(type='str'),\n            partial = dict(default='no', type='bool'),\n            verify_host = dict(default='no', type='bool'),\n            mode = dict(default='push', choices=['push', 'pull']),\n        ),\n        supports_check_mode = True\n    )\n\n    if module.params['_substitute_controller']:\n        try:\n            source = substitute_controller(module.params['src'])\n            dest = substitute_controller(module.params['dest'])\n        except ValueError:\n            module.fail_json(msg='Could not determine controller hostname for rsync to send to')\n    else:\n        source = module.params['src']\n        dest = module.params['dest']\n    dest_port = module.params['dest_port']\n    delete = module.params['delete']\n    private_key = module.params['private_key']\n    rsync_path = module.params['rsync_path']\n    rsync = module.params.get('_local_rsync_path', 'rsync')\n    rsync_timeout = module.params.get('rsync_timeout', 'rsync_timeout')\n    archive = module.params['archive']\n    checksum = module.params['checksum']\n    compress = module.params['compress']\n    existing_only = module.params['existing_only']\n    dirs = module.params['dirs']\n    partial = module.params['partial']\n    # the default of these params depends on the value of archive\n    recursive = module.params['recursive']\n    links = module.params['links']\n    copy_links = module.params['copy_links']\n    perms = module.params['perms']\n    times = module.params['times']\n    owner = module.params['owner']\n    group = module.params['group']\n    rsync_opts = module.params['rsync_opts']\n    ssh_args = module.params['ssh_args']\n    verify_host = module.params['verify_host']\n\n    if '/' not in rsync:\n        rsync = module.get_bin_path(rsync, required=True)\n\n    cmd = [rsync, '--delay-updates', '-F']\n    if compress:\n        cmd.append('--compress')\n    if rsync_timeout:\n        cmd.append('--timeout=%s' % rsync_timeout)\n    if module.check_mode:\n        cmd.append('--dry-run')\n    if delete:\n        cmd.append('--delete-after')\n    if existing_only:\n        cmd.append('--existing')\n    if checksum:\n        cmd.append('--checksum')\n    if copy_links:\n        cmd.append('--copy-links')\n    if archive:\n        cmd.append('--archive')\n        if recursive is False:\n            cmd.append('--no-recursive')\n        if links is False:\n            cmd.append('--no-links')\n        if perms is False:\n            cmd.append('--no-perms')\n        if times is False:\n            cmd.append('--no-times')\n        if owner is False:\n            cmd.append('--no-owner')\n        if group is False:\n            cmd.append('--no-group')\n    else:\n        if recursive is True:\n            cmd.append('--recursive')\n        if links is True:\n            cmd.append('--links')\n        if perms is True:\n            cmd.append('--perms')\n        if times is True:\n            cmd.append('--times')\n        if owner is True:\n            cmd.append('--owner')\n        if group is True:\n            cmd.append('--group')\n    if dirs:\n        cmd.append('--dirs')\n\n    if source.startswith('rsync://') and dest.startswith('rsync://'):\n        module.fail_json(msg='either src or dest must be a localhost', rc=1)\n\n    if is_rsh_needed(source, dest):\n        ssh_cmd = [module.get_bin_path('ssh', required=True), '-S', 'none']\n        if private_key is not None:\n            ssh_cmd.extend(['-i', private_key])\n        # If the user specified a port value\n        # Note:  The action plugin takes care of setting this to a port from\n        # inventory if the user didn't specify an explicit dest_port\n        if dest_port is not None:\n            ssh_cmd.extend(['-o', 'Port=%s' % dest_port])\n        if not verify_host:\n            ssh_cmd.extend(['-o', 'StrictHostKeyChecking=no', '-o', 'UserKnownHostsFile=/dev/null'])\n        ssh_cmd_str = ' '.join(shlex_quote(arg) for arg in ssh_cmd)\n        if ssh_args:\n            ssh_cmd_str += ' %s' % ssh_args\n        cmd.append('--rsh=%s' % ssh_cmd_str)\n\n    if rsync_path:\n        cmd.append('--rsync-path=%s' % rsync_path)\n\n    if rsync_opts:\n        cmd.extend(rsync_opts)\n\n    if partial:\n        cmd.append('--partial')\n\n    changed_marker = '<<CHANGED>>'\n    cmd.append('--out-format=' + changed_marker + '%i %n%L')\n\n    # expand the paths\n    if '@' not in source:\n        source = os.path.expanduser(source)\n    if '@' not in dest:\n        dest = os.path.expanduser(dest)\n\n    cmd.append(source)\n    cmd.append(dest)\n    cmdstr = ' '.join(cmd)\n    (rc, out, err) = module.run_command(cmd)\n    if rc:\n        return module.fail_json(msg=err, rc=rc, cmd=cmdstr)\n    else:\n        changed = changed_marker in out\n        out_clean = out.replace(changed_marker, '')\n        out_lines = out_clean.split('\\n')\n        while '' in out_lines:\n            out_lines.remove('')\n        if module._diff:\n            diff = {'prepared': out_clean}\n            return module.exit_json(changed=changed, msg=out_clean,\n                                    rc=rc, cmd=cmdstr, stdout_lines=out_lines,\n                                    diff=diff)\n        else:\n            return module.exit_json(changed=changed, msg=out_clean,\n                                    rc=rc, cmd=cmdstr, stdout_lines=out_lines)"}
{"input": "def test_load_freesolv_gaffmol2_vs_sybylmol2_vs_obabelpdb(get_fn, tmpdir):\n    tar_filename = \"freesolve_v0.3.tar.bz2\"\n    tar = tarfile.open(get_fn(tar_filename), mode=\"r:bz2\")\n    os.chdir(str(tmpdir))\n    tar.extractall()\n    tar.close()\n\n    with open(\"./v0.3/database.pickle\", 'rb') as f:\n        database = pickle.load(f)\n\n    for key in database:\n        gaff_filename = \"./v0.3/mol2files_gaff/%s.mol2\" % key\n        pdb_filename = \"./v0.3/mol2files_gaff/%s.pdb\" % key\n        sybyl_filename = \"./v0.3/mol2files_sybyl/%s.mol2\" % key\n\n        cmd = \"obabel -imol2 %s -opdb > %s 2>/dev/null\" % (sybyl_filename, pdb_filename)\n        assert os.system(cmd) == 0\n\n        t_pdb = md.load(pdb_filename)\n        t_gaff = md.load(gaff_filename)\n        t_sybyl = md.load(sybyl_filename)\n\n        eq(t_pdb.n_atoms, t_gaff.n_atoms)\n        eq(t_pdb.n_atoms, t_sybyl.n_atoms)\n\n        eq(t_pdb.n_frames, t_gaff.n_frames)\n        eq(t_pdb.n_frames, t_gaff.n_frames)\n\n        eq(t_pdb.xyz, t_gaff.xyz, decimal=4)\n        eq(t_pdb.xyz, t_sybyl.xyz, decimal=4)\n\n        top_pdb, bonds_pdb = t_pdb.top.to_dataframe()\n        top_gaff, bonds_gaff = t_gaff.top.to_dataframe()\n        top_sybyl, bonds_sybyl = t_sybyl.top.to_dataframe()\n\n        eq(top_sybyl.name.values, top_pdb.name.values)\n\n        # eq(top_gaff.name.values, top_sybyl.name.values)  # THEY CAN HAVE DIFFERENT NAMES, so this isn't TRUE!\n\n        def make_bonds_comparable(bond_array):\n            \"\"\"Create a bond connectivity matrix from a numpy array of atom pairs.  Avoids having to compare the order in which bonds are listed.\"\"\"\n            n_bonds = len(bond_array)\n            data = np.ones(n_bonds)\n            i = bond_array[:, 0]\n            j = bond_array[:, 1]\n            matrix = scipy.sparse.coo_matrix((data, (i, j)), shape=(t_pdb.n_atoms, t_pdb.n_atoms)).toarray()\n            return matrix + matrix.T  # Symmetrize to account for (a ~ b) versus (b ~ a)\n\n        bond_matrix_pdb = make_bonds_comparable(bonds_pdb)\n        bond_matrix_gaff = make_bonds_comparable(bonds_gaff)\n        bond_matrix_sybyl = make_bonds_comparable(bonds_sybyl)\n\n        eq(bond_matrix_pdb, bond_matrix_gaff)\n        eq(bond_matrix_pdb, bond_matrix_sybyl)", "output": "def test_load_freesolv_gaffmol2_vs_sybylmol2_vs_obabelpdb(get_fn, tmpdir):\n    tar_filename = \"freesolve_v0.3.tar.bz2\"\n    tar = tarfile.open(get_fn(tar_filename), mode=\"r:bz2\")\n    os.chdir(str(tmpdir))\n    tar.extractall()\n    tar.close()\n\n    with open(\"./v0.3/database.pickle\", 'rb') as f:\n        database = pickle.load(f, encoding='latin-1')\n\n    for key in database:\n        gaff_filename = \"./v0.3/mol2files_gaff/%s.mol2\" % key\n        pdb_filename = \"./v0.3/mol2files_gaff/%s.pdb\" % key\n        sybyl_filename = \"./v0.3/mol2files_sybyl/%s.mol2\" % key\n\n        cmd = \"obabel -imol2 %s -opdb > %s 2>/dev/null\" % (sybyl_filename, pdb_filename)\n        assert os.system(cmd) == 0\n\n        t_pdb = md.load(pdb_filename)\n        t_gaff = md.load(gaff_filename)\n        t_sybyl = md.load(sybyl_filename)\n\n        eq(t_pdb.n_atoms, t_gaff.n_atoms)\n        eq(t_pdb.n_atoms, t_sybyl.n_atoms)\n\n        eq(t_pdb.n_frames, t_gaff.n_frames)\n        eq(t_pdb.n_frames, t_gaff.n_frames)\n\n        eq(t_pdb.xyz, t_gaff.xyz, decimal=4)\n        eq(t_pdb.xyz, t_sybyl.xyz, decimal=4)\n\n        top_pdb, bonds_pdb = t_pdb.top.to_dataframe()\n        top_gaff, bonds_gaff = t_gaff.top.to_dataframe()\n        top_sybyl, bonds_sybyl = t_sybyl.top.to_dataframe()\n\n        eq(top_sybyl.name.values, top_pdb.name.values)\n\n        # eq(top_gaff.name.values, top_sybyl.name.values)  # THEY CAN HAVE DIFFERENT NAMES, so this isn't TRUE!\n\n        def make_bonds_comparable(bond_array):\n            \"\"\"Create a bond connectivity matrix from a numpy array of atom pairs.  Avoids having to compare the order in which bonds are listed.\"\"\"\n            n_bonds = len(bond_array)\n            data = np.ones(n_bonds)\n            i = bond_array[:, 0]\n            j = bond_array[:, 1]\n            matrix = scipy.sparse.coo_matrix((data, (i, j)), shape=(t_pdb.n_atoms, t_pdb.n_atoms)).toarray()\n            return matrix + matrix.T  # Symmetrize to account for (a ~ b) versus (b ~ a)\n\n        bond_matrix_pdb = make_bonds_comparable(bonds_pdb)\n        bond_matrix_gaff = make_bonds_comparable(bonds_gaff)\n        bond_matrix_sybyl = make_bonds_comparable(bonds_sybyl)\n\n        eq(bond_matrix_pdb, bond_matrix_gaff)\n        eq(bond_matrix_pdb, bond_matrix_sybyl)"}
{"input": "def __init__(self, info):\n        self.info = info.ElementInfo\n        idx = len(self.info.Subname)\n        if idx:\n            sub = info.SelSubname[:-idx]\n        else:\n            sub = info.SelSubname\n        _,mat = info.SelObj.getSubObject(sub,1,FreeCAD.Matrix())\n        pos = utils.getElementPos(self.info.Shape)\n        if not pos:\n            pos = self.info.Shape.BoundBox.Center\n        pos = mat.multiply(pos)\n        self.matrix = mat*self.info.Placement.inverse().toMatrix()\n        base = self.matrix.multiply(self.info.Placement.Base)\n        self.offset = pos - base\n\n        self.matrix.invert()\n        self.view = info.SelObj.ViewObject.Document.ActiveView\n        self.callbackMove = self.view.addEventCallback(\n                \"SoLocation2Event\",self.moveMouse)\n        self.callbackClick = self.view.addEventCallback(\n                \"SoMouseButtonEvent\",self.clickMouse)\n        self.callbackKey = self.view.addEventCallback(\n                \"SoKeyboardEvent\",self.keyboardEvent)\n        FreeCAD.setActiveTransaction('Assembly quick move')\n        self.active = True", "output": "def __init__(self, info):\n        self.info = info.ElementInfo\n        idx = len(self.info.Subname)\n        if idx:\n            sub = info.SelSubname[:-idx]\n        else:\n            sub = info.SelSubname\n        _,mat = info.SelObj.getSubObject(sub,1,FreeCAD.Matrix())\n        pos = utils.getElementPos(self.info.Shape)\n        if not pos:\n            pos = self.info.Shape.BoundBox.Center\n        pos = mat.multiply(pos)\n        self.matrix = mat*self.info.Placement.inverse().toMatrix()\n        base = self.matrix.multiply(self.info.Placement.Base)\n        self.offset = pos - base\n\n        self.matrix.invert()\n        self.view = info.SelObj.ViewObject.Document.ActiveView\n        self.callbackMove = self.view.addEventCallback(\n                \"SoLocation2Event\",self.moveMouse)\n        self.callbackClick = self.view.addEventCallback(\n                \"SoMouseButtonEvent\",self.clickMouse)\n        self.callbackKey = self.view.addEventCallback(\n                \"SoKeyboardEvent\",self.keyboardEvent)\n        FreeCAD.setActiveTransaction('Assembly quick move',True)\n        self.active = True"}
{"input": "def _plot_response_curve(y, t, title, continuous=True):\n    _plt.title(title)\n    _plt.xlabel('t/s')\n    _plt.ylabel('Amplitude')\n    _plt.axvline(x=0, color='black')\n    _plt.axhline(y=0, color='black')\n    if continuous:\n        _plt.plot(t, y)\n    else:\n        _plt.step(t, y)\n    _plt.grid()\n    _plt.show()", "output": "def _plot_response_curve(y, t, title, continuous=True):\n    _plt.title(title)\n    _plt.xlabel('t/s')\n    _plt.ylabel('Amplitude')\n    _plt.axvline(x=0, color='black')\n    _plt.axhline(y=0, color='black')\n    if continuous:\n        _plt.plot(t, y)\n    else:\n        _plt.step(t, y, where='post')\n    _plt.grid()\n    _plt.show()"}
{"input": "def write_csv(results, filename):\n    with open(filename, \"w\") as f:\n        writer = csv.writer(f)\n        writer.writerow(['Benchmark', 'Base', 'Changed'])\n        for result in results:\n            name = result.base.get_name()\n            base = average(result.base)\n            changed = average(result.changed)\n            row = [name, format_csv(base), format_csv(changed)]\n            writer.writerow(row)", "output": "def write_csv(results, filename):\n    with open(filename, \"w\", newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Benchmark', 'Base', 'Changed'])\n        for result in results:\n            name = result.base.get_name()\n            base = average(result.base)\n            changed = average(result.changed)\n            row = [name, format_csv(base), format_csv(changed)]\n            writer.writerow(row)"}
{"input": "def _create_ticket_with_attachment(self, url, data):\n        attachments = data['attachments']\n        del data['attachments']\n        multipart_data = []\n\n        for attachment in attachments:\n            file_name = attachment.split(\"/\")[-1:][0]\n            multipart_data.append(('attachments[]', (file_name, open(attachment), None)))\n\n        ticket = self._api._post(url, data=data, files=multipart_data)\n        return ticket", "output": "def _create_ticket_with_attachment(self, url, data):\n        attachments = data['attachments']\n        del data['attachments']\n        multipart_data = []\n\n        for attachment in attachments:\n            file_name = attachment.split(\"/\")[-1:][0]\n            multipart_data.append(('attachments[]', (file_name, open(attachment, 'rb'), None)))\n\n        ticket = self._api._post(url, data=data, files=multipart_data)\n        return ticket"}
{"input": "def scrape_actions(self, bill, url):\n        with self.urlopen(url) as page:\n            page = lxml.html.fromstring(page)\n\n            bill.add_source(url)\n\n            slist = page.xpath(\"//strong[contains(., 'Authors:')]\")[0]\n            slist = slist.tail.split(',')\n            for sponsor in slist:\n                name = sponsor.strip()\n                if name:\n                    bill.add_sponsor(name, 'author')\n\n            act_table = page.xpath(\"//table\")[1]\n            read_yet = False\n\n            for row in act_table.xpath(\"tr\")[1:]:\n                date = row.xpath(\"string(td[1])\").strip()\n                date = datetime.datetime.strptime(date, \"%m/%d/%Y\").date()\n\n                chamber = row.xpath(\"string(td[2])\").strip()\n                if chamber == 'S':\n                    chamber = 'upper'\n                elif chamber == 'H':\n                    chamber = 'lower'\n\n                action = row.xpath(\"string(td[4])\").strip()\n\n                atype = []\n\n                if action.startswith('First reading:'):\n                    if not read_yet:\n                        atype.append('bill:introduced')\n                        read_yet = True\n                    atype.append('bill:reading:1')\n                if 'referred to' in action:\n                    atype.append('committee:referred')\n\n                bill.add_action(chamber, action, date, type=atype)", "output": "def scrape_actions(self, bill, url):\n        with self.urlopen(url) as page:\n            page = lxml.html.fromstring(page)\n\n            bill.add_source(url)\n\n            slist = page.xpath(\"//strong[contains(., 'Authors:')]\")[0]\n            slist = slist.tail.split(',')\n            for sponsor in slist:\n                name = sponsor.strip()\n                if name:\n                    bill.add_sponsor('author', name)\n\n            act_table = page.xpath(\"//table\")[1]\n            read_yet = False\n\n            for row in act_table.xpath(\"tr\")[1:]:\n                date = row.xpath(\"string(td[1])\").strip()\n                date = datetime.datetime.strptime(date, \"%m/%d/%Y\").date()\n\n                chamber = row.xpath(\"string(td[2])\").strip()\n                if chamber == 'S':\n                    chamber = 'upper'\n                elif chamber == 'H':\n                    chamber = 'lower'\n\n                action = row.xpath(\"string(td[4])\").strip()\n\n                atype = []\n\n                if action.startswith('First reading:'):\n                    if not read_yet:\n                        atype.append('bill:introduced')\n                        read_yet = True\n                    atype.append('bill:reading:1')\n                if 'referred to' in action:\n                    atype.append('committee:referred')\n\n                bill.add_action(chamber, action, date, type=atype)"}
{"input": "def release_tag_model(graph: ProvDocument, packages: ReleaseTagPackage):\n    for package in packages:\n        if package.release_package is not None:\n            r_user, release, release_event, release_evidence, assets = package.release_package\n            graph.agent(*r_user)\n            graph.entity(*release)\n            graph.activity(*release_event)\n            graph.entity(*release_evidence)\n            for asset in assets:\n                graph.entity(*asset)\n                graph.hadMember(asset.id, release.id)\n\n            graph.hadMember(release_evidence.id, release.id)\n            graph.wasGeneratedBy(release.id, release_event.id)\n            graph.wasAttributedTo(release.id, r_user.id)\n            graph.wasAssociatedWith(release_event.id, r_user.id)\n\n        if package.tag_package is not None:\n            t_user, tag, tag_event = package.tag_package\n            graph.agent(*t_user)\n            graph.entity(*tag)\n            graph.activity(*tag_event)\n\n            if package.release_package is not None:\n                graph.hadMember(tag.id, release.id)\n            graph.wasGeneratedBy(tag_event.id, tag.id)\n            graph.wasAttributedTo(tag.id, t_user.id)\n            graph.wasAssociatedWith(tag_event.id, t_user.id)\n\n        if package.commit_package is not None:\n            author, commit_event, _, commit, _ = package.commit_package\n            graph.agent(*author)\n            graph.activity(*commit_event)\n            graph.entity(*commit)\n\n            if package.tag_package is not None:\n                graph.hadMember(commit.id, tag.id)\n            graph.wasGeneratedBy(commit.id, commit_event.id)\n            graph.wasAttributedTo(commit.id, author.id)\n            graph.wasAssociatedWith(commit_event.id, author.id)\n    return graph", "output": "def release_tag_model(graph: ProvDocument, packages: ReleaseTagPackage):\n    for package in packages:\n        if package.release_package is not None:\n            r_user, release, release_event, release_evidence, assets = package.release_package\n            graph.agent(*r_user)\n            graph.entity(*release)\n            graph.activity(*release_event)\n            graph.entity(*release_evidence)\n            for asset in assets:\n                graph.entity(*asset)\n                graph.hadMember(asset.id, release.id)\n\n            graph.hadMember(release_evidence.id, release.id)\n            graph.wasGeneratedBy(release.id, release_event.id)\n            graph.wasAttributedTo(release.id, r_user.id)\n            graph.wasAssociatedWith(release_event.id, r_user.id)\n\n        if package.tag_package is not None:\n            t_user, tag, tag_event = package.tag_package\n            graph.agent(*t_user)\n            graph.entity(*tag)\n            graph.activity(*tag_event)\n\n            if package.release_package is not None:\n                graph.hadMember(tag.id, release.id)\n            graph.wasGeneratedBy(tag.id, tag_event.id)\n            graph.wasAttributedTo(tag.id, t_user.id)\n            graph.wasAssociatedWith(tag_event.id, t_user.id)\n\n        if package.commit_package is not None:\n            author, commit_event, _, commit, _ = package.commit_package\n            graph.agent(*author)\n            graph.activity(*commit_event)\n            graph.entity(*commit)\n\n            if package.tag_package is not None:\n                graph.hadMember(commit.id, tag.id)\n            graph.wasGeneratedBy(commit.id, commit_event.id)\n            graph.wasAttributedTo(commit.id, author.id)\n            graph.wasAssociatedWith(commit_event.id, author.id)\n    return graph"}
{"input": "def test_approp_valid(self):\n        \"\"\"Test valid job.\"\"\"\n        jobId = self.jobIdDict[\"valid\"]\n        self.passed = self.run_test(\n            jobId, 200, \"finished\", 63, 10, \"complete\", 0, False)", "output": "def test_approp_valid(self):\n        \"\"\"Test valid job.\"\"\"\n        jobId = self.jobIdDict[\"valid\"]\n        self.passed = self.run_test(\n            jobId, 200, \"finished\", 63, 0, \"complete\", 10, False)"}
{"input": "def test_getListOfGames(self):\n        print(\"testing gettingListOfPlayedGames\")\n        gameStates = [GameState.COMPLETED, GameState.RUNNING]\n        self.instance.getListOfGames(divisionId_Swissdraw,8,gameStates)\n        print(\"#####################################################################\")", "output": "def test_getListOfGames(self):\n        print(\"testing gettingListOfPlayedGames\")\n        gameStates = [GameState.COMPLETED, GameState.RUNNING]\n        self.instance.getListOfGames(divisionId_Swissdraw, gameStates, 8)\n        print(\"#####################################################################\")"}
{"input": "def get_history(self, obj):\n        diff_list = []\n        current = None\n\n        for version in reversed(list(reversion.get_for_object(obj))):\n            if current:\n                issues_diff = self.get_issues_diff(version, current)\n                diff_list.append(issues_diff)\n\n            current = version\n\n        return diff_list", "output": "def get_history(self, obj):\n        diff_list = []\n        current = None\n\n        for version in reversed(list(reversion.get_for_object(obj))):\n            if current:\n                issues_diff = self.get_issues_diff(current, version)\n                diff_list.append(issues_diff)\n\n            current = version\n\n        return diff_list"}
{"input": "def __init__(self, definition_file, cfg_set, repo_path, repo_branch, repo_host='github.com'):\n        self.definition_file = definition_file\n        self.repo_path = repo_path\n        self.repo_branch = repo_branch\n        self.repo_host = repo_host\n        self.cfg_set = cfg_set\n        import model\n        self.job_mapping = model.JobMapping(\n            name='dummy',\n            raw_dict={'concourse_target_team': 'dummy'},\n        )", "output": "def __init__(self, definition_file, cfg_set, repo_path, repo_branch, repo_host='github.com'):\n        self.definition_file = definition_file\n        self.repo_path = repo_path\n        self.repo_branch = repo_branch\n        self.repo_host = repo_host\n        self.cfg_set = cfg_set\n        import model\n        self.job_mapping = model.concourse.JobMapping(\n            name='dummy',\n            raw_dict={'concourse_target_team': 'dummy'},\n        )"}
{"input": "def info_printer(solver,header):\n            net = solver.problem.network\n            if header:\n                print('{0:^5}'.format('vmax'), end=' ')\n                print('{0:^5}'.format('vmin'), end=' ')\n                print('{0:^6}'.format('bvvio'), end=' ')\n                print('{0:^6}'.format('gQvio'), end=' ')\n                print('{0:^6}'.format('gPvio'))\n            else:\n                print('{0:^5.2f}'.format(np.average(net.bus_v_max)), end=' ')\n                print('{0:^5.2f}'.format(np.average(net.bus_v_min)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.bus_v_vio)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.gen_Q_vio)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.gen_P_vio)))", "output": "def info_printer(solver,header):\n            net = solver.problem.wrapped_problem.network\n            if header:\n                print('{0:^5}'.format('vmax'), end=' ')\n                print('{0:^5}'.format('vmin'), end=' ')\n                print('{0:^6}'.format('bvvio'), end=' ')\n                print('{0:^6}'.format('gQvio'), end=' ')\n                print('{0:^6}'.format('gPvio'))\n            else:\n                print('{0:^5.2f}'.format(np.average(net.bus_v_max)), end=' ')\n                print('{0:^5.2f}'.format(np.average(net.bus_v_min)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.bus_v_vio)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.gen_Q_vio)), end=' ')\n                print('{0:^6.0e}'.format(np.average(net.gen_P_vio)))"}
{"input": "def __init__(self, band):\n        self.band = band\n        self.iter = 0\n        self.set_output_filenames()\n\n        self.moon_stripe_mask = HealpixMap(\"/home/users/mberkeley/wisemapper/data/masks/stripe_mask_G.fits\")\n        self.moon_stripe_mask.read_data()\n        self.moon_stripe_inds = np.arange(len(self.moon_stripe_mask.mapdata))[self.moon_stripe_mask.mapdata.astype(bool)]\n\n        self.galaxy_mask = self.mask_galaxy()\n        self.galaxy_mask_inds = np.arange(len(self.galaxy_mask))[self.galaxy_mask]\n\n        self.south_pole_mask = HealpixMap(\"/home/users/mberkeley/wisemapper/data/masks/south_pole_mask.fits\").read_data()\n        self.south_pole_mask_inds = np.arange(len(self.south_pole_mask))[self.south_pole_mask.mapdata.astype(bool)]\n\n        self.numerator = np.zeros_like(self.fsm.mapdata)\n        self.denominator = np.zeros_like(self.fsm.mapdata)\n\n        self.gains = []\n        self.offsets = []\n        self.orbit_sizes = []\n\n        self.all_gains = []\n        self.all_offsets = []", "output": "def __init__(self, band):\n        self.band = band\n        self.iter = 0\n        self.set_output_filenames()\n\n        self.moon_stripe_mask = HealpixMap(\"/home/users/mberkeley/wisemapper/data/masks/stripe_mask_G.fits\")\n        self.moon_stripe_mask.read_data()\n        self.moon_stripe_inds = np.arange(len(self.moon_stripe_mask.mapdata))[self.moon_stripe_mask.mapdata.astype(bool)]\n\n        self.galaxy_mask = self.mask_galaxy()\n        self.galaxy_mask_inds = np.arange(len(self.galaxy_mask))[self.galaxy_mask]\n\n        self.south_pole_mask = HealpixMap(\"/home/users/mberkeley/wisemapper/data/masks/south_pole_mask.fits\").read_data()\n        self.south_pole_mask_inds = np.arange(len(self.south_pole_mask.mapdata))[self.south_pole_mask.mapdata.astype(bool)]\n\n        self.numerator = np.zeros_like(self.fsm.mapdata)\n        self.denominator = np.zeros_like(self.fsm.mapdata)\n\n        self.gains = []\n        self.offsets = []\n        self.orbit_sizes = []\n\n        self.all_gains = []\n        self.all_offsets = []"}
{"input": "def get_stddev(self):\n        \"\"\"\n        Returns stddev with 'price' for existing algorithms.\n\n        Could possibly use existing algos.\n        \"\"\"\n        # Sample standard deviation is undefined for a single event or\n        # no events.\n        if len(self) <= 1:\n            return None\n\n        else:\n            average = self.sum['price'] / len(self.ticks)\n            s_squared = (self.sum_sqr['price'] - self.sum['price'] * average) \\\n                / (len(self.ticks) - 1)\n            stddev = sqrt(s_squared)\n        return stddev", "output": "def get_stddev(self):\n        \"\"\"\n        Returns stddev with 'price' for existing algorithms.\n\n        Could possibly use existing algos.\n        \"\"\"\n        # Sample standard deviation is undefined for a single event or\n        # no events.\n        if len(self.ticks) <= 1:\n            return None\n\n        else:\n            average = self.sum['price'] / len(self.ticks)\n            s_squared = (self.sum_sqr['price'] - self.sum['price'] * average) \\\n                / (len(self.ticks) - 1)\n            stddev = sqrt(s_squared)\n        return stddev"}
{"input": "def __call__(self, xi, order=0):\n        \"\"\"Evaluate the path at given position.\"\"\"\n        ret = [poly.derivative(order)(xi) for poly in self._polys]\n        return np.array(ret)", "output": "def __call__(self, xi, order=0):\n        \"\"\"Evaluate the path at given position.\"\"\"\n        ret = [poly.derivative(order)(xi) for poly in self._polys]\n        return np.array(ret).T"}
{"input": "def _form_extended_payload(cls, msg: int, payload: Union[Iterable, bytes, bytearray]) -> bytearray:\n        if payload is None:\n            payload = bytearray()\n        payload_length = len(payload)\n        assert payload_length <= 0x7f_ff\n        if payload_length > 0x80:\n            payload_length = [(payload_length >> 8) | 0x80, payload_length & 0xff]\n        else:\n            payload_length = [payload_length]\n        packet = bytearray([msg, *payload_length, *payload])\n        crc = cls.VEX_CRC16.compute(bytes([*cls._form_simple_packet(0x56), *packet]))\n        packet = bytearray([*packet, crc >> 8, crc & 0xff])\n        assert (cls.VEX_CRC16.compute(bytes([*cls._form_simple_packet(0x56), *packet])) == 0)\n        return packet", "output": "def _form_extended_payload(cls, msg: int, payload: Union[Iterable, bytes, bytearray]) -> bytearray:\n        if payload is None:\n            payload = bytearray()\n        payload_length = len(payload)\n        assert payload_length <= 0x7f_ff\n        if payload_length >= 0x80:\n            payload_length = [(payload_length >> 8) | 0x80, payload_length & 0xff]\n        else:\n            payload_length = [payload_length]\n        packet = bytearray([msg, *payload_length, *payload])\n        crc = cls.VEX_CRC16.compute(bytes([*cls._form_simple_packet(0x56), *packet]))\n        packet = bytearray([*packet, crc >> 8, crc & 0xff])\n        assert (cls.VEX_CRC16.compute(bytes([*cls._form_simple_packet(0x56), *packet])) == 0)\n        return packet"}
{"input": "def create_security_group(self, context, security_group, default_sg=False):\n        \"\"\"Create security group.\n\n        If default_sg is true that means a we are creating a default security\n        group and we don't need to check if one exists.\n        \"\"\"\n        s = security_group.get('security_group')\n\n        tenant_id = self._get_tenant_id_for_create(context, s)\n        if not default_sg:\n            self._ensure_default_security_group(context, tenant_id)\n        # NOTE(salv-orlando): Pre-generating Neutron ID for security group.\n        neutron_id = str(uuid.uuid4())\n        nsx_secgroup = secgrouplib.create_security_profile(\n            self.cluster, neutron_id, tenant_id, s)\n        with context.session.begin(subtransactions=True):\n            s['id'] = neutron_id\n            sec_group = super(NsxPluginV2, self).create_security_group(\n                context, security_group, default_sg)\n            context.session.flush()\n            # Add mapping between neutron and nsx identifiers\n            nsx_db.add_neutron_nsx_security_group_mapping(\n                context.session, neutron_id, nsx_secgroup['uuid'])\n        return sec_group", "output": "def create_security_group(self, context, security_group, default_sg=False):\n        \"\"\"Create security group.\n\n        If default_sg is true that means a we are creating a default security\n        group and we don't need to check if one exists.\n        \"\"\"\n        s = security_group.get('security_group')\n\n        tenant_id = self._get_tenant_id_for_create(context, s)\n        if not default_sg:\n            self._ensure_default_security_group(context, tenant_id)\n        # NOTE(salv-orlando): Pre-generating Neutron ID for security group.\n        neutron_id = str(uuid.uuid4())\n        nsx_secgroup = secgrouplib.create_security_profile(\n            self.cluster, tenant_id, neutron_id, s)\n        with context.session.begin(subtransactions=True):\n            s['id'] = neutron_id\n            sec_group = super(NsxPluginV2, self).create_security_group(\n                context, security_group, default_sg)\n            context.session.flush()\n            # Add mapping between neutron and nsx identifiers\n            nsx_db.add_neutron_nsx_security_group_mapping(\n                context.session, neutron_id, nsx_secgroup['uuid'])\n        return sec_group"}
{"input": "def _sort_map_entries(category_map, sort_alpha):\n    things = []\n    for title, entry in category_map[\"entries\"].items():\n        if entry[\"sort_key\"] == None and sort_alpha:\n            entry[\"sort_key\"] = title\n        things.append((title, entry))\n    for title, category in category_map[\"subcategories\"].items():\n        things.append((title, category))\n        _sort_map_entries(category_map[\"subcategories\"][title], sort_alpha)\n    category_map[\"children\"] = [x[0] for x in sorted(things, key=lambda x: x[1][\"sort_key\"])]", "output": "def _sort_map_entries(category_map, sort_alpha):\n    things = []\n    for title, entry in category_map[\"entries\"].items():\n        if entry[\"sort_key\"] is None and sort_alpha:\n            entry[\"sort_key\"] = title\n        things.append((title, entry))\n    for title, category in category_map[\"subcategories\"].items():\n        things.append((title, category))\n        _sort_map_entries(category_map[\"subcategories\"][title], sort_alpha)\n    category_map[\"children\"] = [x[0] for x in sorted(things, key=lambda x: x[1][\"sort_key\"])]"}
{"input": "def rem_call(self, instance, callback):\n\t\t\"\"\"Removes all callbacks of 'instance' that are 'callback'\n\t\t@param instance: the instance that would execute the call\n\t\t@param callback: the function to remove\n\t\t@return: int, number of removed calls\n\t\t\"\"\"\n\t\tassert callable(callback)\n\t\tremoved_calls = 0\n\t\tfor key in self.schedule:\n\t\t\tcallback_objects = self.schedule[key]\n\t\t\tfor i in xrange(len(callback_objects) - 1, -1, -1):\n\t\t\t\tif callback_objects[i].class_instance is instance and callback_objects[i].callback == callback and \\\n\t\t\t\t   not hasattr(callback, \"invalid\"):\n\t\t\t\t\tdel callback_objects[i]\n\t\t\t\t\tremoved_calls += 1\n\n\t\ttest = 0\n\t\tif removed_calls >= 0: # there also must be calls in the calls_by_instance dict\n\t\t\tfor i in xrange(len(self.calls_by_instance[instance]) - 1, -1, -1):\n\t\t\t\tobj = self.calls_by_instance[instance][i]\n\t\t\t\tif obj.callback == callback:\n\t\t\t\t\tdel self.calls_by_instance[instance][i]\n\t\t\t\t\ttest += 1\n\t\t\tassert test == removed_calls\n\t\t\tif not self.calls_by_instance[instance]:\n\t\t\t\tdel self.calls_by_instance[instance]\n\n\t\tfor i in xrange(len(self.additional_cur_tick_schedule) - 1, -1, -1):\n\t\t\tif self.additional_cur_tick_schedule[i].class_instance is instance and \\\n\t\t\t\tself.additional_cur_tick_schedule[i].callback == callback:\n\t\t\t\t\tdel callback_objects[i]\n\t\t\t\t\tremoved_calls += 1\n\n\t\treturn removed_calls", "output": "def rem_call(self, instance, callback):\n\t\t\"\"\"Removes all callbacks of 'instance' that are 'callback'\n\t\t@param instance: the instance that would execute the call\n\t\t@param callback: the function to remove\n\t\t@return: int, number of removed calls\n\t\t\"\"\"\n\t\tassert callable(callback)\n\t\tremoved_calls = 0\n\t\tfor key in self.schedule:\n\t\t\tcallback_objects = self.schedule[key]\n\t\t\tfor i in xrange(len(callback_objects) - 1, -1, -1):\n\t\t\t\tif callback_objects[i].class_instance is instance and callback_objects[i].callback == callback and \\\n\t\t\t\t   not hasattr(callback, \"invalid\"):\n\t\t\t\t\tdel callback_objects[i]\n\t\t\t\t\tremoved_calls += 1\n\n\t\ttest = 0\n\t\tif removed_calls > 0: # there also must be calls in the calls_by_instance dict\n\t\t\tfor i in xrange(len(self.calls_by_instance[instance]) - 1, -1, -1):\n\t\t\t\tobj = self.calls_by_instance[instance][i]\n\t\t\t\tif obj.callback == callback:\n\t\t\t\t\tdel self.calls_by_instance[instance][i]\n\t\t\t\t\ttest += 1\n\t\t\tassert test == removed_calls\n\t\t\tif not self.calls_by_instance[instance]:\n\t\t\t\tdel self.calls_by_instance[instance]\n\n\t\tfor i in xrange(len(self.additional_cur_tick_schedule) - 1, -1, -1):\n\t\t\tif self.additional_cur_tick_schedule[i].class_instance is instance and \\\n\t\t\t\tself.additional_cur_tick_schedule[i].callback == callback:\n\t\t\t\t\tdel callback_objects[i]\n\t\t\t\t\tremoved_calls += 1\n\n\t\treturn removed_calls"}
{"input": "def view_profile(request, username=None):\n    user = request.user if username != request.user.username else User.objects.get(username=username)\n    return render(request, 'profile.html', { 'profile':user })", "output": "def view_profile(request, username=None):\n    user = request.user if username == request.user.username else User.objects.get(username=username)\n    return render(request, 'profile.html', { 'profile':user })"}
{"input": "def wait4all(self):\n        \"\"\"Wait for finish of all submitted jobs.\n\n        This method waits until all jobs submitted to service finish its execution (successfully or not).\n        \"\"\"\n        not_finished = True\n        while not_finished:\n            status = self._send_and_validate_result({\n                \"request\": \"status\",\n                \"options\": { \"allJobsFinished\": True }\n            })\n            not_finished = status.get(\"AllJobsFinished\", False) is False\n            if not_finished:\n                time.sleep(self._poll_delay)\n\n        logging.info(\"all jobs finished in manager\")", "output": "def wait4all(self):\n        \"\"\"Wait for finish of all submitted jobs.\n\n        This method waits until all jobs submitted to service finish its execution (successfully or not).\n        \"\"\"\n        not_finished = True\n        while not_finished:\n            status = self._send_and_validate_result({\n                \"request\": \"status\",\n                \"options\": { \"allJobsFinished\": True }\n            })\n            not_finished = status.get(\"AllJobsFinished\", False) is False\n            if not_finished:\n                time.sleep(self._poll_delay)\n\n        _logger.info(\"all jobs finished in manager\")"}
{"input": "def host_get(host_id=None):\n    if host_id:\n        return tils.jsonify(Host.get_host(id=host_id).dict())\n\n    hosts = []\n\n    for host in Host.iter_hosts():\n        hosts.append(host.dict())\n\n    return utils.jsonify(hosts)", "output": "def host_get(host_id=None):\n    if host_id:\n        return utils.jsonify(Host.get_host(id=host_id).dict())\n\n    hosts = []\n\n    for host in Host.iter_hosts():\n        hosts.append(host.dict())\n\n    return utils.jsonify(hosts)"}
{"input": "def convert_record(record, max_depth: int = 7):\n    tmp = \"/\".join(s.split(\"/\")[:max_depth])\n    for elm in string.punctuation:\n        tmp = tmp.replace(elm, \" \")\n    return tmp + \"\\n\"", "output": "def convert_record(record, max_depth: int = 7):\n    tmp = \"/\".join(record.split(\"/\")[:max_depth])\n    for elm in string.punctuation:\n        tmp = tmp.replace(elm, \" \")\n    return tmp + \"\\n\""}
{"input": "def read_stream_data(self, datafile, channel=None, n_samp=None):\n        \"\"\"\n        Loads data taken with the fucntion stream_data_on.\n\n        To do : return header rather than just timestamp2\n\n        Args:\n        -----\n        datafile (str): The full path to the data to read\n\n        Opt Args:\n        ---------\n        channel (int or int array): Channels to load.\n        n_samp (int) : The number of samples to read.\n\n        Ret:\n        ----\n        t (float array): The timestamp data\n        d (float array): The resonator data in units of phi0\n        m (int array): The maskfile that maps smurf num to gcp num\n        \"\"\"\n        try:\n            datafile = glob.glob(datafile+'*')[-1]\n        except:\n            print(f'datafile={datafile}')\n\n        self.log(f'Reading {datafile}')\n\n        if channel is not None:\n            self.log('Only reading channel {}'.format(channel))\n\n\n        # Smurf header structure\n        keys = [\n            'protocol_version',\n            'crate_id',\n            'slot_number',\n            'timing_cond',\n            'number_of_channels',\n            #'tes_bias', < TO DO, include the TES bias values\n            'timestamp',\n            'flux_ramp_increment',\n            'flux_ramp_offset',\n            'counter_0',\n            'counter_1',\n            'counter_2',\n            'reset_bits',\n            'frame_counter',\n            'tes_relays_config',\n            'external_time',\n            'control_field',\n            'test_params',\n            'num_rows',\n            'num_rows_reported',\n            'row_length',\n            'data_rate',\n        ]\n        data_keys = [f'data{i}' for i in range(528)]\n\n        keys.extend(data_keys)\n        keys_dict = dict(zip(keys, range(len(keys))))\n\n        # Read in all channels by default\n        if channel is None:\n            channel = np.arange(512)\n\n        channel = np.ravel(np.asarray(channel))\n        n_chan = len(channel)\n\n        # Indices for input channels\n        channel_mask = np.zeros(n_chan, dtype=int)\n        for i, c in enumerate(channel):\n            channel_mask[i] = keys_dict['data{}'.format(c)]\n\n        eval_n_samp = False\n        if n_samp is not None:\n            eval_n_samp = True\n\n        # Make holder arrays for phase and timestamp\n        phase = np.zeros((n_chan,0))\n        timestamp2 = np.array([])\n        counter = 0\n        n = 20000  # Number of elements to load at a time\n        tmp_phase = np.zeros((n_chan, n))\n        tmp_timestamp2 = np.zeros(n)\n        with open(datafile, mode='rb') as file:\n            while True:\n\n                # Read the Rogue header which is 8-byte long:\n                # - 4 bytes : Length of the following data block in bytes,\n                #             It includes the next 4 bytes in the header.\n                # - 1 byte  : Channel ID.\n                # - 1 byte  : Frame error.\n                # - 2 bytes : Frame flags.\n                rogue_header = dict()\n\n                # Read the first 4-byte word, which is the length\n                chunk = file.read(4)\n\n                # Check if we reach the end of the file\n                if not chunk:\n                    # If frame is incomplete - meaning end of file\n                    phase = np.hstack((phase, tmp_phase[:,:counter%n]))\n                    timestamp2 = np.append(timestamp2, tmp_timestamp2[:counter%n])\n                    break\n\n                # Convert the 4-byte word to length\n                rogue_header['length'] = struct.Struct('I').unpack(chunk)[0]\n\n                # Read the sencond 4-byte word and extract the channel, error, and flags\n                chunk = file.read(4)\n                word = struct.Struct('I').unpack(chunk)[0]\n                rogue_header['channel'] = (word >> 24) & 0xff\n                rogue_header['error'] = (word >> 16) & 0xff\n                rogue_header['flags'] = (word ) & 0xffff\n\n\n                # Check if this is a block of data or metadata\n                # Data comes on channel 0, and metadata on channel 1\n\n                if rogue_header['channel'] == 1:\n\n                    # This is our meta data.\n                    # We need to process it here.\n\n                    # Skip for now\n                    chunk = file.read(rogue_header['length']-4)\n\n                elif rogue_header['channel'] == 0:\n                    # Skip data on unknown channels, but print\n                    # a warning message\n                    self.log(f\"WARNING. Data present on an unknown channel: {rogue_header['channel']}\")\n                    chunk = file.read(rogue_header['length']-4)\n                else:\n                    # This is a data block. Processes it\n\n                    if eval_n_samp:\n                        if counter >= n_samp:\n                            phase = np.hstack((phase, tmp_phase[:,:counter%n]))\n                            timestamp2 = np.append(timestamp2,\n                                                   tmp_timestamp2[:counter%n])\n                            break\n\n                    chunk = file.read(2240)  # Frame size is 2240\n\n                    # This is the structure of the header (see README.SmurfPacket.md for a details)\n                    # Note: This assumes that the header version is 1 (currently the only version available),\n                    # which has a length of 128 bytes. In the future, we should check first the version,\n                    # and then unpack the data base on the version number.\n                    # TO DO: Extract the TES BIAS values\n                    #                         ->| |<-\n                    frame = struct.Struct('BBBBI40xQIIIIQIII4x5B3xBB6xHH4xHH4x528i').unpack(chunk)\n\n                    # Extract detector data\n                    for i, c in enumerate(channel_mask):\n                        tmp_phase[i,counter%n] = frame[c]\n\n                    # Timestamp data\n                    tmp_timestamp2[counter%n] = frame[keys_dict['timestamp']]\n\n                    # Store the data in a useful array and reset tmp arrays\n                    if counter % n == n - 1 :\n                        self.log('{} elements loaded'.format(counter+1))\n                        phase = np.hstack((phase, tmp_phase))\n                        timestamp2 = np.append(timestamp2, tmp_timestamp2)\n                        tmp_phase = np.zeros((n_chan, n))\n                        tmp_timestamp2 = np.zeros(n)\n                    counter = counter + 1\n\n        phase = np.squeeze(phase)\n        phase = phase.astype(float) / 2**15 * np.pi # where is decimal?  Is it in rad?\n\n        rootpath = os.path.dirname(datafile)\n        filename = os.path.basename(datafile)\n        timestamp = filename.split('.')[0]\n\n        # make a mask from mask file\n        mask = self.make_mask_lookup(datafile.replace('.dat', '_mask.txt'))\n\n        return timestamp2, phase, mask", "output": "def read_stream_data(self, datafile, channel=None, n_samp=None):\n        \"\"\"\n        Loads data taken with the fucntion stream_data_on.\n\n        To do : return header rather than just timestamp2\n\n        Args:\n        -----\n        datafile (str): The full path to the data to read\n\n        Opt Args:\n        ---------\n        channel (int or int array): Channels to load.\n        n_samp (int) : The number of samples to read.\n\n        Ret:\n        ----\n        t (float array): The timestamp data\n        d (float array): The resonator data in units of phi0\n        m (int array): The maskfile that maps smurf num to gcp num\n        \"\"\"\n        try:\n            datafile = glob.glob(datafile+'*')[-1]\n        except:\n            print(f'datafile={datafile}')\n\n        self.log(f'Reading {datafile}')\n\n        if channel is not None:\n            self.log('Only reading channel {}'.format(channel))\n\n\n        # Smurf header structure\n        keys = [\n            'protocol_version',\n            'crate_id',\n            'slot_number',\n            'timing_cond',\n            'number_of_channels',\n            #'tes_bias', < TO DO, include the TES bias values\n            'timestamp',\n            'flux_ramp_increment',\n            'flux_ramp_offset',\n            'counter_0',\n            'counter_1',\n            'counter_2',\n            'reset_bits',\n            'frame_counter',\n            'tes_relays_config',\n            'external_time',\n            'control_field',\n            'test_params',\n            'num_rows',\n            'num_rows_reported',\n            'row_length',\n            'data_rate',\n        ]\n        data_keys = [f'data{i}' for i in range(528)]\n\n        keys.extend(data_keys)\n        keys_dict = dict(zip(keys, range(len(keys))))\n\n        # Read in all channels by default\n        if channel is None:\n            channel = np.arange(512)\n\n        channel = np.ravel(np.asarray(channel))\n        n_chan = len(channel)\n\n        # Indices for input channels\n        channel_mask = np.zeros(n_chan, dtype=int)\n        for i, c in enumerate(channel):\n            channel_mask[i] = keys_dict['data{}'.format(c)]\n\n        eval_n_samp = False\n        if n_samp is not None:\n            eval_n_samp = True\n\n        # Make holder arrays for phase and timestamp\n        phase = np.zeros((n_chan,0))\n        timestamp2 = np.array([])\n        counter = 0\n        n = 20000  # Number of elements to load at a time\n        tmp_phase = np.zeros((n_chan, n))\n        tmp_timestamp2 = np.zeros(n)\n        with open(datafile, mode='rb') as file:\n            while True:\n\n                # Read the Rogue header which is 8-byte long:\n                # - 4 bytes : Length of the following data block in bytes,\n                #             It includes the next 4 bytes in the header.\n                # - 1 byte  : Channel ID.\n                # - 1 byte  : Frame error.\n                # - 2 bytes : Frame flags.\n                rogue_header = dict()\n\n                # Read the first 4-byte word, which is the length\n                chunk = file.read(4)\n\n                # Check if we reach the end of the file\n                if not chunk:\n                    # If frame is incomplete - meaning end of file\n                    phase = np.hstack((phase, tmp_phase[:,:counter%n]))\n                    timestamp2 = np.append(timestamp2, tmp_timestamp2[:counter%n])\n                    break\n\n                # Convert the 4-byte word to length\n                rogue_header['length'] = struct.Struct('I').unpack(chunk)[0]\n\n                # Read the sencond 4-byte word and extract the channel, error, and flags\n                chunk = file.read(4)\n                word = struct.Struct('I').unpack(chunk)[0]\n                rogue_header['channel'] = (word >> 24) & 0xff\n                rogue_header['error'] = (word >> 16) & 0xff\n                rogue_header['flags'] = (word ) & 0xffff\n\n\n                # Check if this is a block of data or metadata\n                # Data comes on channel 0, and metadata on channel 1\n\n                if rogue_header['channel'] == 1:\n\n                    # This is our meta data.\n                    # We need to process it here.\n\n                    # Skip for now\n                    chunk = file.read(rogue_header['length']-4)\n\n                elif rogue_header['channel'] != 0:\n                    # Skip data on unknown channels, but print\n                    # a warning message\n                    self.log(f\"WARNING. Data present on an unknown channel: {rogue_header['channel']}\")\n                    chunk = file.read(rogue_header['length']-4)\n                else:\n                    # This is a data block. Processes it\n\n                    if eval_n_samp:\n                        if counter >= n_samp:\n                            phase = np.hstack((phase, tmp_phase[:,:counter%n]))\n                            timestamp2 = np.append(timestamp2,\n                                                   tmp_timestamp2[:counter%n])\n                            break\n\n                    chunk = file.read(2240)  # Frame size is 2240\n\n                    # This is the structure of the header (see README.SmurfPacket.md for a details)\n                    # Note: This assumes that the header version is 1 (currently the only version available),\n                    # which has a length of 128 bytes. In the future, we should check first the version,\n                    # and then unpack the data base on the version number.\n                    # TO DO: Extract the TES BIAS values\n                    #                         ->| |<-\n                    frame = struct.Struct('BBBBI40xQIIIIQIII4x5B3xBB6xHH4xHH4x528i').unpack(chunk)\n\n                    # Extract detector data\n                    for i, c in enumerate(channel_mask):\n                        tmp_phase[i,counter%n] = frame[c]\n\n                    # Timestamp data\n                    tmp_timestamp2[counter%n] = frame[keys_dict['timestamp']]\n\n                    # Store the data in a useful array and reset tmp arrays\n                    if counter % n == n - 1 :\n                        self.log('{} elements loaded'.format(counter+1))\n                        phase = np.hstack((phase, tmp_phase))\n                        timestamp2 = np.append(timestamp2, tmp_timestamp2)\n                        tmp_phase = np.zeros((n_chan, n))\n                        tmp_timestamp2 = np.zeros(n)\n                    counter = counter + 1\n\n        phase = np.squeeze(phase)\n        phase = phase.astype(float) / 2**15 * np.pi # where is decimal?  Is it in rad?\n\n        rootpath = os.path.dirname(datafile)\n        filename = os.path.basename(datafile)\n        timestamp = filename.split('.')[0]\n\n        # make a mask from mask file\n        mask = self.make_mask_lookup(datafile.replace('.dat', '_mask.txt'))\n\n        return timestamp2, phase, mask"}
{"input": "def safe_encode(x):\n    \"\"\"Encodes a python value for prov\n    \"\"\"\n    if x is None:\n        return prov.Literal(\"Unknown\", prov.XSD['string'])\n    if isinstance(x, (str, unicode)):\n        return prov.Literal(x, prov.XSD['string'])\n    if isinstance(x, (int,)):\n        return prov.Literal(int(x), prov.XSD['integer'])\n    if isinstance(x, (float,)):\n        return prov.Literal(x, prov.XSD['float'])\n    return pm.Literal(json.dumps(x), prov.XSD['string'])", "output": "def safe_encode(x):\n    \"\"\"Encodes a python value for prov\n    \"\"\"\n    if x is None:\n        return prov.Literal(\"Unknown\", prov.XSD['string'])\n    if isinstance(x, (str, unicode)):\n        return prov.Literal(x, prov.XSD['string'])\n    if isinstance(x, (int,)):\n        return prov.Literal(int(x), prov.XSD['integer'])\n    if isinstance(x, (float,)):\n        return prov.Literal(x, prov.XSD['float'])\n    return prov.Literal(json.dumps(x), prov.XSD['string'])"}
{"input": "def onUnFlaggedItem(post, user, timestamp=None):\n    if timestamp is None:\n        timestamp = datetime.datetime.now()\n\n    post.offensive_flag_count = post.offensive_flag_count - 1\n    post.save()\n\n    flagged_user = post.author\n\n    flagged_user.receive_reputation(\n        - askbot_settings.REP_LOSS_FOR_RECEIVING_FLAG\n    )\n    flagged_user.save()\n\n    question = post.thread._question_post()\n\n    reputation = Repute(\n                    user=flagged_user,\n                    positive=askbot_settings.REP_LOSS_FOR_RECEIVING_FLAG,\n                    question=question,\n                    reputed_at=timestamp,\n                    reputation_type=-4,#todo: clean up magic number\n                    reputation=flagged_user.reputation\n                )\n    reputation.save()\n\n    signals.remove_flag_offensive.send(\n        sender=post.__class__,\n        instance=post,\n        mark_by=user\n    )\n\n    if post.post_type == 'comment':\n        #do not hide or delete comments automatically yet,\n        #because there is no .deleted field in the comment model\n        return\n\n    #todo: These should be updated to work on same revisions.\n    # The post fell below HIDE treshold - unhide it.\n    if post.offensive_flag_count ==  askbot_settings.MIN_FLAGS_TO_HIDE_POST - 1:\n        #todo: strange - are we supposed to hide the post here or the name of\n        #setting is incorrect?\n        flagged_user.receive_reputation(\n            - askbot_settings.REP_LOSS_FOR_RECEIVING_THREE_FLAGS_PER_REVISION\n        )\n\n        flagged_user.save()\n\n        reputation = Repute(\n            user=flagged_user,\n            positive=\\\n                askbot_settings.REP_LOSS_FOR_RECEIVING_THREE_FLAGS_PER_REVISION,\n            question=question,\n            reputed_at=timestamp,\n            reputation_type=-6,\n            reputation=flagged_user.reputation\n        )\n        reputation.save()\n    # The post fell below DELETE treshold, undelete it\n    elif post.offensive_flag_count == askbot_settings.MIN_FLAGS_TO_DELETE_POST-1 :\n        flagged_user.receive_reputation(\n            - askbot_settings.REP_LOSS_FOR_RECEIVING_FIVE_FLAGS_PER_REVISION\n        )\n\n        flagged_user.save()\n\n        reputation = Repute(\n            user=flagged_user,\n            positive =\\\n                askbot_settings.REP_LOSS_FOR_RECEIVING_FIVE_FLAGS_PER_REVISION,\n            question=question,\n            reputed_at=timestamp,\n            reputation_type=-7,\n            reputation=flagged_user.reputation\n        )\n        reputation.save()\n\n        post.deleted = False\n        post.save()", "output": "def onUnFlaggedItem(post, user, timestamp=None):\n    if timestamp is None:\n        timestamp = datetime.datetime.now()\n\n    post.offensive_flag_count = post.offensive_flag_count - 1\n    post.save()\n\n    flagged_user = post.author\n\n    flagged_user.receive_reputation(\n        askbot_settings.REP_LOSS_FOR_RECEIVING_FLAG\n    )\n    flagged_user.save()\n\n    question = post.thread._question_post()\n\n    reputation = Repute(\n                    user=flagged_user,\n                    positive=askbot_settings.REP_LOSS_FOR_RECEIVING_FLAG,\n                    question=question,\n                    reputed_at=timestamp,\n                    reputation_type=-4,#todo: clean up magic number\n                    reputation=flagged_user.reputation\n                )\n    reputation.save()\n\n    signals.remove_flag_offensive.send(\n        sender=post.__class__,\n        instance=post,\n        mark_by=user\n    )\n\n    if post.post_type == 'comment':\n        #do not hide or delete comments automatically yet,\n        #because there is no .deleted field in the comment model\n        return\n\n    #todo: These should be updated to work on same revisions.\n    # The post fell below HIDE treshold - unhide it.\n    if post.offensive_flag_count ==  askbot_settings.MIN_FLAGS_TO_HIDE_POST - 1:\n        #todo: strange - are we supposed to hide the post here or the name of\n        #setting is incorrect?\n        flagged_user.receive_reputation(\n            - askbot_settings.REP_LOSS_FOR_RECEIVING_THREE_FLAGS_PER_REVISION\n        )\n\n        flagged_user.save()\n\n        reputation = Repute(\n            user=flagged_user,\n            positive=\\\n                askbot_settings.REP_LOSS_FOR_RECEIVING_THREE_FLAGS_PER_REVISION,\n            question=question,\n            reputed_at=timestamp,\n            reputation_type=-6,\n            reputation=flagged_user.reputation\n        )\n        reputation.save()\n    # The post fell below DELETE treshold, undelete it\n    elif post.offensive_flag_count == askbot_settings.MIN_FLAGS_TO_DELETE_POST-1 :\n        flagged_user.receive_reputation(\n            - askbot_settings.REP_LOSS_FOR_RECEIVING_FIVE_FLAGS_PER_REVISION\n        )\n\n        flagged_user.save()\n\n        reputation = Repute(\n            user=flagged_user,\n            positive =\\\n                askbot_settings.REP_LOSS_FOR_RECEIVING_FIVE_FLAGS_PER_REVISION,\n            question=question,\n            reputed_at=timestamp,\n            reputation_type=-7,\n            reputation=flagged_user.reputation\n        )\n        reputation.save()\n\n        post.deleted = False\n        post.save()"}
{"input": "def add_vpc_route(region, vpc_id, network, resource_id):\n    region_key = region.replace('-', '_')\n    aws_key = getattr(settings.app, region_key + '_access_key')\n    aws_secret = getattr(settings.app, region_key + '_secret_key')\n\n    vpc_conn = boto.connect_vpc(\n        aws_access_key_id=aws_key,\n        aws_secret_access_key=aws_secret,\n        region=boto.ec2.get_region(region),\n    )\n\n    tables = vpc_conn.get_all_route_tables(filters={'vpc-id': vpc_id})\n    if tables:\n        raise VpcRouteTableNotFound('Failed to find VPC routing table')\n    table = tables[0]\n\n    instance_id = None\n    interface_id = None\n    if resource_id.startswith('eni-'):\n        interface_id = resource_id\n    else:\n        instance_id = resource_id\n\n    try:\n        vpc_conn.create_route(\n            table.id,\n            network,\n            instance_id=instance_id,\n            interface_id=interface_id,\n        )\n    except:\n        vpc_conn.replace_route(\n            table.id,\n            network,\n            instance_id=instance_id,\n            interface_id=interface_id,\n        )", "output": "def add_vpc_route(region, vpc_id, network, resource_id):\n    region_key = region.replace('-', '_')\n    aws_key = getattr(settings.app, region_key + '_access_key')\n    aws_secret = getattr(settings.app, region_key + '_secret_key')\n\n    vpc_conn = boto.connect_vpc(\n        aws_access_key_id=aws_key,\n        aws_secret_access_key=aws_secret,\n        region=boto.ec2.get_region(region),\n    )\n\n    tables = vpc_conn.get_all_route_tables(filters={'vpc-id': vpc_id})\n    if not tables:\n        raise VpcRouteTableNotFound('Failed to find VPC routing table')\n    table = tables[0]\n\n    instance_id = None\n    interface_id = None\n    if resource_id.startswith('eni-'):\n        interface_id = resource_id\n    else:\n        instance_id = resource_id\n\n    try:\n        vpc_conn.create_route(\n            table.id,\n            network,\n            instance_id=instance_id,\n            interface_id=interface_id,\n        )\n    except:\n        vpc_conn.replace_route(\n            table.id,\n            network,\n            instance_id=instance_id,\n            interface_id=interface_id,\n        )"}
{"input": "def find_used_variables_in_text(variant, recipe_text, selectors=False):\n    used_variables = set()\n    recipe_lines = recipe_text.splitlines()\n    for v in variant:\n        all_res = []\n        compiler_match = re.match(r'(.*?)_compiler$', v)\n        if compiler_match and not selectors:\n            compiler_lang = compiler_match.group(1)\n            compiler_regex = (\n                r\"\\{\\s*compiler\\([\\'\\\"]%s[\\\"\\'][^\\{]*?\\}\" % re.escape(compiler_lang)\n            )\n            all_res.append(compiler_regex)\n            variant_lines = [line for line in recipe_lines if v in line or compiler_lang in line]\n        else:\n            variant_lines = [line for line in recipe_lines if v in line.replace('-', '_')]\n        if not variant_lines:\n            continue\n        v_regex = re.escape(v)\n        v_req_regex = '[-_]'.join(map(re.escape, v.split('_')))\n        variant_regex = r\"\\{\\s*(?:pin_[a-z]+\\(\\s*?['\\\"])?%s[^'\\\"]*?\\}\\}\" % v_regex\n        selector_regex = r\"^[^#\\[]*?\\#?\\s\\[[^\\]]*?(?<![_\\w\\d])%s[=\\s<>!\\]]\" % v_regex\n        conditional_regex = r\"(?:^|[^\\{])\\{%\\s*(?:el)?if\\s*\" + v_regex + r\"\\s*(?:[^%]*?)?%\\}\"\n        # plain req name, no version spec.  Look for end of line after name, or comment or selector\n        requirement_regex = r\"^\\s+\\-\\s+%s\\s*(?:\\s[\\[#]|$)\" % v_req_regex\n        if not selectors:\n            all_res.extend([selector_regex])\n        else:\n            all_res.extend([variant_regex, requirement_regex, conditional_regex])\n        # consolidate all re's into one big one for speedup\n        all_res = r\"|\".join(all_res)\n        if any(re.search(all_res, line) for line in variant_lines):\n            used_variables.add(v)\n    return used_variables", "output": "def find_used_variables_in_text(variant, recipe_text, selectors=False):\n    used_variables = set()\n    recipe_lines = recipe_text.splitlines()\n    for v in variant:\n        all_res = []\n        compiler_match = re.match(r'(.*?)_compiler$', v)\n        if compiler_match and not selectors:\n            compiler_lang = compiler_match.group(1)\n            compiler_regex = (\n                r\"\\{\\s*compiler\\([\\'\\\"]%s[\\\"\\'][^\\{]*?\\}\" % re.escape(compiler_lang)\n            )\n            all_res.append(compiler_regex)\n            variant_lines = [line for line in recipe_lines if v in line or compiler_lang in line]\n        else:\n            variant_lines = [line for line in recipe_lines if v in line.replace('-', '_')]\n        if not variant_lines:\n            continue\n        v_regex = re.escape(v)\n        v_req_regex = '[-_]'.join(map(re.escape, v.split('_')))\n        variant_regex = r\"\\{\\s*(?:pin_[a-z]+\\(\\s*?['\\\"])?%s[^'\\\"]*?\\}\\}\" % v_regex\n        selector_regex = r\"^[^#\\[]*?\\#?\\s\\[[^\\]]*?(?<![_\\w\\d])%s[=\\s<>!\\]]\" % v_regex\n        conditional_regex = r\"(?:^|[^\\{])\\{%\\s*(?:el)?if\\s*\" + v_regex + r\"\\s*(?:[^%]*?)?%\\}\"\n        # plain req name, no version spec.  Look for end of line after name, or comment or selector\n        requirement_regex = r\"^\\s+\\-\\s+%s\\s*(?:\\s[\\[#]|$)\" % v_req_regex\n        if selectors:\n            all_res.extend([selector_regex])\n        else:\n            all_res.extend([variant_regex, requirement_regex, conditional_regex])\n        # consolidate all re's into one big one for speedup\n        all_res = r\"|\".join(all_res)\n        if any(re.search(all_res, line) for line in variant_lines):\n            used_variables.add(v)\n    return used_variables"}
{"input": "def configure_subscription(auth):\n    user = auth.user\n    subscription = request.json\n    event = subscription.get('event')\n    notification_type = subscription.get('notification_type')\n\n    if not event or notification_type:\n        raise HTTPError(http.BAD_REQUEST, data=dict(\n            message_long=\"Must provide an event and notification type for subscription.\")\n        )\n\n    uid = subscription.get('id')\n    event_id = uid + \"_\" + event\n\n    if notification_type == 'adopt_parent':\n        try:\n            s = Subscription.find_one(Q('_id', 'eq', event_id))\n        except NoResultsFound:\n            return\n        s.remove_user_from_subscription(user)\n\n    else:\n        try:\n            s = Subscription(_id=event_id)\n            s.save()\n\n        except KeyExistsException:\n            s = Subscription.find_one(Q('_id', 'eq', event_id))\n\n        s.object_id = uid\n        s.event_name = event\n        s.save()\n\n        # Add user to list of subscribers\n        if notification_type not in s._fields:\n            setattr(s, notification_type, [])\n            s.save()\n\n        if user not in getattr(s, notification_type):\n            getattr(s, notification_type).append(user)\n            s.save()\n\n        for nt in NOTIFICATION_TYPES:\n            if nt != notification_type:\n                if getattr(s, nt) and user in getattr(s, nt):\n                    getattr(s, nt).remove(user)\n                    s.save()", "output": "def configure_subscription(auth):\n    user = auth.user\n    subscription = request.json\n    event = subscription.get('event')\n    notification_type = subscription.get('notification_type')\n\n    if not event or not notification_type:\n        raise HTTPError(http.BAD_REQUEST, data=dict(\n            message_long=\"Must provide an event and notification type for subscription.\")\n        )\n\n    uid = subscription.get('id')\n    event_id = uid + \"_\" + event\n\n    if notification_type == 'adopt_parent':\n        try:\n            s = Subscription.find_one(Q('_id', 'eq', event_id))\n        except NoResultsFound:\n            return\n        s.remove_user_from_subscription(user)\n\n    else:\n        try:\n            s = Subscription(_id=event_id)\n            s.save()\n\n        except KeyExistsException:\n            s = Subscription.find_one(Q('_id', 'eq', event_id))\n\n        s.object_id = uid\n        s.event_name = event\n        s.save()\n\n        # Add user to list of subscribers\n        if notification_type not in s._fields:\n            setattr(s, notification_type, [])\n            s.save()\n\n        if user not in getattr(s, notification_type):\n            getattr(s, notification_type).append(user)\n            s.save()\n\n        for nt in NOTIFICATION_TYPES:\n            if nt != notification_type:\n                if getattr(s, nt) and user in getattr(s, nt):\n                    getattr(s, nt).remove(user)\n                    s.save()"}
{"input": "def _feature_forum_closed(self):\n        return not self.election.feature_closed", "output": "def _feature_forum_closed(self):\n        return self.election.feature_closed"}
{"input": "def log_conditional_probability(segmented_topics, per_topic_postings, num_docs):\n    \"\"\"\n    This function calculates the log-conditional-probability measure\n    which is used by coherence measures such as U_mass.\n    This is defined as: m_lc(S_i) = log[(P(W', W*) + e) / P(W*)]\n\n    Args:\n    ----\n    segmented_topics : Output from the segmentation module of the segmented topics. Is a list of list of tuples.\n    per_topic_postings : Output from the probability_estimation module. Is a dictionary of the posting list of all topics.\n    num_docs : Total number of documents in corresponding corpus.\n\n    Returns:\n    -------\n    m_lc : List of log conditional probability measure on each set in segmented topics.\n    \"\"\"\n    m_lc = []\n    for s_i in segmented_topics:\n        for w_prime, w_star in s_i:\n            w_prime_docs = per_topic_postings[w_prime]\n            w_star_docs = per_topic_postings[w_star]\n            co_docs = w_prime_docs.intersection(w_star_docs)\n            if  w_star_docs:\n                m_lc_i = np.log(((len(co_docs) / float(num_docs)) + EPSILON) / (len(w_star_docs) / float(num_docs)))\n            else:\n                m_lc_i = 0\n            m_lc.append(m_lc_i)\n\n    return m_lc", "output": "def log_conditional_probability(segmented_topics, per_topic_postings, num_docs):\n    \"\"\"\n    This function calculates the log-conditional-probability measure\n    which is used by coherence measures such as U_mass.\n    This is defined as: m_lc(S_i) = log[(P(W', W*) + e) / P(W*)]\n\n    Args:\n    ----\n    segmented_topics : Output from the segmentation module of the segmented topics. Is a list of list of tuples.\n    per_topic_postings : Output from the probability_estimation module. Is a dictionary of the posting list of all topics.\n    num_docs : Total number of documents in corresponding corpus.\n\n    Returns:\n    -------\n    m_lc : List of log conditional probability measure on each set in segmented topics.\n    \"\"\"\n    m_lc = []\n    for s_i in segmented_topics:\n        for w_prime, w_star in s_i:\n            w_prime_docs = per_topic_postings[w_prime]\n            w_star_docs = per_topic_postings[w_star]\n            co_docs = w_prime_docs.intersection(w_star_docs)\n            if  w_star_docs:\n                m_lc_i = np.log(((len(co_docs) / float(num_docs)) + EPSILON) / (len(w_star_docs) / float(num_docs)))\n            else:\n                m_lc_i = 0.0\n            m_lc.append(m_lc_i)\n\n    return m_lc"}
{"input": "def getbool(b):\n    \"\"\" Test if a value it true or not \"\"\"\n    return b.lower() in ('yes', 'true', 'on', 1)", "output": "def getbool(b):\n    \"\"\" Test if a value it true or not \"\"\"\n    return b.lower() in ('yes', 'true', 'on', '1')"}
{"input": "def test_correct_dc_rack_in_nodetool_info(self):\n        \"\"\"\n        @jira_ticket CASSANDRA-10382\n\n        Test that nodetool info returns the correct rack and dc\n        \"\"\"\n\n        cluster = self.cluster\n        cluster.populate([2, 2])\n        cluster.set_configuration_options(values={'endpoint_snitch': 'org.apache.cassandra.locator.GossipingPropertyFileSnitch'})\n\n        for i, node in enumerate(cluster.nodelist()):\n            with open(os.path.join(node.get_conf_dir(), 'cassandra-rackdc.properties'), 'w') as snitch_file:\n                for line in [\"dc={}\".format(node.data_center), \"rack=rack{}\".format(i % 2)]:\n                    snitch_file.write(line + os.linesep)\n\n        cluster.start(wait_for_binary_proto='True')\n\n        for i, node in enumerate(cluster.nodelist()):\n            out, err = node.nodetool('info')\n            self.assertEqual(0, len(err), err)\n            debug(out)\n            for line in out.split(os.linesep):\n                if line.startswith('Data Center'):\n                    self.assertTrue(line.endswith(node.data_center),\n                                    \"Expected dc {} for {} but got {}\".format(node.data_center, node.address(), line.rsplit(None, 1)[-1]))\n                elif line.startswith('Rack'):\n                    rack = \"rack{}\".format(i % 2)\n                    self.assertTrue(line.endswith(rack),\n                                    \"Expected rack {} for {} but got {}\".format(rack, node.address(), line.rsplit(None, 1)[-1]))", "output": "def test_correct_dc_rack_in_nodetool_info(self):\n        \"\"\"\n        @jira_ticket CASSANDRA-10382\n\n        Test that nodetool info returns the correct rack and dc\n        \"\"\"\n\n        cluster = self.cluster\n        cluster.populate([2, 2])\n        cluster.set_configuration_options(values={'endpoint_snitch': 'org.apache.cassandra.locator.GossipingPropertyFileSnitch'})\n\n        for i, node in enumerate(cluster.nodelist()):\n            with open(os.path.join(node.get_conf_dir(), 'cassandra-rackdc.properties'), 'w') as snitch_file:\n                for line in [\"dc={}\".format(node.data_center), \"rack=rack{}\".format(i % 2)]:\n                    snitch_file.write(line + os.linesep)\n\n        cluster.start(wait_for_binary_proto=True)\n\n        for i, node in enumerate(cluster.nodelist()):\n            out, err = node.nodetool('info')\n            self.assertEqual(0, len(err), err)\n            debug(out)\n            for line in out.split(os.linesep):\n                if line.startswith('Data Center'):\n                    self.assertTrue(line.endswith(node.data_center),\n                                    \"Expected dc {} for {} but got {}\".format(node.data_center, node.address(), line.rsplit(None, 1)[-1]))\n                elif line.startswith('Rack'):\n                    rack = \"rack{}\".format(i % 2)\n                    self.assertTrue(line.endswith(rack),\n                                    \"Expected rack {} for {} but got {}\".format(rack, node.address(), line.rsplit(None, 1)[-1]))"}
{"input": "def halftone_cmyk_image(image, pitch, angles=(15, 75, 30, 45), scale=1.0):\n\tc, m, y, k = image.split()\n\tcyan = halftone_image(c, pitch, angles[0], scale)\n\tmagenta = halftone_image(m, pitch, angles[1], scale)\n\tyellow = halftone_image(y, pitch, angles[2], scale)\n\tkey = halftone_image(k, pitch, angles[3], scale)\n\treturn Image.merge(\"CMYK\", [cyan, magenta, yellow, key])", "output": "def halftone_cmyk_image(image, pitch, angles=(15, 75, 30, 45), scale=1):\n\tc, m, y, k = image.split()\n\tcyan = halftone_image(c, pitch, angles[0], scale)\n\tmagenta = halftone_image(m, pitch, angles[1], scale)\n\tyellow = halftone_image(y, pitch, angles[2], scale)\n\tkey = halftone_image(k, pitch, angles[3], scale)\n\treturn Image.merge(\"CMYK\", [cyan, magenta, yellow, key])"}
{"input": "def __init__(self, scrollbars):\n        Gtk.DrawingArea.__init__(self)\n\n        hadj = scrollbars.get_hadjustment()\n        vadj = scrollbars.get_vadjustment()\n\n        self.size_forced = False\n        self.full_size = (1, 1)\n        self.visible_size = (1, 1)\n\n        self.drawers = PriorityQueue()\n\n        self.set_hadjustment(hadj)\n        self.set_vadjustment(vadj)\n\n        self.add_events(Gdk.EventMask.SCROLL_MASK)\n        self.add_events(Gdk.EventMask.BUTTON_PRESS_MASK)\n        self.add_events(Gdk.EventMask.BUTTON_RELEASE_MASK)\n        self.add_events(Gdk.EventMask.POINTER_MOTION_MASK)\n        self.connect(\"size-allocate\", self.__on_size_allocate)\n        self.connect(\"draw\", self.__on_draw)\n        self.connect(\"scroll-event\", self.__on_scroll_event)\n        self.connect(\"button-press-event\", self.__on_button_pressed)\n        self.connect(\"motion-notify-event\", self.__on_motion)\n        self.connect(\"button-release-event\", self.__on_button_released)\n\n        self.set_size_request(-1, -1)\n\n        GLib.timeout_add(1000.0 / 30, self._tick)", "output": "def __init__(self, scrollbars):\n        Gtk.DrawingArea.__init__(self)\n\n        hadj = scrollbars.get_hadjustment()\n        vadj = scrollbars.get_vadjustment()\n\n        self.size_forced = False\n        self.full_size = (1, 1)\n        self.visible_size = (1, 1)\n\n        self.drawers = PriorityQueue()\n\n        self.set_hadjustment(hadj)\n        self.set_vadjustment(vadj)\n\n        self.add_events(Gdk.EventMask.SCROLL_MASK)\n        self.add_events(Gdk.EventMask.BUTTON_PRESS_MASK)\n        self.add_events(Gdk.EventMask.BUTTON_RELEASE_MASK)\n        self.add_events(Gdk.EventMask.POINTER_MOTION_MASK)\n        self.connect(\"size-allocate\", self.__on_size_allocate)\n        self.connect(\"draw\", self.__on_draw)\n        self.connect(\"scroll-event\", self.__on_scroll_event)\n        self.connect(\"button-press-event\", self.__on_button_pressed)\n        self.connect(\"motion-notify-event\", self.__on_motion)\n        self.connect(\"button-release-event\", self.__on_button_released)\n\n        self.set_size_request(-1, -1)\n\n        GLib.timeout_add(1000 / 30, self._tick)"}
{"input": "def get_node(self, path: str, create: bool = False):\n        \"\"\" \n        Get node object from specified path\n        \n        :param path: Path as string\n        :param create: If True, not existing nodes will be created\n        \"\"\"\n        assert isinstance(path, str), \"Node path must be a string type !\"\n\n        node = self.root\n        path = path.lstrip('/')\n        if path:\n            names = path.split('/')\n            for name in names:\n                item = node.get_subnode(name)\n                if item is None:\n                    if create:\n                        item = Node(name)\n                        node.append(item)\n                    else:\n                        raise ValueError(\"Path \\\"{}\\\" doesn't exists\".format(self, path))\n                node = item\n\n        return node", "output": "def get_node(self, path: str, create: bool = False):\n        \"\"\" \n        Get node object from specified path\n        \n        :param path: Path as string\n        :param create: If True, not existing nodes will be created\n        \"\"\"\n        assert isinstance(path, str), \"Node path must be a string type !\"\n\n        node = self.root\n        path = path.lstrip('/')\n        if path:\n            names = path.split('/')\n            for name in names:\n                item = node.get_subnode(name)\n                if item is None:\n                    if create:\n                        item = Node(name)\n                        node.append(item)\n                    else:\n                        raise ValueError(\"Path \\\"{}\\\" doesn't exists\".format(path))\n                node = item\n\n        return node"}
{"input": "def string_references(self, minimum_length=1):\n        \"\"\"\n        ALl of the constant string reference used by this function\n        :param minimum_length: the minimum length of strings to find (default is 1)\n        :return: a list of tuples of (address, string) where is address is the location of the string in memory\n        \"\"\"\n        strings = []\n        memory = self._function_manager._project.ld.memory\n\n        # get known instruction addresses and call targets\n        # these addresses cannot be string references, but show up frequently in the runtime values\n        known_executable_addresses = set()\n        for b in self.basic_blocks:\n            if b in memory:\n                sirsb = self._function_manager._project.block(b)\n                known_executable_addresses.update(sirsb.instruction_addrs())\n        for node in self._function_manager._cfg.nodes():\n            known_executable_addresses.add(node.addr)\n\n        # loop over all local runtime values and check if the value points to a printable string\n        for addr in self.partial_local_runtime_values:\n            if addr in memory:\n                # check that the address isn't an pointing to known executable code\n                # and that it isn't an indirect pointer to known executable code\n                possible_pointer = memory.read_addr_at(addr, self._function_manager._project.ld.main_bin.archinfo)\n                if addr not in known_executable_addresses and possible_pointer not in known_executable_addresses:\n                    # build string\n                    str = \"\"\n                    offset = 0\n                    current_char = memory[addr + offset]\n                    while current_char in string.printable:\n                        str += current_char\n                        offset += 1\n                        current_char = memory[addr + offset]\n\n                    # check that the string was a null terminated string with minimum length\n                    if current_char == \"\\x00\" and len(str) >= minimum_length:\n                        strings.append((addr, str))\n\n        return strings", "output": "def string_references(self, minimum_length=1):\n        \"\"\"\n        ALl of the constant string reference used by this function\n        :param minimum_length: the minimum length of strings to find (default is 1)\n        :return: a list of tuples of (address, string) where is address is the location of the string in memory\n        \"\"\"\n        strings = []\n        memory = self._function_manager._project.ld.memory\n\n        # get known instruction addresses and call targets\n        # these addresses cannot be string references, but show up frequently in the runtime values\n        known_executable_addresses = set()\n        for b in self.basic_blocks:\n            if b in memory:\n                sirsb = self._function_manager._project.block(b)\n                known_executable_addresses.update(sirsb.instruction_addrs())\n        for node in self._function_manager._cfg.nodes():\n            known_executable_addresses.add(node.addr)\n\n        # loop over all local runtime values and check if the value points to a printable string\n        for addr in self.partial_local_runtime_values:\n            if addr in memory:\n                # check that the address isn't an pointing to known executable code\n                # and that it isn't an indirect pointer to known executable code\n                possible_pointer = memory.read_addr_at(addr)\n                if addr not in known_executable_addresses and possible_pointer not in known_executable_addresses:\n                    # build string\n                    str = \"\"\n                    offset = 0\n                    current_char = memory[addr + offset]\n                    while current_char in string.printable:\n                        str += current_char\n                        offset += 1\n                        current_char = memory[addr + offset]\n\n                    # check that the string was a null terminated string with minimum length\n                    if current_char == \"\\x00\" and len(str) >= minimum_length:\n                        strings.append((addr, str))\n\n        return strings"}
{"input": "def from_bitstring(cls, bitstring, level=-1, **kwargs):\n        \"\"\"Initialize from bitstring (e.g. '10010011').\n\n        Parameters\n        ----------\n        bitstring : str\n            String of 1s and 0s.\n        level ; int, optional (default -1)\n            Level of fingerprint. 0th level just uses initial atom\n            identifiers, 1st level is after 1st iteration, `n`th level is\n            after `n` iterations.\n\n        Returns\n        -------\n        Fingerprint : fingerprint\n        \"\"\"\n        indices = [i for i, char in enumerate(bitstring) if char != 0]\n        if kwargs.get(\"bits\", None) is None:\n            kwargs[\"bits\"] = len(bitstring)\n        return cls.from_indices(indices, level=level, **kwargs)", "output": "def from_bitstring(cls, bitstring, level=-1, **kwargs):\n        \"\"\"Initialize from bitstring (e.g. '10010011').\n\n        Parameters\n        ----------\n        bitstring : str\n            String of 1s and 0s.\n        level ; int, optional (default -1)\n            Level of fingerprint. 0th level just uses initial atom\n            identifiers, 1st level is after 1st iteration, `n`th level is\n            after `n` iterations.\n\n        Returns\n        -------\n        Fingerprint : fingerprint\n        \"\"\"\n        indices = [i for i, char in enumerate(bitstring) if char != '0']\n        if kwargs.get(\"bits\", None) is None:\n            kwargs[\"bits\"] = len(bitstring)\n        return cls.from_indices(indices, level=level, **kwargs)"}
{"input": "def addPackageChild(self, name, links, password, root, paused):\n        \"\"\"Adds a package, with links to desired package.\n\n        :param root: parents package id\n        :return: package id of the new package\n        \"\"\"\n        if self.core.config['general']['folder_per_package']:\n            folder = name\n        else:\n            folder = \"\"\n\n        pid = self.createPackage(name, folder, root, password, paused=paused)\n        self.addLinks(pid, links, paused)\n\n        return pid", "output": "def addPackageChild(self, name, links, password, root, paused):\n        \"\"\"Adds a package, with links to desired package.\n\n        :param root: parents package id\n        :return: package id of the new package\n        \"\"\"\n        if self.core.config['general']['folder_per_package']:\n            folder = name\n        else:\n            folder = \"\"\n\n        pid = self.createPackage(name, folder, root, password, paused=paused)\n        self.addLinks(pid, links)\n\n        return pid"}
{"input": "def test_detector_angles():\n    #set a test date\n    date = parse_time('2012-02-15')\n    file = fermi.download_weekly_pointing_file(date)\n    det=fermi.get_detector_sun_angles_for_date(date,file,plot=False)\n    assert len(det) == 12\n    #assert type(det) == collections.OrderedDict\n    assert_almost_equal(det['n0'][0], 20.30309,decimal=1)\n    assert_almost_equal(det['n1'][0], 30.30430, decimal=1)\n    assert_almost_equal(det['n2'][0], 74.86032, decimal=1)\n    assert_almost_equal(det['n3'][0], 31.24400, decimal=1)\n    assert_almost_equal(det['n4'][0], 75.10403, decimal=1)\n    assert_almost_equal(det['n5'][0], 60.40967, decimal=1)\n    assert_almost_equal(det['n6'][0], 46.14087, decimal=1)\n    assert_almost_equal(det['n7'][0], 69.71780, decimal=1)\n    assert_almost_equal(det['n8'][0], 106.08064, decimal=1)\n    assert_almost_equal(det['n9'][0], 68.543067, decimal=1)\n    assert_almost_equal(det['n10'][0], 105.76825, decimal=1)\n    assert_almost_equal(det['n11'][0], 119.69057, decimal=1)\n\n    det2=fermi.get_detector_sun_angles_for_time(parse_time('2012-02-15 02:00'),file)\n    assert len(det2) == 12\n    assert type(det2) == dict\n    assert_almost_equal(det2['n0'], 87.24744,decimal=1)\n    assert_almost_equal(det2['n1'], 69.90883,decimal=1)\n    assert_almost_equal(det2['n10'], 123.56429,decimal=1)\n    assert_almost_equal(det2['n11'], 167.26615,decimal=1)\n    assert_almost_equal(det2['n2'], 59.82642,decimal=1)\n    assert_almost_equal(det2['n3'], 69.18959,decimal=1)\n    assert_almost_equal(det2['n4'], 56.83158,decimal=1)\n    assert_almost_equal(det2['n5'], 12.49959,decimal=1)\n    assert_almost_equal(det2['n6'], 115.31259,decimal=1)\n    assert_almost_equal(det2['n7'], 129.49283,decimal=1)\n    assert_almost_equal(det2['n8'], 121.91083,decimal=1)\n    assert_almost_equal(det2['n9'], 130.04144,decimal=1)", "output": "def test_detector_angles():\n    #set a test date\n    date = parse_time('2012-02-15')\n    file = fermi.download_weekly_pointing_file(date)\n    det=fermi.get_detector_sun_angles_for_date(date,file)\n    assert len(det) == 12\n    #assert type(det) == collections.OrderedDict\n    assert_almost_equal(det['n0'][0], 20.30309,decimal=1)\n    assert_almost_equal(det['n1'][0], 30.30430, decimal=1)\n    assert_almost_equal(det['n2'][0], 74.86032, decimal=1)\n    assert_almost_equal(det['n3'][0], 31.24400, decimal=1)\n    assert_almost_equal(det['n4'][0], 75.10403, decimal=1)\n    assert_almost_equal(det['n5'][0], 60.40967, decimal=1)\n    assert_almost_equal(det['n6'][0], 46.14087, decimal=1)\n    assert_almost_equal(det['n7'][0], 69.71780, decimal=1)\n    assert_almost_equal(det['n8'][0], 106.08064, decimal=1)\n    assert_almost_equal(det['n9'][0], 68.543067, decimal=1)\n    assert_almost_equal(det['n10'][0], 105.76825, decimal=1)\n    assert_almost_equal(det['n11'][0], 119.69057, decimal=1)\n\n    det2=fermi.get_detector_sun_angles_for_time(parse_time('2012-02-15 02:00'),file)\n    assert len(det2) == 12\n    assert type(det2) == dict\n    assert_almost_equal(det2['n0'], 87.24744,decimal=1)\n    assert_almost_equal(det2['n1'], 69.90883,decimal=1)\n    assert_almost_equal(det2['n10'], 123.56429,decimal=1)\n    assert_almost_equal(det2['n11'], 167.26615,decimal=1)\n    assert_almost_equal(det2['n2'], 59.82642,decimal=1)\n    assert_almost_equal(det2['n3'], 69.18959,decimal=1)\n    assert_almost_equal(det2['n4'], 56.83158,decimal=1)\n    assert_almost_equal(det2['n5'], 12.49959,decimal=1)\n    assert_almost_equal(det2['n6'], 115.31259,decimal=1)\n    assert_almost_equal(det2['n7'], 129.49283,decimal=1)\n    assert_almost_equal(det2['n8'], 121.91083,decimal=1)\n    assert_almost_equal(det2['n9'], 130.04144,decimal=1)"}
{"input": "def add_identity(self, category='', itype='', name='', node=''):\n        self.add_node(node)\n        self.nodes[node].addIdentity(category=category,\n                         id_type=itype,\n                         name=name)", "output": "def add_identity(self, category='', itype='', name='', node=''):\n        self.add_node(node)\n        self.nodes[node].addIdentity(category=category,\n                         itype=itype,\n                         name=name)"}
{"input": "def search(request, template=None):\n    \"\"\"Performs search or displays the search form.\"\"\"\n\n    # JSON-specific variables\n    is_json = (request.GET.get('format') == 'json')\n    callback = request.GET.get('callback', '').strip()\n    mimetype = 'application/x-javascript' if callback else 'application/json'\n\n    # Search \"Expires\" header format\n    expires_fmt = '%A, %d %B %Y %H:%M:%S GMT'\n\n    # Check callback is valid\n    if is_json and callback and not jsonp_is_valid(callback):\n        return HttpResponse(\n            json.dumps({'error': _('Invalid callback function.')}),\n            mimetype=mimetype, status=400)\n\n    language = locale_or_default(request.GET.get('language', request.locale))\n    r = request.GET.copy()\n    a = request.GET.get('a', '0')\n\n    # Search default values\n    try:\n        category = map(int, r.getlist('category')) or \\\n                   settings.SEARCH_DEFAULT_CATEGORIES\n    except ValueError:\n        category = settings.SEARCH_DEFAULT_CATEGORIES\n    r.setlist('category', category)\n\n    # Basic form\n    if a == '0':\n        r['w'] = r.get('w', constants.WHERE_BASIC)\n    # Advanced form\n    if a == '2':\n        r['language'] = language\n        r['a'] = '1'\n\n    # TODO: Rewrite so SearchForm is unbound initially and we can use `initial`\n    # on the form fields.\n    if 'include_archived' not in r:\n        r['include_archived'] = False\n\n    search_form = SearchForm(r)\n\n    if not search_form.is_valid() or a == '2':\n        if is_json:\n            return HttpResponse(\n                json.dumps({'error': _('Invalid search data.')}),\n                mimetype=mimetype,\n                status=400)\n\n        t = template if request.MOBILE else 'search/form.html'\n        search_ = jingo.render(request, t,\n                               {'advanced': a, 'request': request,\n                                'search_form': search_form})\n        search_['Cache-Control'] = 'max-age=%s' % \\\n                                   (settings.SEARCH_CACHE_PERIOD * 60)\n        search_['Expires'] = (datetime.utcnow() +\n                              timedelta(\n                                minutes=settings.SEARCH_CACHE_PERIOD)) \\\n                              .strftime(expires_fmt)\n        return search_\n\n    cleaned = search_form.cleaned_data\n\n    page = max(smart_int(request.GET.get('page')), 1)\n    offset = (page - 1) * settings.SEARCH_RESULTS_PER_PAGE\n\n    # get language name for display in template\n    lang = language.lower()\n    if settings.LANGUAGES.get(lang):\n        lang_name = settings.LANGUAGES[lang]\n    else:\n        lang_name = ''\n\n    wiki_s = wiki_search\n    question_s = question_search\n    discussion_s = discussion_search\n\n    documents = []\n\n    # wiki filters\n    # Category filter\n    if cleaned['category']:\n        wiki_s = wiki_s.filter(category__in=cleaned['category'])\n\n    # Locale filter\n    wiki_s = wiki_s.filter(locale=language)\n\n    # Product filter\n    products = cleaned['product']\n    for p in products:\n        wiki_s = wiki_s.filter(tag=p)\n\n    # Tags filter\n    tags = [t.strip() for t in cleaned['tags'].split()]\n    for t in tags:\n        wiki_s = wiki_s.filter(tag=t)\n\n    # Archived bit\n    if a == '0' and not cleaned['include_archived']:\n        # Default to NO for basic search:\n        cleaned['include_archived'] = False\n    if not cleaned['include_archived']:\n        wiki_s = wiki_s.filter(is_archived=False)\n    # End of wiki filters\n\n    # Support questions specific filters\n    if cleaned['w'] & constants.WHERE_SUPPORT:\n\n        # Solved is set by default if using basic search\n        if a == '0' and not cleaned['has_helpful']:\n            cleaned['has_helpful'] = constants.TERNARY_YES\n\n        # These filters are ternary, they can be either YES, NO, or OFF\n        ternary_filters = ('is_locked', 'is_solved', 'has_answers',\n                           'has_helpful')\n        d = dict((filter_name, _ternary_filter(cleaned[filter_name]))\n                 for filter_name in ternary_filters\n                 if cleaned[filter_name])\n        if d:\n            question_s = question_s.filter(**d)\n\n        if cleaned['asked_by']:\n            question_s = question_s.filter(\n                question_creator=cleaned['asked_by'])\n\n        if cleaned['answered_by']:\n            question_s = question_s.filter(\n                answer_creator=cleaned['answered_by'])\n\n        q_tags = [t.strip() for t in cleaned['q_tags'].split()]\n        for t in q_tags:\n            question_s = question_s.filter(tag=t)\n\n    # Discussion forum specific filters\n    if cleaned['w'] & constants.WHERE_DISCUSSION:\n        if cleaned['author']:\n            discussion_s = discussion_s.filter(author_ord=cleaned['author'])\n\n        if cleaned['thread_type']:\n            if constants.DISCUSSION_STICKY in cleaned['thread_type']:\n                discussion_s = discussion_s.filter(is_sticky=1)\n\n            if constants.DISCUSSION_LOCKED in cleaned['thread_type']:\n                discussion_s = discussion_s.filter(is_locked=1)\n\n        if cleaned['forum']:\n            discussion_s = discussion_s.filter(forum_id=cleaned['forum'])\n\n    # Filters common to support and discussion forums\n    # Created filter\n    unix_now = int(time.time())\n    interval_filters = (\n        ('created', cleaned['created'], cleaned['created_date']),\n        ('updated', cleaned['updated'], cleaned['updated_date']),\n        ('question_votes', cleaned['num_voted'], cleaned['num_votes']))\n    for filter_name, filter_option, filter_date in interval_filters:\n        if filter_option == constants.INTERVAL_BEFORE:\n            before = {filter_name + '__gte': 0,\n                      filter_name + '__lte': max(filter_date, 0)}\n\n            if filter_name != 'question_votes':\n                discussion_s = discussion_s.filter(**before)\n            question_s = question_s.filter(**before)\n        elif filter_option == constants.INTERVAL_AFTER:\n            after = {filter_name + '__gte': min(filter_date, unix_now),\n                     filter_name + '__lte': unix_now}\n\n            if filter_name != 'question_votes':\n                discussion_s = discussion_s.filter(**after)\n            question_s = question_s.filter(**after)\n\n    sortby = smart_int(request.GET.get('sortby'))\n    try:\n        max_results = settings.SEARCH_MAX_RESULTS\n        cleaned_q = cleaned['q']\n\n        if cleaned['w'] & constants.WHERE_WIKI:\n            wiki_s = wiki_s.query(cleaned_q)[:max_results]\n            # Execute the query and append to documents\n            documents += [('wiki', (pair[0], pair[1]))\n                          for pair in enumerate(wiki_s.object_ids())]\n\n        if cleaned['w'] & constants.WHERE_SUPPORT:\n            # Sort results by\n            try:\n                question_s = question_s.order_by(\n                    *constants.SORT_QUESTIONS[sortby])\n            except IndexError:\n                pass\n\n            question_s = question_s.highlight(\n                'content',\n                before_match='<b>',\n                after_match='</b>',\n                limit=settings.SEARCH_SUMMARY_LENGTH)\n\n            question_s = question_s.query(cleaned_q)[:max_results]\n            documents += [('question', (pair[0], pair[1]))\n                          for pair in enumerate(question_s.object_ids())]\n\n        if cleaned['w'] & constants.WHERE_DISCUSSION:\n            # Sort results by\n            try:\n                # Note that the first attribute needs to be the same\n                # here and in forums/models.py discussion_search.\n                discussion_s = discussion_s.group_by(\n                    'thread_id', constants.GROUPSORT[sortby])\n            except IndexError:\n                pass\n\n            discussion_s = discussion_s.highlight(\n                'content',\n                before_match='<b>',\n                after_match='</b>',\n                limit=settings.SEARCH_SUMMARY_LENGTH)\n\n            discussion_s = discussion_s.query(cleaned_q)[:max_results]\n            documents += [('discussion', (pair[0], pair[1]))\n                          for pair in enumerate(discussion_s.object_ids())]\n\n    except SearchError:\n        if is_json:\n            return HttpResponse(json.dumps({'error':\n                                             _('Search Unavailable')}),\n                                mimetype=mimetype, status=503)\n\n        t = 'search/mobile/down.html' if request.MOBILE else 'search/down.html'\n        return jingo.render(request, t, {'q': cleaned['q']}, status=503)\n\n    pages = paginate(request, documents, settings.SEARCH_RESULTS_PER_PAGE)\n\n    # Build a dict of { type_ -> list of indexes } for the specific\n    # docs that we're going to display on this page.  This makes it\n    # easy for us to slice the appropriate search Ss so we're limiting\n    # our db hits to just the items we're showing.\n    documents_dict = {}\n    for doc in documents[offset:offset + settings.SEARCH_RESULTS_PER_PAGE]:\n        documents_dict.setdefault(doc[0], []).append(doc[1][0])\n\n    docs_for_page = []\n    for type_, search_s in [('wiki', wiki_s),\n                            ('question', question_s),\n                            ('discussion', discussion_s)]:\n        if type_ not in documents_dict:\n            continue\n\n        # documents_dict[type_] is a list of indexes--one for each\n        # object id search result for that type_.  We use the values\n        # at the beginning and end of the list for slice boundaries.\n        begin = documents_dict[type_][0]\n        end = documents_dict[type_][-1] + 1\n        docs_for_page += [(type_, doc) for doc in search_s[begin:end]]\n\n    results = []\n    for i, docinfo in enumerate(docs_for_page):\n        rank = i + offset\n        type_, doc = docinfo\n        try:\n            if type_ == 'wiki':\n                summary = doc.current_revision.summary\n\n                result = {\n                    'search_summary': summary,\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'document',\n                    'rank': rank,\n                    'object': doc,\n                }\n                results.append(result)\n\n            elif type_ == 'question':\n                try:\n                    excerpt = question_s.excerpt(doc)[0]\n                except ExcerptTimeoutError:\n                    statsd.incr('search.excerpt.timeout')\n                    excerpt = u''\n                except ExcerptSocketErrorError:\n                    statsd.incr('search.excerpt.socketerror')\n                    excerpt = u''\n\n                summary = jinja2.Markup(clean_excerpt(excerpt))\n\n                result = {\n                    'search_summary': summary,\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'question',\n                    'rank': rank,\n                    'object': doc,\n                }\n                results.append(result)\n\n            else:\n                # discussion_s is based on Post--not Thread, so we have\n                # to get this manually.\n                thread = Thread.objects.get(pk=doc.thread_id)\n\n                try:\n                    excerpt = discussion_s.excerpt(doc)[0]\n                except ExcerptTimeoutError:\n                    statsd.incr('search.excerpt.timeout')\n                    excerpt = u''\n                except ExcerptSocketErrorError:\n                    statsd.incr('search.excerpt.socketerror')\n                    excerpt = u''\n\n                summary = jinja2.Markup(clean_excerpt(excerpt))\n\n                result = {\n                    'search_summary': summary,\n                    'url': thread.get_absolute_url(),\n                    'title': thread.title,\n                    'type': 'thread',\n                    'rank': rank,\n                    'object': thread,\n                }\n                results.append(result)\n        except IndexError:\n            break\n        except ObjectDoesNotExist:\n            continue\n\n    items = [(k, v) for k in search_form.fields for\n             v in r.getlist(k) if v and k != 'a']\n    items.append(('a', '2'))\n\n    if is_json:\n        # Models are not json serializable.\n        for r in results:\n            del r['object']\n        data = {}\n        data['results'] = results\n        data['total'] = len(results)\n        data['query'] = cleaned['q']\n        if not results:\n            data['message'] = _('No pages matched the search criteria')\n        json_data = json.dumps(data)\n        if callback:\n            json_data = callback + '(' + json_data + ');'\n\n        return HttpResponse(json_data, mimetype=mimetype)\n\n    results_ = jingo.render(request, template,\n        {'num_results': len(documents), 'results': results, 'q': cleaned['q'],\n         'pages': pages, 'w': cleaned['w'],\n         'search_form': search_form, 'lang_name': lang_name, })\n    results_['Cache-Control'] = 'max-age=%s' % \\\n                                (settings.SEARCH_CACHE_PERIOD * 60)\n    results_['Expires'] = (datetime.utcnow() +\n                           timedelta(minutes=settings.SEARCH_CACHE_PERIOD)) \\\n                           .strftime(expires_fmt)\n    results_.set_cookie(settings.LAST_SEARCH_COOKIE, urlquote(cleaned['q']),\n                        max_age=3600, secure=False, httponly=False)\n    return results_", "output": "def search(request, template=None):\n    \"\"\"Performs search or displays the search form.\"\"\"\n\n    # JSON-specific variables\n    is_json = (request.GET.get('format') == 'json')\n    callback = request.GET.get('callback', '').strip()\n    mimetype = 'application/x-javascript' if callback else 'application/json'\n\n    # Search \"Expires\" header format\n    expires_fmt = '%A, %d %B %Y %H:%M:%S GMT'\n\n    # Check callback is valid\n    if is_json and callback and not jsonp_is_valid(callback):\n        return HttpResponse(\n            json.dumps({'error': _('Invalid callback function.')}),\n            mimetype=mimetype, status=400)\n\n    language = locale_or_default(request.GET.get('language', request.locale))\n    r = request.GET.copy()\n    a = request.GET.get('a', '0')\n\n    # Search default values\n    try:\n        category = map(int, r.getlist('category')) or \\\n                   settings.SEARCH_DEFAULT_CATEGORIES\n    except ValueError:\n        category = settings.SEARCH_DEFAULT_CATEGORIES\n    r.setlist('category', category)\n\n    # Basic form\n    if a == '0':\n        r['w'] = r.get('w', constants.WHERE_BASIC)\n    # Advanced form\n    if a == '2':\n        r['language'] = language\n        r['a'] = '1'\n\n    # TODO: Rewrite so SearchForm is unbound initially and we can use `initial`\n    # on the form fields.\n    if 'include_archived' not in r:\n        r['include_archived'] = False\n\n    search_form = SearchForm(r)\n\n    if not search_form.is_valid() or a == '2':\n        if is_json:\n            return HttpResponse(\n                json.dumps({'error': _('Invalid search data.')}),\n                mimetype=mimetype,\n                status=400)\n\n        t = template if request.MOBILE else 'search/form.html'\n        search_ = jingo.render(request, t,\n                               {'advanced': a, 'request': request,\n                                'search_form': search_form})\n        search_['Cache-Control'] = 'max-age=%s' % \\\n                                   (settings.SEARCH_CACHE_PERIOD * 60)\n        search_['Expires'] = (datetime.utcnow() +\n                              timedelta(\n                                minutes=settings.SEARCH_CACHE_PERIOD)) \\\n                              .strftime(expires_fmt)\n        return search_\n\n    cleaned = search_form.cleaned_data\n\n    page = max(smart_int(request.GET.get('page')), 1)\n    offset = (page - 1) * settings.SEARCH_RESULTS_PER_PAGE\n\n    # get language name for display in template\n    lang = language.lower()\n    if settings.LANGUAGES.get(lang):\n        lang_name = settings.LANGUAGES[lang]\n    else:\n        lang_name = ''\n\n    wiki_s = wiki_search\n    question_s = question_search\n    discussion_s = discussion_search\n\n    documents = []\n\n    # wiki filters\n    # Category filter\n    if cleaned['category']:\n        wiki_s = wiki_s.filter(category__in=cleaned['category'])\n\n    # Locale filter\n    wiki_s = wiki_s.filter(locale=language)\n\n    # Product filter\n    products = cleaned['product']\n    for p in products:\n        wiki_s = wiki_s.filter(tag=p)\n\n    # Tags filter\n    tags = [t.strip() for t in cleaned['tags'].split()]\n    for t in tags:\n        wiki_s = wiki_s.filter(tag=t)\n\n    # Archived bit\n    if a == '0' and not cleaned['include_archived']:\n        # Default to NO for basic search:\n        cleaned['include_archived'] = False\n    if not cleaned['include_archived']:\n        wiki_s = wiki_s.filter(is_archived=False)\n    # End of wiki filters\n\n    # Support questions specific filters\n    if cleaned['w'] & constants.WHERE_SUPPORT:\n\n        # Solved is set by default if using basic search\n        if a == '0' and not cleaned['has_helpful']:\n            cleaned['has_helpful'] = constants.TERNARY_YES\n\n        # These filters are ternary, they can be either YES, NO, or OFF\n        ternary_filters = ('is_locked', 'is_solved', 'has_answers',\n                           'has_helpful')\n        d = dict((filter_name, _ternary_filter(cleaned[filter_name]))\n                 for filter_name in ternary_filters\n                 if cleaned[filter_name])\n        if d:\n            question_s = question_s.filter(**d)\n\n        if cleaned['asked_by']:\n            question_s = question_s.filter(\n                question_creator=cleaned['asked_by'])\n\n        if cleaned['answered_by']:\n            question_s = question_s.filter(\n                answer_creator=cleaned['answered_by'])\n\n        q_tags = [t.strip() for t in cleaned['q_tags'].split()]\n        for t in q_tags:\n            question_s = question_s.filter(tag=t)\n\n    # Discussion forum specific filters\n    if cleaned['w'] & constants.WHERE_DISCUSSION:\n        if cleaned['author']:\n            discussion_s = discussion_s.filter(author_ord=cleaned['author'])\n\n        if cleaned['thread_type']:\n            if constants.DISCUSSION_STICKY in cleaned['thread_type']:\n                discussion_s = discussion_s.filter(is_sticky=1)\n\n            if constants.DISCUSSION_LOCKED in cleaned['thread_type']:\n                discussion_s = discussion_s.filter(is_locked=1)\n\n        if cleaned['forum']:\n            discussion_s = discussion_s.filter(forum_id__in=cleaned['forum'])\n\n    # Filters common to support and discussion forums\n    # Created filter\n    unix_now = int(time.time())\n    interval_filters = (\n        ('created', cleaned['created'], cleaned['created_date']),\n        ('updated', cleaned['updated'], cleaned['updated_date']),\n        ('question_votes', cleaned['num_voted'], cleaned['num_votes']))\n    for filter_name, filter_option, filter_date in interval_filters:\n        if filter_option == constants.INTERVAL_BEFORE:\n            before = {filter_name + '__gte': 0,\n                      filter_name + '__lte': max(filter_date, 0)}\n\n            if filter_name != 'question_votes':\n                discussion_s = discussion_s.filter(**before)\n            question_s = question_s.filter(**before)\n        elif filter_option == constants.INTERVAL_AFTER:\n            after = {filter_name + '__gte': min(filter_date, unix_now),\n                     filter_name + '__lte': unix_now}\n\n            if filter_name != 'question_votes':\n                discussion_s = discussion_s.filter(**after)\n            question_s = question_s.filter(**after)\n\n    sortby = smart_int(request.GET.get('sortby'))\n    try:\n        max_results = settings.SEARCH_MAX_RESULTS\n        cleaned_q = cleaned['q']\n\n        if cleaned['w'] & constants.WHERE_WIKI:\n            wiki_s = wiki_s.query(cleaned_q)[:max_results]\n            # Execute the query and append to documents\n            documents += [('wiki', (pair[0], pair[1]))\n                          for pair in enumerate(wiki_s.object_ids())]\n\n        if cleaned['w'] & constants.WHERE_SUPPORT:\n            # Sort results by\n            try:\n                question_s = question_s.order_by(\n                    *constants.SORT_QUESTIONS[sortby])\n            except IndexError:\n                pass\n\n            question_s = question_s.highlight(\n                'content',\n                before_match='<b>',\n                after_match='</b>',\n                limit=settings.SEARCH_SUMMARY_LENGTH)\n\n            question_s = question_s.query(cleaned_q)[:max_results]\n            documents += [('question', (pair[0], pair[1]))\n                          for pair in enumerate(question_s.object_ids())]\n\n        if cleaned['w'] & constants.WHERE_DISCUSSION:\n            # Sort results by\n            try:\n                # Note that the first attribute needs to be the same\n                # here and in forums/models.py discussion_search.\n                discussion_s = discussion_s.group_by(\n                    'thread_id', constants.GROUPSORT[sortby])\n            except IndexError:\n                pass\n\n            discussion_s = discussion_s.highlight(\n                'content',\n                before_match='<b>',\n                after_match='</b>',\n                limit=settings.SEARCH_SUMMARY_LENGTH)\n\n            discussion_s = discussion_s.query(cleaned_q)[:max_results]\n            documents += [('discussion', (pair[0], pair[1]))\n                          for pair in enumerate(discussion_s.object_ids())]\n\n    except SearchError:\n        if is_json:\n            return HttpResponse(json.dumps({'error':\n                                             _('Search Unavailable')}),\n                                mimetype=mimetype, status=503)\n\n        t = 'search/mobile/down.html' if request.MOBILE else 'search/down.html'\n        return jingo.render(request, t, {'q': cleaned['q']}, status=503)\n\n    pages = paginate(request, documents, settings.SEARCH_RESULTS_PER_PAGE)\n\n    # Build a dict of { type_ -> list of indexes } for the specific\n    # docs that we're going to display on this page.  This makes it\n    # easy for us to slice the appropriate search Ss so we're limiting\n    # our db hits to just the items we're showing.\n    documents_dict = {}\n    for doc in documents[offset:offset + settings.SEARCH_RESULTS_PER_PAGE]:\n        documents_dict.setdefault(doc[0], []).append(doc[1][0])\n\n    docs_for_page = []\n    for type_, search_s in [('wiki', wiki_s),\n                            ('question', question_s),\n                            ('discussion', discussion_s)]:\n        if type_ not in documents_dict:\n            continue\n\n        # documents_dict[type_] is a list of indexes--one for each\n        # object id search result for that type_.  We use the values\n        # at the beginning and end of the list for slice boundaries.\n        begin = documents_dict[type_][0]\n        end = documents_dict[type_][-1] + 1\n        docs_for_page += [(type_, doc) for doc in search_s[begin:end]]\n\n    results = []\n    for i, docinfo in enumerate(docs_for_page):\n        rank = i + offset\n        type_, doc = docinfo\n        try:\n            if type_ == 'wiki':\n                summary = doc.current_revision.summary\n\n                result = {\n                    'search_summary': summary,\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'document',\n                    'rank': rank,\n                    'object': doc,\n                }\n                results.append(result)\n\n            elif type_ == 'question':\n                try:\n                    excerpt = question_s.excerpt(doc)[0]\n                except ExcerptTimeoutError:\n                    statsd.incr('search.excerpt.timeout')\n                    excerpt = u''\n                except ExcerptSocketErrorError:\n                    statsd.incr('search.excerpt.socketerror')\n                    excerpt = u''\n\n                summary = jinja2.Markup(clean_excerpt(excerpt))\n\n                result = {\n                    'search_summary': summary,\n                    'url': doc.get_absolute_url(),\n                    'title': doc.title,\n                    'type': 'question',\n                    'rank': rank,\n                    'object': doc,\n                }\n                results.append(result)\n\n            else:\n                # discussion_s is based on Post--not Thread, so we have\n                # to get this manually.\n                thread = Thread.objects.get(pk=doc.thread_id)\n\n                try:\n                    excerpt = discussion_s.excerpt(doc)[0]\n                except ExcerptTimeoutError:\n                    statsd.incr('search.excerpt.timeout')\n                    excerpt = u''\n                except ExcerptSocketErrorError:\n                    statsd.incr('search.excerpt.socketerror')\n                    excerpt = u''\n\n                summary = jinja2.Markup(clean_excerpt(excerpt))\n\n                result = {\n                    'search_summary': summary,\n                    'url': thread.get_absolute_url(),\n                    'title': thread.title,\n                    'type': 'thread',\n                    'rank': rank,\n                    'object': thread,\n                }\n                results.append(result)\n        except IndexError:\n            break\n        except ObjectDoesNotExist:\n            continue\n\n    items = [(k, v) for k in search_form.fields for\n             v in r.getlist(k) if v and k != 'a']\n    items.append(('a', '2'))\n\n    if is_json:\n        # Models are not json serializable.\n        for r in results:\n            del r['object']\n        data = {}\n        data['results'] = results\n        data['total'] = len(results)\n        data['query'] = cleaned['q']\n        if not results:\n            data['message'] = _('No pages matched the search criteria')\n        json_data = json.dumps(data)\n        if callback:\n            json_data = callback + '(' + json_data + ');'\n\n        return HttpResponse(json_data, mimetype=mimetype)\n\n    results_ = jingo.render(request, template,\n        {'num_results': len(documents), 'results': results, 'q': cleaned['q'],\n         'pages': pages, 'w': cleaned['w'],\n         'search_form': search_form, 'lang_name': lang_name, })\n    results_['Cache-Control'] = 'max-age=%s' % \\\n                                (settings.SEARCH_CACHE_PERIOD * 60)\n    results_['Expires'] = (datetime.utcnow() +\n                           timedelta(minutes=settings.SEARCH_CACHE_PERIOD)) \\\n                           .strftime(expires_fmt)\n    results_.set_cookie(settings.LAST_SEARCH_COOKIE, urlquote(cleaned['q']),\n                        max_age=3600, secure=False, httponly=False)\n    return results_"}
{"input": "def __init__(self, session):\n\t\tScreen.__init__(self, session)\n\t\tself[\"video\"] = VideoWindow()\n\t\tself.currentService = None\n\t\tconfig.av.pip = ConfigPosition(default=[-1, -1, -1, -1], limits = (719, 567, 720, 568))\n\t\tself.onLayoutFinish.append(self.LayoutFinished)", "output": "def __init__(self, session):\n\t\tScreen.__init__(self, session)\n\t\tself[\"video\"] = VideoWindow()\n\t\tself.currentService = None\n\t\tconfig.av.pip = ConfigPosition(default=[-1, -1, -1, -1], args = (719, 567, 720, 568))\n\t\tself.onLayoutFinish.append(self.LayoutFinished)"}
{"input": "def semich_bite():\n    # Hamiltonian Parameters\n    A = 2*CoFa.eV_to_au\n\n    # Gaps used in the dirac system\n    mx = 0.05*CoFa.eV_to_au\n    muz = 0.033\n\n    semich_bite_system = cued.hamiltonian.Semiconductor(A=A, mz=muz, mx=mx,\n                                                        a=8.28834, align=True)\n    return semich_bite_system", "output": "def semich_bite():\n    # Hamiltonian Parameters\n    A = 2*CoFa.eV_to_au\n\n    # Gaps used in the dirac system\n    mx = 0.05*CoFa.eV_to_au\n    muz = 0.033\n\n    semich_bite_system = cued.hamiltonian.Semiconductor(A=A, mz=muz, mx=mx,\n                                                        a=8.28834, nature=True)\n    return semich_bite_system"}
{"input": "def post_list(request):\n\tposts = Post.objects.filter(published_date_lte=timezone.now()).order_by('published_date')\n\treturn render(request, 'blog/post_list.html', {'posts':posts})", "output": "def post_list(request):\n\tposts = Post.objects.filter(published_date__lte=timezone.now()).order_by('published_date')\n\treturn render(request, 'blog/post_list.html', {'posts':posts})"}
